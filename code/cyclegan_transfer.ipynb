{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d96ee4-a44f-4eb3-9900-8f4d12316ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "import torch.utils.data\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from pytorchCycleGANandpix2pix.models import networks as offnets\n",
    "from pytorchCycleGANandpix2pix.util.image_pool import ImagePool\n",
    "from pytorchCycleGANandpix2pix.util.util import tensor2im\n",
    "\n",
    "import cyclegan_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5abea7-9e6c-452a-b512-3f79e85a5c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def revert_normalisation(tensor):\n",
    "    return (tensor.permute(1, 2, 0) + 1) / 2\n",
    "\n",
    "def revert_normalisation_batch(tensors):\n",
    "    return (tensors.permute(0, 2, 3, 1) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c2827c-f032-4749-9ec5-968490b20182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013b8ae9-0cd3-432e-8b56-99a03dc4e2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = \"./runs/1677749332.549509/115\"\n",
    "x_to_y = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0b90cf-8d4e-42bf-ab32-63569dd36a33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cyclegan_models.Generator()\n",
    "model.load_state_dict(torch.load(f\"{model_dir}/{'G' if x_to_y else 'F'}.pth\"))\n",
    "model.eval()\n",
    "model.to(device)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c6d9f5-15c6-4927-aa7c-5b1d865abc8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transfer_input = [\n",
    "    \"./cyclegan_original_data/horse2zebra/testA/n02381460_840.jpg\",\n",
    "    \"./cyclegan_original_data/horse2zebra/testA/n02381460_2100.jpg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f3ed24f-3bf7-48bd-8332-301b0db0afe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1488528856.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[80], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    transforms.Resize(128)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(128)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "def transfer_to_batch(images):\n",
    "    transformed = torch.stack([eval_transform(image) for image in images]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        transferred = model(transformed).detach().cpu()\n",
    "    \n",
    "    return revert_normalisation_batch(transferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bf3c7156-77fb-4c32-afeb-956d7b2e30c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transfer_to_video(video_loc):\n",
    "    inp_vid = cv2.VideoCapture(video_loc)\n",
    "    inp_fps = int(inp_vid.get(cv2.CAP_PROP_FPS))\n",
    "    inp_len = int(inp_vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    success, frame = inp_vid.read()\n",
    "    \n",
    "    height, width, channels = frame.shape\n",
    "    \n",
    "    split_loc = video_loc.split(\".\")\n",
    "    ext = split_loc[-1]\n",
    "    no_ext_loc = \".\".join(split_loc[:-1])\n",
    "    save_loc = f\"{no_ext_loc}_{time.time():.0f}.{ext}\"\n",
    "    out_vid = cv2.VideoWriter(save_loc, -1, inp_fps, (width, height))\n",
    "    \n",
    "    print(save_loc)\n",
    "    \n",
    "    batched = []\n",
    "    c = 0\n",
    "    \n",
    "    while success:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        batched.append(frame.copy())\n",
    "        \n",
    "        if len(batched) == 4:\n",
    "            stacked = np.stack(batched)\n",
    "            transferred = transfer_to_batch(stacked)\n",
    "            \n",
    "            for new_frame in transferred:\n",
    "                new_frame = (new_frame.numpy() * 255).astype(np.uint8)\n",
    "                new_frame = cv2.cvtColor(new_frame, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                cv2.imwrite(f\"./examples/{time.time()}.png\", new_frame)\n",
    "                out_vid.write(new_frame)\n",
    "                \n",
    "            batched = []\n",
    "        \n",
    "        success, frame = inp_vid.read()\n",
    "        c += 1\n",
    "        \n",
    "        if c == 40:\n",
    "            break\n",
    "    \n",
    "    out_vid.release()\n",
    "    inp_vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cecfe1cd-84ba-4cc9-8b42-a57bc4a4db5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./examples/horse_vid_1677771975.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtransfer_to_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./examples/horse_vid.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[89], line 27\u001b[0m, in \u001b[0;36mtransfer_to_video\u001b[1;34m(video_loc)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batched) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m     26\u001b[0m     stacked \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(batched)\n\u001b[1;32m---> 27\u001b[0m     transferred \u001b[38;5;241m=\u001b[39m \u001b[43mtransfer_to_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m new_frame \u001b[38;5;129;01min\u001b[39;00m transferred:\n\u001b[0;32m     30\u001b[0m         new_frame \u001b[38;5;241m=\u001b[39m (new_frame\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m, in \u001b[0;36mtransfer_to_batch\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m      7\u001b[0m transformed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([eval_transform(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 10\u001b[0m     transferred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m revert_normalisation_batch(transferred)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transfer_to_video(\"./examples/horse_vid.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fad3e-9ff6-4517-b9bd-846988d475c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
