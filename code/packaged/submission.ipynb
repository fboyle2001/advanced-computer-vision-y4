{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011db540-76a2-4754-9535-435c968eb1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.models\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# These imports contain code that is my own work\n",
    "from skeletal_pose import PoseKeypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c425dcb-98ac-415d-885d-4ad3f7b099cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(f\"It is highly recommended to use a GPU! This is likely to run extremely slowly otherwise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5b2fd-9d38-4a0d-ae35-68b280fb9b38",
   "metadata": {},
   "source": [
    "# 1: Human Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d02f9-170e-44fe-9896-7f71b40f5819",
   "metadata": {},
   "source": [
    "General setup instructions:\n",
    "\n",
    "* Please place the original data into a folder called 'original_data' in the same directory as this notebook\n",
    "* Please ensure ffmpeg is available on your system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99938d90-d9a3-497c-b6fb-52cce8f09de7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1: Human Patch Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5827d8b-0888-40fd-8f32-7bc73f226941",
   "metadata": {},
   "source": [
    "The process I follow to extract the human patches is as follows:\n",
    "\n",
    "1. Extract the frames from the training videos using ffmpeg\n",
    "2. Use a pre-trained MaskR-CNN model to segment and extract the human patches\n",
    "3. Save the human patches for future processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d18a58-7545-483f-9490-ee283bcf85dc",
   "metadata": {},
   "source": [
    "Firstly, I use ffmpeg to extract all frames from the videos and store them for later processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73f7e3-fa2b-4119-9520-ac8ad9a7d85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "./frame_extraction.sh ./original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c0b41-8482-4fdf-8f88-c80736f7aead",
   "metadata": {},
   "source": [
    "Next, I use a pre-trained MaskR-CNN model to extract the human patches.\n",
    "This part is heavily inspired by the practical 'Semantic Segmentation Mask R-CNN.ipynb' on Blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a10d4-0cfc-456b-b791-fe53c67114a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "maskrcnn.to(device).eval()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ff6df-fa85-4e8f-abc5-ffbbb4df4417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 91 COCO class names, directly from Semantic Segmentation Mask R-CNN.ipynb\n",
    "coco_names = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d0a54-d0d7-4e41-8b7f-928a57202193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def batch_loader(input_directory, output_directory, batch_size):\n",
    "    # Get all files in the directory that are frames\n",
    "    file_names = [file_name for file_name in os.listdir(input_directory) if file_name.endswith(\".jpg\")]\n",
    "    \n",
    "    for i in tqdm(range(len(file_names) // batch_size + 1)):\n",
    "        selected = []\n",
    "        start_idx = batch_size * i\n",
    "        limit_idx = batch_size * (i + 1)\n",
    "        \n",
    "        # Get the images to put through the model\n",
    "        if limit_idx > len(file_names):\n",
    "            selected = file_names[start_idx:]\n",
    "        else:\n",
    "            selected = file_names[start_idx:limit_idx]\n",
    "            \n",
    "        if len(selected) == 0:\n",
    "            break\n",
    "        \n",
    "        output_files = []\n",
    "        raw_frames = []\n",
    "        usable_frames = []\n",
    "        \n",
    "        for file_name in selected:\n",
    "            # Pre-determine the save location and load the frame with RGB\n",
    "            output_files.append(f\"{output_directory}/Segmented_{file_name.split('.')[0]}\")\n",
    "            input_file = f\"{input_directory}/{file_name}\"\n",
    "            raw_frame = cv2.imread(input_file, cv2.IMREAD_COLOR)\n",
    "            raw_frames.append(raw_frame)\n",
    "            \n",
    "            usable_frame = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2RGB)\n",
    "            usable_frame = tensor_transform(usable_frame)\n",
    "            usable_frames.append(usable_frame)\n",
    "        \n",
    "        # Return a tensor to use with the model and additional info for cropping and saving\n",
    "        stacked_frames = torch.stack(usable_frames).to(device)\n",
    "        yield stacked_frames, raw_frames, output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac26cdf-8cab-4849-859c-c3f9d9d8d5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_maskrcnn_output(maskrcnn_output, original_frame, save_loc, threshold):\n",
    "    # Get the relevant model output on to the CPU in a usable format\n",
    "    scores = output[\"scores\"].detach().cpu().numpy()\n",
    "    boxes = output[\"boxes\"].detach().cpu().numpy().astype(int)\n",
    "    label_indices = output[\"labels\"].detach().cpu()\n",
    "    \n",
    "    # Process each entry found by the model\n",
    "    for i, (confidence, box, label_idx) in enumerate(zip(scores, boxes, label_indices)):\n",
    "        label = coco_names[label_idx.item()]\n",
    "        \n",
    "        if label != \"person\":\n",
    "            continue\n",
    "        \n",
    "        # The confidences are sorted high to low so stop once we're below the threshold\n",
    "        if confidence < threshold:\n",
    "            break\n",
    "          \n",
    "        # Crop and save the patch using its bounding box\n",
    "        x_0, y_0, x_1, y_1 = box\n",
    "        patch = original_frame.copy()[y_0:y_1, x_0:x_1]\n",
    "        cv2.imwrite(f\"{save_loc}_{i}.jpg\", patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eee283-6bbe-4416-8bda-96b696af8279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If everything is setup correctly and the frames have been extracted these settings should be fine\n",
    "base_folders = [\"Train/Game\", \"Train/Movie\"]\n",
    "base_frame_folder = \"./extracted_data/frames\"\n",
    "base_output_folder = \"./extracted_data/unclassified_human_patches\"\n",
    "\n",
    "batch_size = 4\n",
    "threshold = 0.965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b56ffa-92a7-4451-99a5-9c7996c10469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process each folder and every frame in them\n",
    "for base_folder in base_folders:\n",
    "    in_directory = f\"{base_frame_folder}/{base_folder}\"\n",
    "    out_directory = f\"{base_output_folder}/{base_folder}\"\n",
    "    \n",
    "    os.makedirs(out_directory)\n",
    "    \n",
    "    print(f\"{in_directory} -> {out_directory}\")\n",
    "    \n",
    "    batch_generator = batch_loader(in_directory, out_directory, batch_size)\n",
    "\n",
    "    for i, (batch, raw_frames, output_files) in enumerate(batch_generator):\n",
    "        with torch.no_grad():\n",
    "            outputs = maskrcnn(batch)\n",
    "\n",
    "        for output, frame, output_file in zip(outputs, raw_frames, output_files):\n",
    "            process_maskrcnn_output(output, frame, output_file, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff914a2-2095-4728-ac1f-8d0885719cce",
   "metadata": {},
   "source": [
    "In testing, this gave:\n",
    "\n",
    "* 18,454 human patches for the movie clips\n",
    "* 37,530 human patches for the game clips \n",
    "\n",
    "I now sample 50 random images from each domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922652c3-5952-4308-9044-eb26a3e3111a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_submission_folder = \"./data_for_submission/1_1_sampled_human_patches\"\n",
    "sample_count = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c956f58-b4f0-408a-a2e1-e125b026cda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for base_folder in base_folders:\n",
    "    patch_directory = f\"{base_output_folder}/{base_folder}\"\n",
    "    sampled_directory = f\"{base_submission_folder}/{base_folder}\"\n",
    "    \n",
    "    os.makedirs(sampled_directory) \n",
    "    \n",
    "    print(f\"{patch_directory} -> {sampled_directory}\")\n",
    "    \n",
    "    file_names = [file_name for file_name in os.listdir(patch_directory) if file_name.endswith(\".jpg\")]\n",
    "    sampled_file_names = random.sample(file_names, k=sample_count)\n",
    "    \n",
    "    for sampled in sampled_file_names:\n",
    "        shutil.copy(f\"{patch_directory}/{sampled}\", sampled_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f3ffa-c212-4ebb-92a2-e23b455b8f09",
   "metadata": {},
   "source": [
    "## 1.2: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76022d-72a4-4c8a-bb5d-710c8c46a55d",
   "metadata": {},
   "source": [
    "Using the human patches from the previous task, I use the following process to classify them according to pose:\n",
    "1. Use OpenPose to extract the pose keypoints from each human patch\n",
    "2. Load the extracted pose into Python and analyse the data to determine the pose based on joint visibility\n",
    "3. To determine if they are standing or sitting I compute the angle between the hips and knees if they are visible\n",
    "4. Save the images into folders based on their classified pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b1d9e-491e-4641-8b40-f03148222b64",
   "metadata": {},
   "source": [
    "Firstly, I use OpenPose to extract the pose. Depending on your install of OpenPose you may need to adjust the command slightly below to point to the OpenPose executable. I generated normalised keypoints so that image scale is irrelevant for determining the pose and limit the resolution and disable the display to work within my GPU resource constraints. Depending on your operating system use the %%cmd cell for Windows and the %%bash cell for Unix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ffe1a-6ffb-4125-aac5-e10ea1427573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%cmd\n",
    "mkdir -p .\\extracted_data\\human_poses\\Train\\Game\n",
    ".\\openpose\\bin\\OpenPoseDemo.exe --image_dir .\\extracted_data\\unclassified_human_patches\\Train\\Game --write_json .\\extracted_data\\human_poses\\Train\\Game --keypoint_scale 3 --net_resolution \"656x368\" --display 0 --render_pose 0\n",
    "mkdir .\\extracted_data\\human_poses\\Train\\Movie\n",
    ".\\openpose\\bin\\OpenPoseDemo.exe --image_dir .\\extracted_data\\unclassified_human_patches\\Train\\Movie --write_json .\\extracted_data\\human_poses\\Train\\Movie --keypoint_scale 3 --net_resolution \"656x368\" --display 0 --render_pose 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59a809-f067-4548-96a0-469714bad021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ./extracted_data/human_poses/Train/Game\n",
    "./openpose/build/examples/openpose/openpose.bin --image_dir ./extracted_data/unclassified_human_patches/Train/Game --write_json ./extracted_data/human_poses/Train/Game --keypoint_scale 3 --net_resolution \"656x368\" --display 0 --render_pose 0\n",
    "mkdir -p ./extracted_data/human_poses/Train/Movie\n",
    "./openpose/build/examples/openpose/openpose.bin --image_dir ./extracted_data/unclassified_human_patches/Train/Movie --write_json ./extracted_data/human_poses/Train/Movie --keypoint_scale 3 --net_resolution \"656x368\" --display 0 --render_pose 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bec2a-d860-4aef-a339-852705619ea8",
   "metadata": {},
   "source": [
    "Now that I have the extracted pose data saved as .json files in a one-to-one mapping with each human patch (although there may be multiple humans detected by the pose detector!), I process these using Python. Before this I need to define what I count as each class. In connection with Q1.3 I consider those that are facing away to be in the class 'Other' regardless of all other attributes. I group the joints and measure the visibility of each group, the groups are:\n",
    "\n",
    "* Head: Nose, Left Eye, Left Ear, Right Eye, Right Eye\n",
    "* Torso and Arms: Neck, Left Shoulder, Left Elbow, Left Wrist, Right Shoulder, Right Elbow, Right Wrist\n",
    "* Hips: Mid Hip, Left Hip, Right Hip\n",
    "* Legs: Left Knee, Left Ankle, Right Knee, Right Ankle\n",
    "* Feet: Left Ankle, Left Heel, Left Big Toe, Left Small Toe, Right Ankle, Right Heel, Right Big Toe, Right Small Toe\n",
    "\n",
    "Then to classify by pose:\n",
    "\n",
    "* Head Only: At least 60% of the Head group visible, less than 45% of the Torso and Arms group visible and none of the Hips, Legs or Feet groups visible\n",
    "* Half Body: At least 40% of the Head group visible, at least 50% of the Torso and Arms group visible, any amount of Hips, and none of the Legs or Feet groups visible\n",
    "* Full Body: At least 40% of the Head group visible, at least 70% of the Torso and Arms group visible, at least 50% of the Hips and Legs visible, and any amount of Feet visible\n",
    "\n",
    "Finally, to classify sitting and standing I measure the angle between the knees and hips. If this is less than 50 degrees they are sitting otherwise they are standing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e6d0ed4-cbc3-44ca-a12e-d53060ff500f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_folders = [\"Train/Movie\", \"Train/Game\"]\n",
    "base_patch_directory = \"./extracted_data/unclassified_human_patches\"\n",
    "base_pose_directory = \"./extracted_data/human_poses\"\n",
    "base_classification_directory = \"./extracted_data/classified_human_patches\"\n",
    "base_sample_directory = \"./extracted_data/human_patches_validation_set/raw\"\n",
    "base_sample_classified_directory = \"./extracted_data/human_patches_validation_set/hand_classified\"\n",
    "\n",
    "sample_count = 150\n",
    "\n",
    "classes = [\"Full Body Sitting\", \"Full Body Standing\", \"Half Body\", \"Head Only\", \"Other\"]\n",
    "show_skeleton = False\n",
    "enforce_facing = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f8ccf-7604-46d6-907c-336000d2c28e",
   "metadata": {},
   "source": [
    "Prior to classifying them, I sample 150 patches from each domain to classify by hand to evaluate the performance of the pose classification process. I put the classified files in the following directory: ./extracted_data/human_patches_validation_set/{base_folder}/{class_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb21c83-390c-42a4-9416-905d28017f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for base_folder in base_folders:\n",
    "    patch_directory = f\"{base_patch_directory}/{base_folder}\"\n",
    "    sampled_directory = f\"{base_sample_directory}/{base_folder}\"\n",
    "    \n",
    "    os.makedirs(sampled_directory) \n",
    "    \n",
    "    print(f\"{patch_directory} -> {sampled_directory}\")\n",
    "    \n",
    "    file_names = [file_name for file_name in os.listdir(patch_directory) if file_name.endswith(\".jpg\")]\n",
    "    sampled_file_names = random.sample(file_names, k=sample_count)\n",
    "    \n",
    "    for sampled in sampled_file_names:\n",
    "        shutil.copy(f\"{patch_directory}/{sampled}\", sampled_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77eecbfb-557a-4a82-b8e2-e2fe16865af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd467ac042e1442cb3e7d0d4ecaefff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384e857314f040748e6afa2f2700db32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for base_folder in base_folders:\n",
    "    patch_directory = f\"{base_patch_directory}/{base_folder}\"\n",
    "    pose_directory = f\"{base_pose_directory}/{base_folder}\"\n",
    "    output_directory = f\"{base_classification_directory}/{base_folder}\"\n",
    "    \n",
    "    for class_name in classes:\n",
    "        os.makedirs(f\"{output_directory}/{class_name}\")\n",
    "    \n",
    "    file_names = [''.join(file_name.split(\".\")[:-1]) for file_name in os.listdir(patch_directory) if file_name.endswith(\".jpg\")]\n",
    "    \n",
    "    for file_name in tqdm(file_names):\n",
    "        patch_loc = f\"{patch_directory}/{file_name}.jpg\"\n",
    "        keypoints_loc = f\"{pose_directory}/{file_name}_keypoints.json\"\n",
    "        \n",
    "        with open(keypoints_loc, \"r\") as fp:\n",
    "            raw_data = json.load(fp)\n",
    "\n",
    "        if len(raw_data[\"people\"]) != 1:\n",
    "            continue\n",
    "        \n",
    "        kps = raw_data[\"people\"][0][\"pose_keypoints_2d\"]\n",
    "        pose = PoseKeypoints.load_keypoints(kps)\n",
    "        \n",
    "        classification = pose.classify(enforce_facing=enforce_facing)\n",
    "        save_loc = f\"{output_directory}/{classification}/{file_name}.jpg\"\n",
    "        \n",
    "        patch = cv2.imread(patch_loc)\n",
    "    \n",
    "        if show_skeleton:\n",
    "            patch = pose.overlay_pose(patch)\n",
    "        \n",
    "        cv2.imwrite(save_loc, patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54778d69-932a-4d80-9945-a6b6591e4b6e",
   "metadata": {},
   "source": [
    "I now evaluate the results using the hand classified set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c068e3b-00c9-478f-b106-11cf0eeee208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7115384615384616\n",
      "Accuracy: 0.6017699115044248\n"
     ]
    }
   ],
   "source": [
    "for base_folder in base_folders:\n",
    "    classification_data = {}\n",
    "    hand_classified_folder = f\"{base_sample_classified_directory}/{base_folder}\"\n",
    "    pred_classified_folder = f\"{base_classification_directory}/{base_folder}\"\n",
    "    \n",
    "    for class_name in classes:\n",
    "        for file_name in os.listdir(f\"{hand_classified_folder}/{class_name}\"):\n",
    "            classification_data[file_name] = {\"ground_truth\": class_name, \"predicted\": None}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        classified_files = os.listdir(f\"{pred_classified_folder}/{class_name}\")\n",
    "        \n",
    "        for file_name in classification_data.keys():\n",
    "            if file_name in classified_files:\n",
    "                classification_data[file_name][\"predicted\"] = class_name\n",
    "                \n",
    "    correct = 0\n",
    "    \n",
    "    classification_data_rows = [{\"file\": key, **value} for key, value in classification_data.items() if value[\"predicted\"] is not None]\n",
    "    df = pd.DataFrame(classification_data_rows)\n",
    "    \n",
    "    df[\"match\"] = df.apply(lambda row: row[\"ground_truth\"] == row[\"predicted\"], axis=1)\n",
    "    \n",
    "    print(f\"Accuracy:\", df[\"match\"].sum() / len(df))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cad786-35c6-43c3-9014-7285f09ff347",
   "metadata": {},
   "source": [
    "I now sample 10 images from each class per domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78b5fec4-4314-40ab-b085-1cdb5cd2498f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_submission_folder = \"./data_for_submission/1_2_sampled_pose_classified\"\n",
    "sample_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e929c39-1fbb-4940-a7e0-54ff7e85a785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./extracted_data/classified_human_patches/Train/Movie/Full Body Sitting -> ./data_for_submission/1_2_sampled_pose_classified/Train/Movie/Full Body Sitting\n",
      "./extracted_data/classified_human_patches/Train/Movie/Full Body Standing -> ./data_for_submission/1_2_sampled_pose_classified/Train/Movie/Full Body Standing\n",
      "./extracted_data/classified_human_patches/Train/Movie/Half Body -> ./data_for_submission/1_2_sampled_pose_classified/Train/Movie/Half Body\n",
      "./extracted_data/classified_human_patches/Train/Movie/Head Only -> ./data_for_submission/1_2_sampled_pose_classified/Train/Movie/Head Only\n",
      "./extracted_data/classified_human_patches/Train/Movie/Other -> ./data_for_submission/1_2_sampled_pose_classified/Train/Movie/Other\n",
      "./extracted_data/classified_human_patches/Train/Game/Full Body Sitting -> ./data_for_submission/1_2_sampled_pose_classified/Train/Game/Full Body Sitting\n",
      "./extracted_data/classified_human_patches/Train/Game/Full Body Standing -> ./data_for_submission/1_2_sampled_pose_classified/Train/Game/Full Body Standing\n",
      "./extracted_data/classified_human_patches/Train/Game/Half Body -> ./data_for_submission/1_2_sampled_pose_classified/Train/Game/Half Body\n",
      "./extracted_data/classified_human_patches/Train/Game/Head Only -> ./data_for_submission/1_2_sampled_pose_classified/Train/Game/Head Only\n",
      "./extracted_data/classified_human_patches/Train/Game/Other -> ./data_for_submission/1_2_sampled_pose_classified/Train/Game/Other\n"
     ]
    }
   ],
   "source": [
    "for base_folder in base_folders:\n",
    "    for class_name in classes:\n",
    "        sampled_directory = f\"{base_submission_folder}/{base_folder}/{class_name}\"\n",
    "        os.makedirs(sampled_directory)\n",
    "        \n",
    "        class_folder = f\"{base_classification_directory}/{base_folder}/{class_name}\"\n",
    "        print(f\"{class_folder} -> {sampled_directory}\")\n",
    "        \n",
    "        file_names = [file_name for file_name in os.listdir(class_folder) if file_name.endswith(\".jpg\")]\n",
    "        sampled_file_names = random.sample(file_names, k=min(sample_count, len(file_names)))\n",
    "        \n",
    "        for sampled in sampled_file_names:\n",
    "            shutil.copy(f\"{class_folder}/{sampled}\", sampled_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8199a95-1e43-463e-a842-a1fa7cb9363b",
   "metadata": {},
   "source": [
    "## 1.3: Training Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb33ffd-be8f-455a-ba8d-e98f3751ffc8",
   "metadata": {},
   "source": [
    "Now that I have classified the human patches according to pose, I need to select the most appropriate data for training the model. I will use CycleGAN initially as such I need approximately 1200 images from both domains to form a quality dataset. This needs to be diverse (i.e. not just similar patches a couple of frames apart) and representative of the data. To achieve this I group human patches into sequences to reduce oversampling of extremely similar patches which will degrade the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "584e967c-8393-44e6-adc7-183b1653e898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_similar_images(ordered_dir, cutoff, window):\n",
    "    image_locs = [f\"{ordered_dir}/{file_name}\" for file_name in os.listdir(ordered_dir) if file_name.endswith(\".jpg\")]\n",
    "    groups = [[image_locs[0]]]\n",
    "    \n",
    "    for image_loc in tqdm(image_locs):\n",
    "        img_hash = imagehash.average_hash(Image.open(image_loc))\n",
    "        \n",
    "        closest_group_idx = -1\n",
    "        closest_group_diff = 65\n",
    "        \n",
    "        for offset, group in enumerate(groups[-window:][::-1]):\n",
    "            group_idx = len(groups) - offset - 1\n",
    "            last_hash = imagehash.average_hash(Image.open(group[-1]))\n",
    "            diff = img_hash - last_hash\n",
    "            \n",
    "            if diff < closest_group_diff:\n",
    "                closest_group_idx = group_idx\n",
    "                closest_group_diff = diff\n",
    "        \n",
    "        if closest_group_diff <= cutoff:\n",
    "            groups[closest_group_idx].append(image_loc)\n",
    "        else:\n",
    "            groups.append([image_loc])\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdb54f70-c40b-4e91-8411-cd238fe90c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_classes(base_directory, classes, cutoff, window):\n",
    "    groups = []\n",
    "    \n",
    "    for class_name in classes:\n",
    "        pose_directory = f\"{base_directory}/{class_name}\"\n",
    "        groups += group_similar_images(pose_directory, cutoff, window)\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53507d3a-73e2-45cf-8444-c14c5e4c780c",
   "metadata": {},
   "source": [
    "I discard all images from the 'Other' category as I determine that these will likely not be of use to the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "185105af-d72e-47ab-a456-a7a9793cd5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = [\"Full Body Sitting\", \"Full Body Standing\", \"Half Body\", \"Head Only\"]\n",
    "base_pose_directory = \"./extracted_data/classified_human_patches\"\n",
    "cutoff = 8\n",
    "window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8ac4355-ff6f-4c18-92bf-326d37612334",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41097799ecc34e87a4b26d75b12f7a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8ab9b6a427474e9c5889b0d6e51cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae50b62376154e7080ba9f6429b08baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abf87956f7841b0be59c9dca0fc8a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c698a40d899c40459e201478c521b5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b488610d8d4f46c79eae6ff533801448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b180650f72a0408ebf4e753f7498855a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b00713b7c2f4b9ca230d6fd228edcab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game_groups = group_classes(f\"{base_pose_directory}/Train/Game\", classes, cutoff, window) \n",
    "movie_groups = group_classes(f\"{base_pose_directory}/Train/Movie\", classes, cutoff, window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d0840-e205-4723-a4d6-f41bfcd6bee6",
   "metadata": {},
   "source": [
    "Now I sample from these sequences of images. I also discard images that are too small, as I will training on 128x128 images for CycleGAN I discard all images that are smaller than 64x64 pixels as they will be too poor quality to be useful when resized to 128x128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a09e6fbe-96fe-40a6-911b-785857ded481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_test_count = 250\n",
    "target_train_count = 1200\n",
    "output_size = 128\n",
    "min_size = 64\n",
    "sequence_proportion = 0.05\n",
    "\n",
    "train_output_directory = lambda base: f\"./extracted_data/cyclegan_training_data/{base}/Train\"\n",
    "test_output_directory = lambda base: f\"./extracted_data/cyclegan_training_data/{base}/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "526bf0bd-c47e-4be2-b30d-82b29a44eb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_train_test_split(groups, train_dir, test_dir, train_target, test_target, output_size, min_size, sequence_proportion):\n",
    "    os.makedirs(train_dir)\n",
    "    os.makedirs(test_dir)\n",
    "    \n",
    "    for group_no, group in enumerate(tqdm(groups)):\n",
    "        selected_patches = random.sample(range(len(group)), k=max(1, int(sequence_proportion * len(group))))\n",
    "        \n",
    "        for i in selected_patches:\n",
    "            patch = Image.open(group[i])\n",
    "            \n",
    "            if patch.width < min_size or patch.height < min_size:\n",
    "                continue\n",
    "                \n",
    "            patch = patch.resize((output_size, output_size))\n",
    "            patch.save(f\"{train_dir}/{group_no:05d}_{i:05d}.jpg\")\n",
    "    \n",
    "    valid_train_files = [file_name for file_name in os.listdir(train_dir) if file_name.endswith(\".jpg\")]\n",
    "    \n",
    "    if len(valid_train_files) <= train_target:\n",
    "        print(f\"Only found {len(valid_train_files)} valid files, not able to produce a test set\")\n",
    "        return\n",
    "    \n",
    "    available_for_test = min(len(valid_train_files) - train_target, test_target)\n",
    "    selected_for_test = random.sample(valid_train_files, k=available_for_test)\n",
    "    \n",
    "    for file_name in selected_for_test:\n",
    "        os.rename(f\"{train_dir}/{file_name}\", f\"{test_dir}/{file_name}\")\n",
    "    \n",
    "    print(\"Moved\", len(selected_for_test), \"files to test set\")\n",
    "    \n",
    "    valid_train_files = [file_name for file_name in os.listdir(train_dir) if file_name.endswith(\".jpg\")]\n",
    "    \n",
    "    if len(valid_train_files) > train_target:\n",
    "        selected_for_delete = random.sample(valid_train_files, k=len(valid_train_files) - train_target)\n",
    "    \n",
    "        for file_name in selected_for_delete:\n",
    "            os.remove(f\"{train_dir}/{file_name}\")\n",
    "\n",
    "        print(\"Deleted\", len(selected_for_delete), \"files from the train set\")\n",
    "    \n",
    "    print(\"Train Size:\", len([file_name for file_name in os.listdir(train_dir) if file_name.endswith(\".jpg\")]))\n",
    "    print(\"Test Size:\", len([file_name for file_name in os.listdir(test_dir) if file_name.endswith(\".jpg\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0d4ee0e-52a7-4c6c-9df0-2cd654ae99d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a973da479d4f62913c8c2fc98f12da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 250 files to test set\n",
      "Deleted 183 files from the train set\n",
      "Train Size: 1200\n",
      "Test Size: 250\n"
     ]
    }
   ],
   "source": [
    "create_train_test_split(game_groups, train_output_directory(\"Game\"), test_output_directory(\"Game\"), target_train_count, target_test_count, output_size, min_size, sequence_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d4ae863-9d32-4ae0-8a33-74cfb0b873b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1dcdfc1872474aaa7f9af65e9ac231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 106 files to test set\n",
      "Train Size: 1200\n",
      "Test Size: 106\n"
     ]
    }
   ],
   "source": [
    "create_train_test_split(movie_groups, train_output_directory(\"Movie\"), test_output_directory(\"Movie\"), target_train_count, target_test_count, output_size, min_size, sequence_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1beb9e-246f-486c-8258-da8300768d12",
   "metadata": {},
   "source": [
    "Now I sample 50 random images from each training dataset for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4578ac4d-5b07-479b-86f7-a4b3ff9904cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_submission_folder = \"./data_for_submission/1_3_sampled_training_data\"\n",
    "sample_count = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a472cde-0d2c-45e4-867a-41b8e32c30b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./extracted_data/cyclegan_training_data/Game/Train -> ./data_for_submission/1_3_sampled_training_data/Game\n",
      "./extracted_data/cyclegan_training_data/Movie/Train -> ./data_for_submission/1_3_sampled_training_data/Movie\n"
     ]
    }
   ],
   "source": [
    "for base_folder in [\"Game\", \"Movie\"]:\n",
    "    train_directory = train_output_directory(base_folder)\n",
    "    sampled_directory = f\"{base_submission_folder}/{base_folder}\"\n",
    "    \n",
    "    os.makedirs(sampled_directory) \n",
    "    \n",
    "    print(f\"{train_directory} -> {sampled_directory}\")\n",
    "    \n",
    "    file_names = [file_name for file_name in os.listdir(train_directory) if file_name.endswith(\".jpg\")]\n",
    "    sampled_file_names = random.sample(file_names, k=sample_count)\n",
    "    \n",
    "    for sampled in sampled_file_names:\n",
    "        shutil.copy(f\"{train_directory}/{sampled}\", sampled_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41ab59-62ba-465a-8298-5cb4ff6132c1",
   "metadata": {},
   "source": [
    "# Real-world Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c23dfb06-ff13-4870-9204-3bdb763c9e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from joint_dataset import JointDomainImageDataset, JointDomainTripletDataset\n",
    "from generator_model import Generator\n",
    "from cycle_gan import CycleGAN\n",
    "from recycle_gan import RecycleGAN\n",
    "from visdom_utils import MultiLinePlot\n",
    "from image_utils import revert_normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea568e-42df-4803-93fd-100e43829d32",
   "metadata": {},
   "source": [
    "## 2.1: Image Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873d60a-2017-444a-b0b0-d6a65c6ba043",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba5805c7-f918-4c0c-a0f0-2bd1619ff290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using_google_gpus = False\n",
    "\n",
    "train_X_loc = \"./extracted_data/cyclegan_training_data/Game/Train\" \n",
    "test_X_loc = \"./extracted_data/cyclegan_training_data/Game/Test\" \n",
    "\n",
    "train_Y_loc = \"./extracted_data/cyclegan_training_data/Movie/Train\"\n",
    "test_Y_loc = \"./extracted_data/cyclegan_training_data/Movie/Test\"\n",
    "\n",
    "run_data_directory = \"./runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c969af7-5f6e-4c5a-be28-6b7b804d9fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if using_google_gpus:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    vis = None\n",
    "else:\n",
    "    import visdom\n",
    "    vis = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85931a-29ad-4425-9496-fb29bb56aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/cyclegan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff0c67-274c-4e1a-870b-a28a7da84645",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JointDomainImageDataset(train_X_loc, train_Y_loc, train=True, img_size=128)\n",
    "test_dataset = JointDomainImageDataset(test_X_loc, test_Y_loc, train=False, img_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d81a74-7359-4197-9936-623d4e14ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xs = []\n",
    "test_ys = []\n",
    "\n",
    "for i in random.sample(range(0, len(test_dataset)), 16):\n",
    "    x, y = test_dataset[i]\n",
    "    test_xs.append(x)\n",
    "    test_ys.append(y)\n",
    "\n",
    "test_xs = torch.stack(test_xs)\n",
    "test_ys = torch.stack(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9538f2-ad81-4828-80e3-71c2a7356078",
   "metadata": {},
   "outputs": [],
   "source": [
    "if vis is not None:\n",
    "    vis.images(torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in test_xs]), nrow=4, opts={\"title\": \"X_test originals\"})\n",
    "    vis.images(torch.stack([revert_normalisation(y).permute(2, 0, 1) for y in test_ys]), nrow=4, opts={\"title\": \"Y_test originals\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c26f71-e553-41c5-b442-da2a63ede0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = test_dataset[random.randint(0, len(test_dataset))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf14622-23f3-4f77-86a7-167e2ac2ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_img.min(), test_img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72827ab8-0ef3-41bf-b415-c56f6f7a1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2847bf-e756-486a-83e4-a947b9bfc619",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(revert_normalisation(test_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a1a63-e7d0-4d5b-9a5c-4b8c795e2358",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0649aa7-7fe1-4600-8297-5e6e45aafb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 100\n",
    "lambda_weight = 10\n",
    "lambda_idt_X = 0.5\n",
    "lambda_idt_Y = 0.5\n",
    "\n",
    "blocks = 6\n",
    "upsample_strategy = [\"upsample\", \"conv_transpose\", \"pixel_shuffle\"][0]\n",
    "pool_size = 50\n",
    "opt_scheduler_type = \"linear_decay_with_warmup\"\n",
    "\n",
    "checkpoint_instance_dir = None\n",
    "checkpoint_epoch_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c021f-d634-4c02-a457-8076d2df8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f30f66-d599-4931-8857-09b481b287a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint_instance_dir is not None and checkpoint_epoch_dir is not None:\n",
    "    cyclegan = CycleGAN.load(f\"{run_data_directory}/{checkpoint_instance_dir}\", f\"{checkpoint_epoch_dir}\", device, blocks)\n",
    "else:\n",
    "    cyclegan = CycleGAN(blocks, upsample_strategy, device, pool_size, opt_scheduler_type, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2304f-045b-421e-a2fd-66221c2a1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{cyclegan.save_folder}/info_{checkpoint_epoch_dir}.json\", \"w+\") as fp:\n",
    "    json.dump({\n",
    "        \"block_count\": cyclegan.resnet_block_count,\n",
    "        \"upsample_strategy\": upsample_strategy,\n",
    "        \"pool_size\": pool_size,\n",
    "        \"opt_scheduler_type\": opt_scheduler_type,\n",
    "        \"data_folders\": {\n",
    "            \"train_X\": train_X_loc,\n",
    "            \"test_X\": test_X_loc,\n",
    "            \"train_Y\": train_Y_loc,\n",
    "            \"test_Y\": test_Y_loc\n",
    "        },\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_epochs\": epochs,\n",
    "        \"start_epoch\": cyclegan.start_epoch,\n",
    "        \"lambda_weight\": lambda_weight,\n",
    "        \"lambda_idt_X\": lambda_idt_X,\n",
    "        \"lambda_idt_Y\": lambda_idt_Y,\n",
    "        \"checkpoint\": {\n",
    "            \"instance\": checkpoint_instance_dir,\n",
    "            \"epoch\": checkpoint_epoch_dir\n",
    "        }\n",
    "    }, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e1468-fb3d-4eae-b228-d1c99c545808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could also enable random flipping\n",
    "def generate_noisy_labels(shape, real, device):\n",
    "    # Randomly generated between 0 and 1\n",
    "    labels = torch.rand(shape, device=device)\n",
    "    \n",
    "    if real:\n",
    "        # Now they are between 0.7 and 1.1\n",
    "        labels = (2 * labels / 5) + 0.7\n",
    "    else:\n",
    "        # Now they are between 0 and 0.3\n",
    "        labels = (labels * 3) / 10\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04939a-dc0e-4dee-9322-653cff0a8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_loss(real, fake, pool, discriminator, loss_func):\n",
    "    # Discriminator should give (1) for a real image and (0) for a fake\n",
    "    real_pred = discriminator(real)\n",
    "    real_loss = loss_func(real_pred, generate_noisy_labels(real_pred.shape, True, device)) # Should dampen?\n",
    "    \n",
    "    # We draw from the history buffer\n",
    "    pool_fake = pool.randomise_existing_batch(fake)\n",
    "    fake_pred = discriminator(pool_fake)\n",
    "    # Fake images should not fool the discriminator\n",
    "    fake_loss = loss_func(fake_pred.detach(), generate_noisy_labels(fake_pred.shape, False, device))\n",
    "    \n",
    "    avg_loss = (real_loss + fake_loss) * 0.5\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cd1af-4317-40ec-9a0c-f2eb6342a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cyclegan.start_epoch != 0:\n",
    "    print(f\"Resuming training from epoch {cyclegan.start_epoch + 1}\")\n",
    "else:\n",
    "    print(\"Starting training from scratch\")\n",
    "\n",
    "if vis is not None:\n",
    "    loss_plot = MultiLinePlot(vis, \"Losses\", \"Epoch\", \"Loss\")\n",
    "\n",
    "for epoch in range(cyclegan.start_epoch + 1, epochs + 1):\n",
    "    cyclegan.G.train()\n",
    "    cyclegan.F.train()\n",
    "    \n",
    "    cyclegan.D_X.train()\n",
    "    cyclegan.D_Y.train()\n",
    "\n",
    "    batch_start_time = time.time()\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    cum_loss_idt_x = 0\n",
    "    cum_loss_idt_y = 0\n",
    "    cum_G_fool_loss = 0\n",
    "    cum_F_fool_loss = 0\n",
    "    cum_cycled_x_loss = 0\n",
    "    cum_cycled_y_loss = 0\n",
    "    cum_D_X_loss = 0\n",
    "    cum_D_Y_loss = 0\n",
    "    \n",
    "    ep_loss_idt_x = 0\n",
    "    ep_loss_idt_y = 0\n",
    "    ep_G_fool_loss = 0\n",
    "    ep_F_fool_loss = 0\n",
    "    ep_cycled_x_loss = 0\n",
    "    ep_cycled_y_loss = 0\n",
    "    ep_D_X_loss = 0\n",
    "    ep_D_Y_loss = 0\n",
    "    \n",
    "    for batch_no, (x, y) in enumerate(dataloader):\n",
    "        # Load the sequence of frames to the GPU\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Firstly, generate fake images in domain Y using the x_t images\n",
    "        fake_y = cyclegan.G(x)\n",
    "        \n",
    "        # Then generate the fake images in domain X using the y_t images\n",
    "        fake_x = cyclegan.F(y)\n",
    "        \n",
    "        ### GENERATOR TRAINING\n",
    "        \n",
    "        # Freeze discriminator weights\n",
    "        for param in cyclegan.D_X.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in cyclegan.D_Y.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Zero the gradients for the generators and predictors\n",
    "        cyclegan.G_opt.zero_grad()\n",
    "        cyclegan.F_opt.zero_grad()\n",
    "        \n",
    "        # Calculate the identity loss, this tries to enforce that G(y) = I(y) = y i.e. the identity\n",
    "        idt_x = cyclegan.F(x)\n",
    "        loss_idt_x = cyclegan.identity_loss(idt_x, x)\n",
    "        \n",
    "        idt_y = cyclegan.G(y)\n",
    "        loss_idt_y = cyclegan.identity_loss(idt_y, y)\n",
    "        \n",
    "        # Now we try and fool the discriminators\n",
    "        # D_X tries to tell if an image is from X (1) or from F(Y) (0) so it takes input fake_x_t\n",
    "        # D_X is supposed to be 1 if the image is from X but here we are trying to get it to be 1 if it is from G(X) -> Y which is incorrect\n",
    "        G_fool = cyclegan.D_Y(fake_y) \n",
    "        # We want to tell if fake_y_0 has fooled D_X, so we measure how far from the output 1 it is \n",
    "        G_fool_loss = cyclegan.gan_loss(G_fool, generate_noisy_labels(G_fool.shape, True, device))\n",
    "        \n",
    "        # D_Y tries to tell if an image is from Y (1) or from G(X) (0) so it takes input fake_y_t\n",
    "        F_fool = cyclegan.D_X(fake_x)\n",
    "        # We want to tell if fake_x_0 has fooled D_Y, so we measure how far from the output 1 it is \n",
    "        F_fool_loss = cyclegan.gan_loss(F_fool, generate_noisy_labels(F_fool.shape, True, device))\n",
    "        \n",
    "        # Now do the cycle loss\n",
    "        cycled_x = cyclegan.F(fake_y)\n",
    "        cycled_loss_x = cyclegan.cycle_loss(cycled_x, x)\n",
    "        \n",
    "        cycled_y = cyclegan.G(fake_x)\n",
    "        cycled_loss_y = cyclegan.cycle_loss(cycled_y, y)\n",
    "        \n",
    "        # Backpropagate and step the gradients\n",
    "        generator_loss = G_fool_loss + F_fool_loss + lambda_weight * (cycled_loss_x + cycled_loss_y + lambda_idt_X * loss_idt_x + lambda_idt_Y * loss_idt_y)\n",
    "        generator_loss.backward()\n",
    "        \n",
    "        cyclegan.G_opt.step()\n",
    "        cyclegan.F_opt.step()\n",
    "        \n",
    "        ### DISCRIMINATOR TRAINING\n",
    "        # Unfreeze discriminator weights\n",
    "        for param in cyclegan.D_X.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in cyclegan.D_Y.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Zero the gradients\n",
    "        cyclegan.D_X_opt.zero_grad()\n",
    "        cyclegan.D_Y_opt.zero_grad()\n",
    "        \n",
    "        D_X_loss = get_discriminator_loss(x, fake_x, cyclegan.fake_X_buffer, cyclegan.D_X, cyclegan.gan_loss)\n",
    "        \n",
    "        D_Y_loss = get_discriminator_loss(y, fake_y, cyclegan.fake_Y_buffer, cyclegan.D_Y, cyclegan.gan_loss)\n",
    "        \n",
    "        # Backpropagate and step the gradients\n",
    "        D_loss = D_X_loss + D_Y_loss\n",
    "        D_loss.backward()\n",
    "        \n",
    "        cyclegan.D_X_opt.step()\n",
    "        cyclegan.D_Y_opt.step()\n",
    "        \n",
    "        ### UPDATE POOLS\n",
    "        cyclegan.fake_X_buffer.add(fake_x.detach())\n",
    "        cyclegan.fake_Y_buffer.add(fake_y.detach())\n",
    "        \n",
    "        ### TRACK LOSS\n",
    "        cum_loss_idt_x += loss_idt_x.item()\n",
    "        cum_loss_idt_y += loss_idt_y.item()\n",
    "        cum_G_fool_loss += G_fool_loss.item()\n",
    "        cum_F_fool_loss += F_fool_loss.item()\n",
    "        cum_cycled_x_loss += cycled_loss_x.item()\n",
    "        cum_cycled_y_loss += cycled_loss_y.item()\n",
    "        cum_D_X_loss += D_X_loss.item()\n",
    "        cum_D_Y_loss += D_Y_loss.item()\n",
    "        \n",
    "        ep_loss_idt_x += loss_idt_x.item()\n",
    "        ep_loss_idt_y += loss_idt_y.item()\n",
    "        ep_G_fool_loss += G_fool_loss.item()\n",
    "        ep_F_fool_loss += F_fool_loss.item()\n",
    "        ep_cycled_x_loss += cycled_loss_x.item()\n",
    "        ep_cycled_y_loss += cycled_loss_y.item()\n",
    "        ep_D_X_loss += D_X_loss.item()\n",
    "        ep_D_Y_loss += D_Y_loss.item()\n",
    "        \n",
    "        if epoch == 0 and batch_no < 55:\n",
    "            print(f\"[{epoch}:{batch_no}] fake_X_buffer: {len(cyclegan.fake_X_buffer)}, fake_Y_buffer: {len(cyclegan.fake_Y_buffer)}\")\n",
    "        \n",
    "        if batch_no % 100 == 0 and batch_no != 0: \n",
    "            duration = time.time() - batch_start_time\n",
    "            \n",
    "            updated_losses = {\n",
    "                \"loss_idt_x\": cum_loss_idt_x / 100,\n",
    "                \"loss_idt_y\": cum_loss_idt_y / 100,\n",
    "                \"G_fool_loss\": cum_G_fool_loss / 100,\n",
    "                \"F_fool_loss\": cum_F_fool_loss / 100,\n",
    "                \"cycled_x_loss\": cum_cycled_x_loss / 100,\n",
    "                \"cycled_y_loss\": cum_cycled_y_loss / 100,\n",
    "                \"D_X_loss\": cum_D_X_loss / 100,\n",
    "                \"D_Y_loss\": cum_D_Y_loss / 100\n",
    "            }\n",
    "            \n",
    "            x_loss_str = f\"[{epoch}:{batch_no}]\"\n",
    "            y_loss_str = f\"[{epoch}:{batch_no}]\"\n",
    "            \n",
    "            for i, (key, value) in enumerate(updated_losses.items()):\n",
    "                if i % 2 == 0:\n",
    "                    x_loss_str = f\"{x_loss_str} {key}: {value},\"\n",
    "                else:\n",
    "                    y_loss_str = f\"{y_loss_str} {key}: {value},\"\n",
    "            \n",
    "            x_loss_str = x_loss_str[:-1]\n",
    "            y_loss_str = y_loss_str[:-1]\n",
    "\n",
    "            print(f\"[{epoch}:{batch_no}] Took {duration:.2f}s\")\n",
    "            print(x_loss_str)\n",
    "            print(y_loss_str)\n",
    "            print(f\"[{epoch}:{batch_no}] fake_X_buffer: {len(cyclegan.fake_X_buffer)}, fake_Y_buffer: {len(cyclegan.fake_Y_buffer)}\")\n",
    "            \n",
    "            if vis is not None:\n",
    "                loss_plot.append_values(epoch + batch_no / len(dataloader), updated_losses)\n",
    "            \n",
    "            cum_loss_idt_x = 0\n",
    "            cum_loss_idt_y = 0\n",
    "            cum_G_fool_loss = 0\n",
    "            cum_F_fool_loss = 0\n",
    "            cum_cycled_x_loss = 0\n",
    "            cum_cycled_y_loss = 0\n",
    "            cum_D_X_loss = 0\n",
    "            cum_D_Y_loss = 0\n",
    "\n",
    "            batch_start_time = time.time()\n",
    "    \n",
    "    print(f\"[{epoch}:END] Completed epoch in {time.time() - epoch_start_time}s\")\n",
    "    \n",
    "    print(f\"[{epoch}:{batch_no}]\", \n",
    "          f\"ep_loss_idt_x: {ep_loss_idt_x / len(dataloader):.3f}\", \n",
    "          f\"ep_G_fool_loss: {ep_G_fool_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_cycled_x_loss: {ep_cycled_x_loss / len(dataloader):.3f}\",\n",
    "          f\"ep_D_X_loss: {ep_D_X_loss / len(dataloader):.3f}\")\n",
    "    \n",
    "    print(f\"[{epoch}:{batch_no}]\", \n",
    "          f\"ep_loss_idt_y: {ep_loss_idt_y / len(dataloader):.3f}\", \n",
    "          f\"ep_F_fool_loss: {ep_F_fool_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_cycled_y_loss: {ep_cycled_y_loss / len(dataloader):.3f}\",\n",
    "          f\"ep_D_Y_loss: {ep_D_Y_loss / len(dataloader):.3f}\")\n",
    "    \n",
    "    cyclegan.G.eval()\n",
    "    cyclegan.F.eval()\n",
    "    \n",
    "    if vis is not None:\n",
    "        eval_start_time = time.time()\n",
    "\n",
    "        G_eval_forward = cyclegan.apply(test_xs, x_to_y=True)\n",
    "        F_eval_forward = cyclegan.apply(test_ys, x_to_y=False)\n",
    "\n",
    "        G_rev = cyclegan.apply(G_eval_forward, x_to_y=False)\n",
    "        F_rev = cyclegan.apply(F_eval_forward, x_to_y=True)\n",
    "\n",
    "        G_eval_forward = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in G_eval_forward])\n",
    "        F_eval_forward = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in F_eval_forward])\n",
    "        G_rev = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in G_rev])\n",
    "        F_rev = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in F_rev])\n",
    "\n",
    "        G_eval_grid = torchvision.utils.make_grid(G_eval_forward, nrow=4)\n",
    "        F_eval_grid = torchvision.utils.make_grid(F_eval_forward, nrow=4)\n",
    "        G_rev_grid = torchvision.utils.make_grid(G_rev, nrow=4)\n",
    "        F_rev_grid = torchvision.utils.make_grid(F_rev, nrow=4)\n",
    "        \n",
    "        folder = f\"{cyclegan.save_folder}/{epoch}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        \n",
    "        torchvision.utils.save_image(G_eval_grid, f\"{folder}/X_to_Y.png\")\n",
    "        torchvision.utils.save_image(F_eval_grid, f\"{folder}/Y_to_X.png\")\n",
    "        torchvision.utils.save_image(G_rev_grid, f\"{folder}/X_to_Y_to_X.png\")\n",
    "        torchvision.utils.save_image(F_rev_grid, f\"{folder}/Y_to_X_to_Y.png\")\n",
    "\n",
    "        vis.image(G_eval_grid, win=\"G_eval\", opts={\n",
    "            \"caption\": f\"X -> Y evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(F_eval_grid, win=\"F_eval\", opts={\n",
    "            \"caption\": f\"Y -> X evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(G_rev_grid, win=\"G_rev\", opts={\n",
    "            \"caption\": f\"X -> Y -> X evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(F_rev_grid, win=\"F_rev\", opts={\n",
    "            \"caption\": f\"Y -> X -> Y evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        print(f\"[{epoch}:END] Completed eval in {time.time() - eval_start_time}s\")\n",
    "\n",
    "    cyclegan.G.train()\n",
    "    cyclegan.F.train()\n",
    "\n",
    "    cyclegan.step_learning_rates()\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"[{epoch}:END] Saving models and training information permanently\")\n",
    "        cyclegan.save(epoch, full_save=True)\n",
    "        cyclegan.save(epoch, full_save=True, folder=\"latest\")\n",
    "    else:\n",
    "        print(f\"[{epoch}:END] Saving models and training information temporarily to latest and saving generators permanently\")\n",
    "        cyclegan.save(epoch, full_save=True, folder=\"latest\")\n",
    "        cyclegan.save(epoch, full_save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8dbcb-a37c-4cf8-9f39-67f732e93bd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877f5b3-81d8-4927-b150-727b29ead81c",
   "metadata": {},
   "source": [
    "#### Segmentation Masks\n",
    "I start by using the test datasets with Mask-RCNN, the process is as follows:\n",
    "\n",
    "1. Use Mask-RCNN to get the predicted segmentation map for each test image\n",
    "2. Cycle each image such that an image from domain X is cycled from X->Y->X\n",
    "3. Use Mask-RCNN to get the predicted segmentation map for each cycled image\n",
    "4. Compute the per-pixel accuracy\n",
    "\n",
    "I repeat this for both domains to evaluate in both directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd8a84-b3ca-4942-b14e-a334eccbe7bf",
   "metadata": {},
   "source": [
    "I begin by cycling the images from each test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1855afe1-ec3a-489d-8224-7a99842ad031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_transfer_folder = \"./extracted_data/cyclegan_transferred\"\n",
    "test_input_folder = lambda base: f\"./extracted_data/cyclegan_training_data/{base}/Test\"\n",
    "\n",
    "model_parent_folder = \"./runs/CycleGAN/1680981254.7780375\"\n",
    "game_to_movie_path = f\"{model_parent_folder}/latest/G.pth\"\n",
    "movie_to_game_path = f\"{model_parent_folder}/latest/F.pth\"\n",
    "\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ed95801-9c6c-4043-8e7b-5913577cc89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "def transfer_style_to_batch(cv2_images, model):\n",
    "    imgs = [preprocess_transform(img) for img in cv2_images]\n",
    "    imgs = torch.stack(imgs).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        imgs_transferred = model(imgs)\n",
    "    \n",
    "    imgs_transferred = [revert_normalisation(img_t.cpu()) for img_t in imgs_transferred]\n",
    "    return imgs_transferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "99231acb-81e1-4b1b-8183-ac73890bf55f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706d46a3eea946e48c24a1b7132a2be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fddf7aafcc04441a282f157c6f778f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(f\"{model_parent_folder}/info_None.json\", \"r\") as fp:\n",
    "    model_info = json.load(fp)\n",
    "\n",
    "upsample_strategy = model_info[\"upsample_strategy\"]\n",
    "block_count = model_info[\"block_count\"]\n",
    "\n",
    "for base in [\"Game\", \"Movie\"]:\n",
    "    other_domain = \"Movie\" if base == \"Game\" else \"Game\"\n",
    "    model_path =  game_to_movie_path if base == \"Game\" else movie_to_game_path\n",
    "    test_folder = test_input_folder(base)\n",
    "    transferred_folder = f\"{base_transfer_folder}/{base}_to_{other_domain}\"\n",
    "    \n",
    "    os.makedirs(transferred_folder)\n",
    "    \n",
    "    model = Generator(block_count, upsample_strategy).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    file_names = [file_name for file_name in os.listdir(test_folder) if file_name.endswith(\".jpg\")]\n",
    "    batch_count = len(file_names) // batch_size + 1\n",
    "    \n",
    "    for batch_no in tqdm(range(batch_count)):\n",
    "        selected_files = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            real_i = batch_size * batch_no + i\n",
    "            \n",
    "            if real_i >= len(file_names):\n",
    "                break\n",
    "            \n",
    "            selected_files.append(file_names[real_i])\n",
    "        \n",
    "        if len(selected_files) == 0:\n",
    "            break\n",
    "            \n",
    "        batch_images = [cv2.cvtColor(cv2.imread(f\"{test_folder}/{file_name}\"), cv2.COLOR_BGR2RGB) for file_name in selected_files]\n",
    "        transferred = transfer_style_to_batch(batch_images, model)\n",
    "        \n",
    "        for file_name, transferred_image in zip(selected_files, transferred):\n",
    "            cv2.imwrite(f\"{transferred_folder}/{''.join(file_name.split('.')[:-1])}_transferred.jpg\", cv2.cvtColor((transferred_image.numpy() * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ce980d01-e515-45f5-a194-c998ecee9029",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2465719686dd4b4399756f82eccea110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f60e593967e4999b1ebbc31e0f85ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(f\"{model_parent_folder}/info_None.json\", \"r\") as fp:\n",
    "    model_info = json.load(fp)\n",
    "\n",
    "upsample_strategy = model_info[\"upsample_strategy\"]\n",
    "block_count = model_info[\"block_count\"]\n",
    "\n",
    "for base in [\"Game\", \"Movie\"]:\n",
    "    other_domain = \"Movie\" if base == \"Game\" else \"Game\"\n",
    "    model_path =  movie_to_game_path if base == \"Game\" else game_to_movie_path\n",
    "    test_folder = f\"{base_transfer_folder}/{base}_to_{other_domain}\"\n",
    "    transferred_folder = f\"{base_transfer_folder}/{base}_to_{other_domain}_to_{base}\"\n",
    "    \n",
    "    os.makedirs(transferred_folder)\n",
    "    \n",
    "    model = Generator(block_count, upsample_strategy).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    file_names = [file_name for file_name in os.listdir(test_folder) if file_name.endswith(\".jpg\")]\n",
    "    batch_count = len(file_names) // batch_size + 1\n",
    "    \n",
    "    for batch_no in tqdm(range(batch_count)):\n",
    "        selected_files = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            real_i = batch_size * batch_no + i\n",
    "            \n",
    "            if real_i >= len(file_names):\n",
    "                break\n",
    "            \n",
    "            selected_files.append(file_names[real_i])\n",
    "        \n",
    "        if len(selected_files) == 0:\n",
    "            break\n",
    "            \n",
    "        batch_images = [cv2.cvtColor(cv2.imread(f\"{test_folder}/{file_name}\"), cv2.COLOR_BGR2RGB) for file_name in selected_files]\n",
    "        transferred = transfer_style_to_batch(batch_images, model)\n",
    "        \n",
    "        for file_name, transferred_image in zip(selected_files, transferred):\n",
    "            cv2.imwrite(f\"{transferred_folder}/{''.join(file_name.split('.')[:-1])}_transferred.jpg\", cv2.cvtColor((transferred_image.numpy() * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199179a2-7e58-4d80-a5cf-6d5fcac57fb9",
   "metadata": {},
   "source": [
    "Now I pair the original image with the cycled image and compute their segmentation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "34a3de8e-8c98-45b1-89ba-1edfbadeaad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_game_folder = \"./extracted_data/cyclegan_training_data/Game/Test\"\n",
    "cycled_game_folder = \"./extracted_data/cyclegan_transferred/Game_to_Movie_to_Game\"\n",
    "original_game_files = [f\"{original_game_folder}/{file_name}\" for file_name in os.listdir(original_game_folder) if file_name.endswith(\".jpg\")]\n",
    "cycled_game_files = [f\"{cycled_game_folder}/{file_name}\" for file_name in os.listdir(cycled_game_folder) if file_name.endswith(\".jpg\")]\n",
    "\n",
    "original_movie_folder = \"./extracted_data/cyclegan_training_data/Movie/Test\"\n",
    "cycled_movie_folder = \"./extracted_data/cyclegan_transferred/Movie_to_Game_to_Movie\"\n",
    "original_movie_files = [f\"{original_movie_folder}/{file_name}\" for file_name in os.listdir(original_movie_folder) if file_name.endswith(\".jpg\")]\n",
    "cycled_movie_files = [f\"{cycled_movie_folder}/{file_name}\" for file_name in os.listdir(cycled_movie_folder) if file_name.endswith(\".jpg\")]\n",
    "\n",
    "threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "84134521-daa8-4e35-9384-f97967d3d374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "maskrcnn.to(device).eval()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "33591941-6f4b-479c-b7d7-1ace0c1df1a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 91 COCO class names, directly from Semantic Segmentation Mask R-CNN.ipynb\n",
    "coco_names = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "15ba50f6-6203-4f3a-b118-eaeaef703f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_segmentation_map(output, frame, threshold):\n",
    "    # Get the relevant model output on to the CPU in a usable format\n",
    "    scores = output[\"scores\"].detach().cpu().numpy()\n",
    "    boxes = output[\"boxes\"].detach().cpu().numpy().astype(int)\n",
    "    label_indices = output[\"labels\"].detach().cpu()\n",
    "    masks = (output[\"masks\"] > 0.5).detach().cpu()\n",
    "    \n",
    "    segmentation_maps = {label: np.zeros(frame.shape[:2]).astype(np.uint8) for label in coco_names[1:]}\n",
    "    \n",
    "    # Process each entry found by the model\n",
    "    for i, (confidence, box, label_idx, mask) in enumerate(zip(scores, boxes, label_indices, masks)):\n",
    "        label = coco_names[label_idx.item()]\n",
    "        \n",
    "        # The confidences are sorted high to low so stop once we're below the threshold\n",
    "        if confidence < threshold:\n",
    "            break\n",
    "        \n",
    "        segmentation_maps[label][(mask == 1).squeeze()] = 1\n",
    "    \n",
    "    return segmentation_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "fab73127-6209-48a6-b35d-dd9765ce84a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def measure_segmentation_map_similarity(original_files, cycled_files):\n",
    "    tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mean_accuracies = []\n",
    "    \n",
    "    for original_file_name, cycled_file_name in tqdm(zip(original_files, cycled_files), total=len(original_files)):\n",
    "        original_file = cv2.imread(original_file_name, cv2.IMREAD_COLOR)\n",
    "        cycled_file = cv2.imread(cycled_file_name, cv2.IMREAD_COLOR)\n",
    "        maskrcnn_batch = torch.stack([tensor_transform(original_file), tensor_transform(cycled_file)]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = maskrcnn(maskrcnn_batch)\n",
    "            \n",
    "        original_seg_maps = retrieve_segmentation_map(outputs[0], original_file, threshold)\n",
    "        cycled_seg_maps = retrieve_segmentation_map(outputs[1], cycled_file, threshold)\n",
    "        \n",
    "        ins_accuracies = []\n",
    "        \n",
    "        for label in coco_names[1:]:\n",
    "            original_seg_map = original_seg_maps[label]\n",
    "            cycled_seg_map = cycled_seg_maps[label]\n",
    "            \n",
    "            assert original_seg_map.shape == cycled_seg_map.shape\n",
    "            \n",
    "            if not original_seg_map.any() and not cycled_seg_map.any():\n",
    "                continue\n",
    "            \n",
    "            correct_pixels = ((original_seg_map == 1) & (cycled_seg_map == 1)).sum()\n",
    "            total_pixels = (original_seg_map == 1).sum()\n",
    "            \n",
    "#             dupe_frame = original_file.copy()\n",
    "#             y = np.stack([original_seg_map, original_seg_map, original_seg_map], axis=2).astype(np.uint8)\n",
    "#             cv2.addWeighted(dupe_frame, 1.0, y * 200, 0.6, 0.0, dupe_frame)\n",
    "#             cv2.imwrite(f\"./test_{time.time()}_{label}_org.png\", dupe_frame)\n",
    "            \n",
    "#             dupe_frame = cycled_file.copy()\n",
    "#             y = np.stack([cycled_seg_map, cycled_seg_map, cycled_seg_map], axis=2).astype(np.uint8)\n",
    "#             cv2.addWeighted(dupe_frame, 1.0, y * 200, 0.6, 0.0, dupe_frame)\n",
    "#             cv2.imwrite(f\"./test_{time.time()}_{label}_cycled.png\", dupe_frame)\n",
    "            \n",
    "            if total_pixels == 0:\n",
    "                continue\n",
    "    \n",
    "            accuracy = correct_pixels / total_pixels\n",
    "            ins_accuracies.append(accuracy)\n",
    "            \n",
    "        if len(ins_accuracies) == 0:\n",
    "            continue\n",
    "        \n",
    "        mean_accuracy = sum(ins_accuracies) / len(ins_accuracies)\n",
    "        mean_accuracies.append(mean_accuracy)\n",
    "    \n",
    "    return sum(mean_accuracies) / len(mean_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "b9f982ba-5ba9-4313-ae44-2e33c65f13cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcacdc6471c0494f8a2ed781762430f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./extracted_data/cyclegan_training_data/Game/Test/00001_00008.jpg ./extracted_data/cyclegan_transferred/Game_to_Movie_to_Game/00001_00008_transferred_transferred.jpg\n",
      "./extracted_data/cyclegan_training_data/Game/Test/00001_00039.jpg ./extracted_data/cyclegan_transferred/Game_to_Movie_to_Game/00001_00039_transferred_transferred.jpg\n",
      "./extracted_data/cyclegan_training_data/Game/Test/00001_00063.jpg ./extracted_data/cyclegan_transferred/Game_to_Movie_to_Game/00001_00063_transferred_transferred.jpg\n",
      "./extracted_data/cyclegan_training_data/Game/Test/00001_00150.jpg ./extracted_data/cyclegan_transferred/Game_to_Movie_to_Game/00001_00150_transferred_transferred.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[354], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m game_fcn \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_segmentation_map_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_game_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycled_game_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[353], line 12\u001b[0m, in \u001b[0;36mmeasure_segmentation_map_similarity\u001b[1;34m(original_files, cycled_files)\u001b[0m\n\u001b[0;32m      9\u001b[0m maskrcnn_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([tensor_transform(original_file), tensor_transform(cycled_file)])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmaskrcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaskrcnn_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m original_seg_maps \u001b[38;5;241m=\u001b[39m retrieve_segmentation_map(outputs[\u001b[38;5;241m0\u001b[39m], original_file, threshold)\n\u001b[0;32m     15\u001b[0m cycled_seg_maps \u001b[38;5;241m=\u001b[39m retrieve_segmentation_map(outputs[\u001b[38;5;241m1\u001b[39m], cycled_file, threshold)\n",
      "File \u001b[1;32mF:\\Documents\\Development\\GitHub\\advanced-computer-vision-y4\\code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mF:\\Documents\\Development\\GitHub\\advanced-computer-vision-y4\\code\\env\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Documents\\Development\\GitHub\\advanced-computer-vision-y4\\code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mF:\\Documents\\Development\\GitHub\\advanced-computer-vision-y4\\code\\env\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py:374\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    370\u001b[0m objectness, pred_bbox_deltas \u001b[38;5;241m=\u001b[39m concat_box_prediction_layers(objectness, pred_bbox_deltas)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# apply pred_bbox_deltas to anchors to obtain the decoded proposals\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# the proposals\u001b[39;00m\n\u001b[1;32m--> 374\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_coder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bbox_deltas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m proposals \u001b[38;5;241m=\u001b[39m proposals\u001b[38;5;241m.\u001b[39mview(num_images, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    376\u001b[0m boxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_proposals(proposals, objectness, images\u001b[38;5;241m.\u001b[39mimage_sizes, num_anchors_per_level)\n",
      "File \u001b[1;32mF:\\Documents\\Development\\GitHub\\advanced-computer-vision-y4\\code\\env\\lib\\site-packages\\torchvision\\models\\detection\\_utils.py:178\u001b[0m, in \u001b[0;36mBoxCoder.decode\u001b[1;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    177\u001b[0m     rel_codes \u001b[38;5;241m=\u001b[39m rel_codes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 178\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    180\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m pred_boxes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mF:\\Documents\\Development\\GitHub\\advanced-computer-vision-y4\\code\\env\\lib\\site-packages\\torchvision\\models\\detection\\_utils.py:216\u001b[0m, in \u001b[0;36mBoxCoder.decode_single\u001b[1;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[0;32m    213\u001b[0m pred_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(dh) \u001b[38;5;241m*\u001b[39m heights[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Distance from center to box's corner.\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m c_to_c_h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_ctr_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m pred_h\n\u001b[0;32m    217\u001b[0m c_to_c_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mpred_ctr_x\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mpred_w\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m pred_w\n\u001b[0;32m    219\u001b[0m pred_boxes1 \u001b[38;5;241m=\u001b[39m pred_ctr_x \u001b[38;5;241m-\u001b[39m c_to_c_w\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game_fcn = measure_segmentation_map_similarity(original_game_files, cycled_game_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "76d56e66-42a1-4924-980d-65039005114d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11668321322467086"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "c97258b8-9325-4b6e-9988-429ed75a07d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b16fa7ec5947099e142ac1d8cee45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movie_fcn = measure_segmentation_map_similarity(original_movie_files, cycled_movie_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "5c8bfc36-ce5e-4a82-b6d0-b25cb9b50293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24343726588524994"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_fcn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e07c68-8237-49f2-82ff-33cae1e92f64",
   "metadata": {},
   "source": [
    "#### FID Score\n",
    "Frechlet Inception Distance (FID) is a metric that compares images from the same domain. We can use it to compare real images and fake images that have been style transferred to the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e156aacc-be69-4ba4-bbc0-fc47043442eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cleanfid import fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ee8e5dc7-2a63-4a86-9011-6fd3ef5f05e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID between two folders\n",
      "Found 212 images in the folder ./extracted_data/cyclegan_training_data/Movie/Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID Test : 100%|█████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images in the folder ./extracted_data/cyclegan_transferred/Game_to_Movie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID Game_to_Movie : 100%|██████████████████████████████████████████████████████████████| 16/16 [00:04<00:00,  3.66it/s]\n"
     ]
    }
   ],
   "source": [
    "score = fid.compute_fid(test_input_folder(\"Movie\"), \"./extracted_data/cyclegan_transferred/Game_to_Movie\", num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "364be512-4df4-4995-a72e-beee52cc17f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207.76521902759873"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "90d5db94-d105-4887-ab04-cdab640b4177",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID between two folders\n",
      "Found 500 images in the folder ./extracted_data/cyclegan_training_data/Game/Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID Test : 100%|███████████████████████████████████████████████████████████████████████| 16/16 [00:03<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 212 images in the folder ./extracted_data/cyclegan_transferred/Movie_to_Game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID Movie_to_Game : 100%|████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "score = fid.compute_fid(test_input_folder(\"Game\"), \"./extracted_data/cyclegan_transferred/Movie_to_Game\", num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1325cfab-ad2e-40b5-bffd-09785a22623c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207.4777962289672"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d6d6b-fae0-4c73-b560-f026ed9904cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Video Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0163f7f0-9dcc-49fe-b606-2101bea4e60a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_video(video_path, save_loc, model, batch_size=4):\n",
    "    # Try to delete the existing saved file\n",
    "    try:\n",
    "        os.remove(save_loc)\n",
    "    except OSError:\n",
    "        pass \n",
    "    \n",
    "    video = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    success, frame = video.read()\n",
    "    height, width, _ = frame.shape\n",
    "    \n",
    "    frame_buffer = []\n",
    "    video_out = cv2.VideoWriter(save_loc, -1, fps, (width, height))\n",
    "    \n",
    "    with tqdm(total=total_frames) as progress:\n",
    "        while success:\n",
    "            frame_buffer.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            if len(frame_buffer) == batch_size:\n",
    "                transferred_frames = transfer_style_to_batch(frame_buffer, model)\n",
    "\n",
    "                for out_frame in transferred_frames:\n",
    "                    coloured_out_frame = cv2.cvtColor((out_frame.numpy() * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "                    video_out.write(coloured_out_frame)\n",
    "\n",
    "                frame_buffer = []\n",
    "\n",
    "            success, frame = video.read()\n",
    "            progress.update(1)\n",
    "    \n",
    "    if len(frame_buffer) != 0:\n",
    "        # Process any additional final frames that don't make a full batch\n",
    "        transferred_frames = transfer_style_to_batch(frame_buffer, model)\n",
    "\n",
    "        for out_frame in transferred_frames:\n",
    "            coloured_out_frame = cv2.cvtColor((out_frame.numpy() * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "            video_out.write(coloured_out_frame)\n",
    "    \n",
    "    video_out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8ab9e22d-f9c6-4f4e-bd32-866e43d164cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_time = \"1680981254.7780375\"\n",
    "model_parent_directory = f\"./runs/CycleGAN/{model_time}\"\n",
    "epoch_directory = \"latest\"\n",
    "model_name = \"F.pth\" # F: Y -> X which is the style transfer we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0265f3d6-ba8b-45e7-b26a-60668fee48e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{model_parent_directory}/info_None.json\", \"r\") as fp:\n",
    "    model_info = json.load(fp)\n",
    "\n",
    "upsample_strategy = model_info[\"upsample_strategy\"]\n",
    "block_count = model_info[\"block_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "81de7445-e038-41ed-90e9-3bba84193397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Generator(block_count, upsample_strategy).to(device)\n",
    "model.load_state_dict(torch.load(f\"{model_parent_directory}/{epoch_directory}/{model_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0abb0ca0-fb9e-4f1b-a5ed-4c5c169c44d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_video_path = \"./original_data/Test/Test Movie.mp4\"\n",
    "output_video_path = f\"./video_transfer/cyclegan_{model_time}.mp4\"\n",
    "batch_size = 4\n",
    "\n",
    "os.makedirs(\"./video_transfer\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "58bae42e-83e9-4026-b02b-03bdee9bfffb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af066816e284740a8635cf878da3419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_video(input_video_path, output_video_path, model, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627c870-ef8e-4dc9-9cab-73ce48b3db99",
   "metadata": {},
   "source": [
    "## 2.2: Local (temporal) Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44cf89-80a9-4d27-9cf1-d7945b9fa747",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Generation\n",
    "Unlike CycleGAN, the data generated in Question 1 cannot be used directly with RecycleGAN. I firstly need to process the data from Q1.2 to create triplets. These are 3 consecutive human patches concatenated together. There is no overlap between triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2b10ce9-f7cf-4309-a96c-f72b0744ea21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236705e1b5794347b71f7d56339f3d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36480e700be84eb49e2e14a3ff8a6606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f56ec7e57c446c4a700caa7db9437ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb790c74f1b241de83e9cda23bb5b8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4feb2120f447bca9186968261fffa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f04c7ec7c54ce3974a3a37034b0eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557288e84edd40a0af8aa913c6a93abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429996ef59e94818b615e8bd595be1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = [\"Full Body Sitting\", \"Full Body Standing\", \"Half Body\", \"Head Only\"]\n",
    "base_pose_directory = \"./extracted_data/classified_human_patches\"\n",
    "cutoff = 8\n",
    "window = 5\n",
    "\n",
    "game_groups = group_classes(f\"{base_pose_directory}/Train/Game\", classes, cutoff, window) \n",
    "movie_groups = group_classes(f\"{base_pose_directory}/Train/Movie\", classes, cutoff, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "816db814-e3b7-4949-bb49-c7cc7ce93d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_test_count = 250\n",
    "target_train_count = 600\n",
    "min_triplets_from_clip = 3\n",
    "max_triplets_from_clip = 5\n",
    "output_size = 256\n",
    "min_size = 96\n",
    "\n",
    "train_output_directory = lambda base: f\"./extracted_data/recyclegan_training_data/{base}/Train\"\n",
    "test_output_directory = lambda base: f\"./extracted_data/recyclegan_training_data/{base}/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a036bb7-8a95-4ae4-9c5b-d8dbec54431f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_available_triplet_lengths(groups, min_triplets_from_clip):\n",
    "    lengths = {}\n",
    "    min_length = min_triplets_from_clip * 3\n",
    "    excluded_count = 0\n",
    "\n",
    "    for group in groups:\n",
    "        l = len(group)\n",
    "\n",
    "        if l < min_length:\n",
    "            excluded_count += l\n",
    "            continue\n",
    "\n",
    "        if l not in lengths.keys():\n",
    "            lengths[l] = 0\n",
    "\n",
    "        lengths[l] += 1\n",
    "    \n",
    "    return lengths, excluded_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c34b137-5f3b-4d4f-919a-946dc7db3d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_recyclegan_train_test_split(groups, train_dir, test_dir, train_count, test_count, min_triplets_from_clip, max_triplets_from_clip):\n",
    "    os.makedirs(train_dir)\n",
    "    os.makedirs(test_dir)\n",
    "    \n",
    "    lengths, excluded_count = compute_available_triplet_lengths(groups, min_triplets_from_clip)\n",
    "    test_save_prob = test_count / excluded_count\n",
    "    min_length = min_triplets_from_clip * 3\n",
    "    \n",
    "    for group_no, group in enumerate(tqdm(groups)):\n",
    "        if len(group) < min_length:\n",
    "            for i in range(len(group)):\n",
    "                if random.uniform(0, 1) < test_save_prob:\n",
    "                    Image.open(group[i]).save(f\"{test_dir}/{group_no:05d}_{i:05d}.jpg\")\n",
    "\n",
    "            continue \n",
    "        \n",
    "        triplet_count = len(group) // 3\n",
    "        selection_count = min(triplet_count, max_triplets_from_clip)\n",
    "        selected_triplets = random.sample(range(triplet_count), k=selection_count)\n",
    "        \n",
    "        for i in selected_triplets:\n",
    "            triplet_image = Image.new(\"RGB\", (3 * output_size, output_size))\n",
    "\n",
    "            x = Image.open(group[3 * i])\n",
    "            y = Image.open(group[3 * i + 1])\n",
    "            z = Image.open(group[3 * i + 2])\n",
    "\n",
    "            if x.width < min_size or x.height < min_size:\n",
    "                continue\n",
    "\n",
    "            if y.width < min_size or y.height < min_size:\n",
    "                continue\n",
    "\n",
    "            if z.width < min_size or z.height < min_size:\n",
    "                continue\n",
    "\n",
    "            triplet_image.paste(x.resize((output_size, output_size)), (0, 0))\n",
    "            triplet_image.paste(y.resize((output_size, output_size)), (output_size, 0))\n",
    "            triplet_image.paste(z.resize((output_size, output_size)), (2 * output_size, 0))\n",
    "\n",
    "            triplet_image.save(f\"{train_dir}/{group_no:05d}_{i:05d}.jpg\")\n",
    "    \n",
    "    valid_input_files = [file_name for file_name in os.listdir(train_dir) if file_name.endswith(\".jpg\")]\n",
    "    print(\"Uncapped Train Size:\", len(valid_input_files))\n",
    "    \n",
    "    if len(valid_input_files) > train_count:\n",
    "        selected_for_delete = random.sample(valid_input_files, k=len(valid_input_files) - train_count)\n",
    "\n",
    "        for file_name in selected_for_delete:\n",
    "            os.remove(f\"{train_dir}/{file_name}\")\n",
    "\n",
    "        print(\"Deleted\", len(selected_for_delete), \"files\")\n",
    "    \n",
    "    print(\"Capped Train Size:\", len([file_name for file_name in os.listdir(train_dir) if file_name.endswith(\".jpg\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "114cec97-7ac9-48e7-86d1-050b7d99db04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: './extracted_data/recyclegan_training_data/Game/Train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcreate_recyclegan_train_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_output_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_output_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_train_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_test_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_triplets_from_clip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_triplets_from_clip\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 2\u001b[0m, in \u001b[0;36mcreate_recyclegan_train_test_split\u001b[1;34m(groups, train_dir, test_dir, train_count, test_count, min_triplets_from_clip, max_triplets_from_clip)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_recyclegan_train_test_split\u001b[39m(groups, train_dir, test_dir, train_count, test_count, min_triplets_from_clip, max_triplets_from_clip):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(test_dir)\n\u001b[0;32m      5\u001b[0m     lengths, excluded_count \u001b[38;5;241m=\u001b[39m compute_available_triplet_lengths(groups, min_triplets_from_clip)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: './extracted_data/recyclegan_training_data/Game/Train'"
     ]
    }
   ],
   "source": [
    "create_recyclegan_train_test_split(game_groups, train_output_directory(\"Game\"), test_output_directory(\"Game\"), target_train_count, target_test_count, min_triplets_from_clip, max_triplets_from_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2c27f2d7-cfe1-4787-a439-b941fdbd4729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9125c773c432403daec37d8ba5d50164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncapped Train Size: 689\n",
      "Deleted 89 files\n",
      "Capped Train Size: 600\n"
     ]
    }
   ],
   "source": [
    "create_recyclegan_train_test_split(movie_groups, train_output_directory(\"Movie\"), test_output_directory(\"Movie\"), target_train_count, target_test_count, min_triplets_from_clip, max_triplets_from_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a0033-fdea-409f-8969-977ba273d586",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e26624e2-4f41-4674-b40e-2b08380f0325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using_google_gpus = False\n",
    "\n",
    "train_X_loc = \"./extracted_data/recyclegan_training_data/Game/Train\" \n",
    "test_X_loc = \"./extracted_data/recyclegan_training_data/Game/Test\" \n",
    "\n",
    "train_Y_loc = \"./extracted_data/recyclegan_training_data/Movie/Train\" \n",
    "test_Y_loc = \"./extracted_data/recyclegan_training_data/Movie/Test\" \n",
    "\n",
    "run_data_directory = \"./runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49bb616-39f9-49d2-9f69-ec617524b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_google_gpus:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    vis = None\n",
    "else:\n",
    "    import visdom\n",
    "    vis = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cb104-c5c2-45f8-828e-190864efe112",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/cyclegan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a0d6745-5b4c-4393-b14f-f785d696ce17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "JointDomainImageDataset.__init__() missing 1 required positional argument: 'load_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m JointDomainTripletDataset(train_X_loc, train_Y_loc, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mJointDomainImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_X_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_Y_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: JointDomainImageDataset.__init__() missing 1 required positional argument: 'load_size'"
     ]
    }
   ],
   "source": [
    "train_dataset = JointDomainTripletDataset(train_X_loc, train_Y_loc, train=True, img_size=256)\n",
    "test_dataset = JointDomainImageDataset(test_X_loc, test_Y_loc, train=False, img_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f3307-79a8-4b5a-8a59-509326530f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xs = []\n",
    "test_ys = []\n",
    "\n",
    "for i in random.sample(range(0, len(test_dataset)), 16):\n",
    "    x, y = test_dataset[i]\n",
    "    test_xs.append(x)\n",
    "    test_ys.append(y)\n",
    "\n",
    "test_xs = torch.stack(test_xs)\n",
    "test_ys = torch.stack(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53467f-44bb-44a5-b5cc-d0044cff8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "if vis is not None:\n",
    "    vis.images(torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in test_xs]), nrow=4, opts={\"title\": \"X_test originals\"})\n",
    "    vis.images(torch.stack([revert_normalisation(y).permute(2, 0, 1) for y in test_ys]), nrow=4, opts={\"title\": \"Y_test originals\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04515947-c0d7-4c18-9bef-e13c1c346751",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = test_dataset[random.randint(0, len(test_dataset))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8e894-58f5-4ffc-9e57-8b696be0d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_img.min(), test_img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e79b33-6f23-483e-99ac-d9e5b0255d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e8195-d39d-4d75-9a5e-9afbb456fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(revert_normalisation(test_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a713bdd-a544-45a1-988c-d2449219011d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeadc15-172b-4213-8dd7-595c6c1e04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 100\n",
    "lambda_weight = 5\n",
    "lambda_idt_X = 0.5\n",
    "lambda_idt_Y = 0.5\n",
    "\n",
    "blocks = 6\n",
    "upsample_strategy = [\"upsample\", \"conv_transpose\", \"pixel_shuffle\"][0]\n",
    "pool_size = 20\n",
    "opt_scheduler_type = \"linear_decay_with_warmup\"\n",
    "\n",
    "checkpoint_instance_dir = None\n",
    "checkpoint_epoch_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658f697-ca61-4a6f-ba1f-abd9cc485f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c31e2-fe43-420a-ac7f-094a3559ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint_instance_dir is not None and checkpoint_epoch_dir is not None:\n",
    "    cyclegan = RecycleGAN.load(f\"{run_data_directory}/{checkpoint_instance_dir}\", f\"{checkpoint_epoch_dir}\", device, blocks)\n",
    "else:\n",
    "    cyclegan = RecycleGAN(blocks, upsample_strategy, device, pool_size, opt_scheduler_type, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442bcbf-8ac7-4a62-a1a1-31bc35b7d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{cyclegan.save_folder}/info_{checkpoint_epoch_dir}.json\", \"w+\") as fp:\n",
    "    json.dump({\n",
    "        \"block_count\": cyclegan.resnet_block_count,\n",
    "        \"upsample_strategy\": upsample_strategy,\n",
    "        \"pool_size\": pool_size,\n",
    "        \"opt_scheduler_type\": opt_scheduler_type,\n",
    "        \"data_folders\": {\n",
    "            \"train_X\": train_X_loc,\n",
    "            \"test_X\": test_X_loc,\n",
    "            \"train_Y\": train_Y_loc,\n",
    "            \"test_Y\": test_Y_loc\n",
    "        },\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_epochs\": epochs,\n",
    "        \"start_epoch\": cyclegan.start_epoch,\n",
    "        \"lambda_weight\": lambda_weight,\n",
    "        \"lambda_idt_X\": lambda_idt_X,\n",
    "        \"lambda_idt_Y\": lambda_idt_Y,\n",
    "        \"checkpoint\": {\n",
    "            \"instance\": checkpoint_instance_dir,\n",
    "            \"epoch\": checkpoint_epoch_dir\n",
    "        }\n",
    "    }, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d092986-28a4-48d2-8f29-a8eacb55eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could also enable random flipping\n",
    "def generate_noisy_labels(shape, real, device):\n",
    "    # Randomly generated between 0 and 1\n",
    "    labels = torch.rand(shape, device=device)\n",
    "    \n",
    "    if real:\n",
    "        # Now they are between 0.7 and 1.1\n",
    "        labels = (2 * labels / 5) + 0.7\n",
    "    else:\n",
    "        # Now they are between 0 and 0.3\n",
    "        labels = (labels * 3) / 10\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8713a-f86c-4c3a-894b-c12a4a5e97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_loss(real, fake, pool, discriminator, loss_func):\n",
    "    # Discriminator should give (1) for a real image and (0) for a fake\n",
    "    real_pred = discriminator(real)\n",
    "    real_loss = loss_func(real_pred, generate_noisy_labels(real_pred.shape, True, device)) # Should dampen?\n",
    "    \n",
    "    # We draw from the history buffer\n",
    "    pool_fake = pool.randomise_existing_batch(fake)\n",
    "    fake_pred = discriminator(pool_fake)\n",
    "    # Fake images should not fool the discriminator\n",
    "    fake_loss = loss_func(fake_pred.detach(), generate_noisy_labels(fake_pred.shape, False, device))\n",
    "    \n",
    "    avg_loss = (real_loss + fake_loss) * 0.5\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a462990-62f3-4551-9681-8866c0d5f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cyclegan.start_epoch != 0:\n",
    "    print(f\"Resuming training from epoch {cyclegan.start_epoch + 1}\")\n",
    "else:\n",
    "    print(\"Starting training from scratch\")\n",
    "\n",
    "if vis is not None:\n",
    "    loss_plot = MultiLinePlot(vis, \"Losses\", \"Epoch\", \"Loss\")\n",
    "\n",
    "for epoch in range(cyclegan.start_epoch + 1, epochs + 1):\n",
    "    cyclegan.G.train()\n",
    "    cyclegan.F.train()\n",
    "    \n",
    "    cyclegan.D_X.train()\n",
    "    cyclegan.D_Y.train()\n",
    "    \n",
    "    cyclegan.P_X.train()\n",
    "    cyclegan.P_Y.train()\n",
    "\n",
    "    batch_start_time = time.time()\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    cum_loss_idt_x = 0\n",
    "    cum_loss_idt_y = 0\n",
    "    cum_G_fool_loss = 0\n",
    "    cum_F_fool_loss = 0\n",
    "    cum_predictor_x_loss = 0\n",
    "    cum_predictor_y_loss = 0\n",
    "    cum_cycled_x_loss = 0\n",
    "    cum_cycled_y_loss = 0\n",
    "    cum_D_X_loss = 0\n",
    "    cum_D_Y_loss = 0\n",
    "    \n",
    "    ep_loss_idt_x = 0\n",
    "    ep_loss_idt_y = 0\n",
    "    ep_G_fool_loss = 0\n",
    "    ep_F_fool_loss = 0\n",
    "    ep_predictor_x_loss = 0\n",
    "    ep_predictor_y_loss = 0\n",
    "    ep_cycled_x_loss = 0\n",
    "    ep_cycled_y_loss = 0\n",
    "    ep_D_X_loss = 0\n",
    "    ep_D_Y_loss = 0\n",
    "    \n",
    "    for batch_no, ((x_0, x_1, x_2), (y_0, y_1, y_2)) in enumerate(dataloader):\n",
    "        # Load the sequence of frames to the GPU\n",
    "        x_0 = x_0.to(device)\n",
    "        x_1 = x_1.to(device)\n",
    "        x_2 = x_2.to(device)\n",
    "        \n",
    "        y_0 = y_0.to(device)\n",
    "        y_1 = y_1.to(device)\n",
    "        y_2 = y_2.to(device)\n",
    "        \n",
    "        # Firstly, generate fake images in domain Y using the x_t images\n",
    "        fake_y_0 = cyclegan.G(x_0)\n",
    "        fake_y_1 = cyclegan.G(x_1)\n",
    "        \n",
    "        # For the final frame we use the predictor model, we are doing this in domain Y space so use P_Y not P_X\n",
    "        fake_y_2 = cyclegan.P_Y(torch.cat((fake_y_0, fake_y_1), 1))\n",
    "        \n",
    "        # Then generate the fake images in domain X using the y_t images\n",
    "        fake_x_0 = cyclegan.F(y_0)\n",
    "        fake_x_1 = cyclegan.F(y_1)\n",
    "        # For the final frame we use the predictor model, we are doing this in domain X space so use P_X not P_Y\n",
    "        fake_x_2 = cyclegan.P_X(torch.cat((fake_x_0, fake_x_1), 1))\n",
    "        \n",
    "        ### GENERATOR TRAINING\n",
    "        \n",
    "        # Freeze discriminator weights\n",
    "        for param in cyclegan.D_X.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in cyclegan.D_Y.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Zero the gradients for the generators and predictors\n",
    "        cyclegan.G_opt.zero_grad()\n",
    "        cyclegan.F_opt.zero_grad()\n",
    "        cyclegan.P_X_opt.zero_grad()\n",
    "        cyclegan.P_Y_opt.zero_grad()\n",
    "        \n",
    "        # Calculate the identity loss, this tries to enforce that G(y) = I(y) = y i.e. the identity\n",
    "        idt_x_0 = cyclegan.F(x_0)\n",
    "        idt_x_1 = cyclegan.F(x_1)\n",
    "        loss_idt_x = cyclegan.identity_loss(idt_x_0, x_0) + cyclegan.identity_loss(idt_x_1, x_1)\n",
    "        \n",
    "        idt_y_0 = cyclegan.G(y_0)\n",
    "        idt_y_1 = cyclegan.G(y_1)\n",
    "        loss_idt_y = cyclegan.identity_loss(idt_y_0, y_0) + cyclegan.identity_loss(idt_y_1, y_1)\n",
    "        \n",
    "        # Now we try and fool the discriminators\n",
    "        # D_X tries to tell if an image is from X (1) or from F(Y) (0) so it takes input fake_x_t\n",
    "        # D_X is supposed to be 1 if the image is from X but here we are trying to get it to be 1 if it is from G(X) -> Y which is incorrect\n",
    "        G_fool_0 = cyclegan.D_Y(fake_y_0) \n",
    "        # We want to tell if fake_y_0 has fooled D_X, so we measure how far from the output 1 it is \n",
    "        G_fool_loss_0 = cyclegan.gan_loss(G_fool_0, generate_noisy_labels(G_fool_0.shape, True, device))\n",
    "        \n",
    "        G_fool_1 = cyclegan.D_Y(fake_y_1)\n",
    "        G_fool_loss_1 = cyclegan.gan_loss(G_fool_1, generate_noisy_labels(G_fool_1.shape, True, device))\n",
    "        \n",
    "        G_fool_2 = cyclegan.D_Y(fake_y_2)\n",
    "        G_fool_loss_2 = cyclegan.gan_loss(G_fool_2, generate_noisy_labels(G_fool_2.shape, True, device))\n",
    "        \n",
    "        G_fool_loss = G_fool_loss_0 + G_fool_loss_1 + G_fool_loss_2\n",
    "        \n",
    "        # D_Y tries to tell if an image is from Y (1) or from G(X) (0) so it takes input fake_y_t\n",
    "        F_fool_0 = cyclegan.D_X(fake_x_0)\n",
    "        # We want to tell if fake_x_0 has fooled D_Y, so we measure how far from the output 1 it is \n",
    "        F_fool_loss_0 = cyclegan.gan_loss(F_fool_0, generate_noisy_labels(F_fool_0.shape, True, device))\n",
    "        \n",
    "        F_fool_1 = cyclegan.D_X(fake_x_1)\n",
    "        F_fool_loss_1 = cyclegan.gan_loss(F_fool_1, generate_noisy_labels(F_fool_1.shape, True, device))\n",
    "        \n",
    "        F_fool_2 = cyclegan.D_X(fake_x_2)\n",
    "        F_fool_loss_2 = cyclegan.gan_loss(F_fool_2, generate_noisy_labels(F_fool_2.shape, True, device))\n",
    "        \n",
    "        F_fool_loss = F_fool_loss_0 + F_fool_loss_1 + F_fool_loss_2\n",
    "        \n",
    "        # Now we train the predictors on the real images using the recurrent loss\n",
    "        # x_2 first\n",
    "        predicted_x_2 = cyclegan.P_X(torch.cat((x_0, x_1), 1))\n",
    "        predicted_x_loss = cyclegan.recurrent_loss(predicted_x_2, x_2)\n",
    "        \n",
    "        # So recycled_x_2 = F(P_Y(G(x_0), G(x_1)))\n",
    "        recycled_x_2 = cyclegan.F(fake_y_2)\n",
    "        recycled_x_loss = cyclegan.recycle_loss(recycled_x_2, x_2)\n",
    "        \n",
    "        predictor_x_loss = predicted_x_loss + recycled_x_loss\n",
    "        \n",
    "        # now y_2\n",
    "        predicted_y_2 = cyclegan.P_Y(torch.cat((y_0, y_1), 1))\n",
    "        predicted_y_loss = cyclegan.recurrent_loss(predicted_y_2, y_2)\n",
    "        \n",
    "        # So recycled_y_2 = G(P_X(F(y_0), F(y_1)))\n",
    "        recycled_y_2 = cyclegan.G(fake_x_2)\n",
    "        recycled_y_loss = cyclegan.recycle_loss(recycled_y_2, y_2)\n",
    "        \n",
    "        predictor_y_loss = predicted_y_loss + recycled_y_loss\n",
    "        \n",
    "        # Now do the cycle loss\n",
    "        cycled_x_0 = cyclegan.F(fake_y_0)\n",
    "        cycled_loss_x_0 = cyclegan.cycle_loss(cycled_x_0, x_0)\n",
    "        \n",
    "        cycled_x_1 = cyclegan.F(fake_y_1)\n",
    "        cycled_loss_x_1 = cyclegan.cycle_loss(cycled_x_1, x_1)\n",
    "        \n",
    "        cycled_loss_x = cycled_loss_x_0 + cycled_loss_x_1\n",
    "        \n",
    "        cycled_y_0 = cyclegan.G(fake_x_0)\n",
    "        cycled_loss_y_0 = cyclegan.cycle_loss(cycled_y_0, y_0)\n",
    "        \n",
    "        cycled_y_1 = cyclegan.G(fake_x_1)\n",
    "        cycled_loss_y_1 = cyclegan.cycle_loss(cycled_y_1, y_1)\n",
    "        \n",
    "        cycled_loss_y = cycled_loss_y_0 + cycled_loss_y_1\n",
    "        \n",
    "        # Backpropagate and step the gradients\n",
    "        generator_loss = G_fool_loss + F_fool_loss + lambda_weight * (cycled_loss_x + cycled_loss_y + predictor_x_loss + predictor_y_loss + lambda_idt_X * loss_idt_x + lambda_idt_Y * loss_idt_y)\n",
    "        generator_loss.backward()\n",
    "        \n",
    "        cyclegan.G_opt.step()\n",
    "        cyclegan.F_opt.step()\n",
    "        cyclegan.P_X_opt.step()\n",
    "        cyclegan.P_Y_opt.step()\n",
    "        \n",
    "        del G_fool_0\n",
    "        del G_fool_1\n",
    "        del G_fool_2\n",
    "        del F_fool_0\n",
    "        del F_fool_1\n",
    "        del F_fool_2\n",
    "        \n",
    "        ### DISCRIMINATOR TRAINING\n",
    "        # Unfreeze discriminator weights\n",
    "        for param in cyclegan.D_X.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in cyclegan.D_Y.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Zero the gradients\n",
    "        cyclegan.D_X_opt.zero_grad()\n",
    "        cyclegan.D_Y_opt.zero_grad()\n",
    "        \n",
    "        D_X_loss_0 = get_discriminator_loss(x_0, fake_x_0, cyclegan.fake_X_buffer, cyclegan.D_X, cyclegan.gan_loss)\n",
    "        D_X_loss_1 = get_discriminator_loss(x_1, fake_x_1, cyclegan.fake_X_buffer, cyclegan.D_X, cyclegan.gan_loss)\n",
    "        D_X_loss_fake_pred = get_discriminator_loss(x_2, fake_x_2, cyclegan.fake_X_buffer, cyclegan.D_X, cyclegan.gan_loss)\n",
    "        D_X_loss_real_pred = get_discriminator_loss(x_2, predicted_x_2, cyclegan.fake_X_buffer, cyclegan.D_X, cyclegan.gan_loss)\n",
    "        D_X_loss = D_X_loss_0 + D_X_loss_1 + D_X_loss_fake_pred + D_X_loss_real_pred\n",
    "        \n",
    "        D_Y_loss_0 = get_discriminator_loss(y_0, fake_y_0, cyclegan.fake_Y_buffer, cyclegan.D_Y, cyclegan.gan_loss)\n",
    "        D_Y_loss_1 = get_discriminator_loss(y_1, fake_y_1, cyclegan.fake_Y_buffer, cyclegan.D_Y, cyclegan.gan_loss)\n",
    "        D_Y_loss_fake_pred = get_discriminator_loss(y_2, fake_y_2, cyclegan.fake_Y_buffer, cyclegan.D_Y, cyclegan.gan_loss)\n",
    "        D_Y_loss_real_pred = get_discriminator_loss(y_2, predicted_y_2, cyclegan.fake_Y_buffer, cyclegan.D_Y, cyclegan.gan_loss)\n",
    "        D_Y_loss = D_Y_loss_0 + D_Y_loss_1 + D_Y_loss_fake_pred + D_Y_loss_real_pred\n",
    "        \n",
    "        # Backpropagate and step the gradients\n",
    "        D_loss = D_X_loss + D_Y_loss\n",
    "        D_loss.backward()\n",
    "        \n",
    "        cyclegan.D_X_opt.step()\n",
    "        cyclegan.D_Y_opt.step()\n",
    "        \n",
    "        ### UPDATE POOLS\n",
    "        cyclegan.fake_X_buffer.add(fake_x_0.detach())\n",
    "        cyclegan.fake_X_buffer.add(fake_x_1.detach())\n",
    "        cyclegan.fake_X_buffer.add(fake_x_2.detach())\n",
    "        \n",
    "        cyclegan.fake_Y_buffer.add(fake_y_0.detach())\n",
    "        cyclegan.fake_Y_buffer.add(fake_y_1.detach())\n",
    "        cyclegan.fake_Y_buffer.add(fake_y_2.detach())\n",
    "        \n",
    "        ### TRACK LOSS\n",
    "        cum_loss_idt_x += loss_idt_x.item()\n",
    "        cum_loss_idt_y += loss_idt_y.item()\n",
    "        cum_G_fool_loss += G_fool_loss.item()\n",
    "        cum_F_fool_loss += F_fool_loss.item()\n",
    "        cum_predictor_x_loss += predictor_x_loss.item()\n",
    "        cum_predictor_y_loss += predictor_y_loss.item()\n",
    "        cum_cycled_x_loss += cycled_loss_x.item()\n",
    "        cum_cycled_y_loss += cycled_loss_y.item()\n",
    "        cum_D_X_loss += D_X_loss.item()\n",
    "        cum_D_Y_loss += D_Y_loss.item()\n",
    "        \n",
    "        ep_loss_idt_x += loss_idt_x.item()\n",
    "        ep_loss_idt_y += loss_idt_y.item()\n",
    "        ep_G_fool_loss += G_fool_loss.item()\n",
    "        ep_F_fool_loss += F_fool_loss.item()\n",
    "        ep_predictor_x_loss += predictor_x_loss.item()\n",
    "        ep_predictor_y_loss += predictor_y_loss.item()\n",
    "        ep_cycled_x_loss += cycled_loss_x.item()\n",
    "        ep_cycled_y_loss += cycled_loss_y.item()\n",
    "        ep_D_X_loss += D_X_loss.item()\n",
    "        ep_D_Y_loss += D_Y_loss.item()\n",
    "        \n",
    "        if epoch == 0 and batch_no < 55:\n",
    "            print(f\"[{epoch}:{batch_no}] fake_X_buffer: {len(cyclegan.fake_X_buffer)}, fake_Y_buffer: {len(cyclegan.fake_Y_buffer)}\")\n",
    "        \n",
    "        if batch_no % 100 == 0 and batch_no != 0: \n",
    "            duration = time.time() - batch_start_time\n",
    "            \n",
    "            updated_losses = {\n",
    "                \"loss_idt_x\": cum_loss_idt_x / 100,\n",
    "                \"loss_idt_y\": cum_loss_idt_y / 100,\n",
    "                \"G_fool_loss\": cum_G_fool_loss / 100,\n",
    "                \"F_fool_loss\": cum_F_fool_loss / 100,\n",
    "                \"predictor_x_loss\": cum_predictor_x_loss / 100,\n",
    "                \"predictor_y_loss\": cum_predictor_y_loss / 100,\n",
    "                \"cycled_x_loss\": cum_cycled_x_loss / 100,\n",
    "                \"cycled_y_loss\": cum_cycled_y_loss / 100,\n",
    "                \"D_X_loss\": cum_D_X_loss / 100,\n",
    "                \"D_Y_loss\": cum_D_Y_loss / 100\n",
    "            }\n",
    "            \n",
    "            x_loss_str = f\"[{epoch}:{batch_no}]\"\n",
    "            y_loss_str = f\"[{epoch}:{batch_no}]\"\n",
    "            \n",
    "            for i, (key, value) in enumerate(updated_losses.items()):\n",
    "                if i % 2 == 0:\n",
    "                    x_loss_str = f\"{x_loss_str} {key}: {value},\"\n",
    "                else:\n",
    "                    y_loss_str = f\"{y_loss_str} {key}: {value},\"\n",
    "            \n",
    "            x_loss_str = x_loss_str[:-1]\n",
    "            y_loss_str = y_loss_str[:-1]\n",
    "\n",
    "            print(f\"[{epoch}:{batch_no}] Took {duration:.2f}s\")\n",
    "            print(x_loss_str)\n",
    "            print(y_loss_str)\n",
    "            print(f\"[{epoch}:{batch_no}] fake_X_buffer: {len(cyclegan.fake_X_buffer)}, fake_Y_buffer: {len(cyclegan.fake_Y_buffer)}\")\n",
    "            \n",
    "            if vis is not None:\n",
    "                loss_plot.append_values(epoch + batch_no / len(dataloader), updated_losses)\n",
    "            \n",
    "            cum_loss_idt_x = 0\n",
    "            cum_loss_idt_y = 0\n",
    "            cum_G_fool_loss = 0\n",
    "            cum_F_fool_loss = 0\n",
    "            cum_predictor_x_loss = 0\n",
    "            cum_predictor_y_loss = 0\n",
    "            cum_cycled_x_loss = 0\n",
    "            cum_cycled_y_loss = 0\n",
    "            cum_D_X_loss = 0\n",
    "            cum_D_Y_loss = 0\n",
    "\n",
    "            batch_start_time = time.time()\n",
    "    \n",
    "    print(f\"[{epoch}:END] Completed epoch in {time.time() - epoch_start_time}s\")\n",
    "    \n",
    "    print(f\"[{epoch}:{batch_no}]\", \n",
    "          f\"ep_loss_idt_x: {ep_loss_idt_x / len(dataloader):.3f}\", \n",
    "          f\"ep_G_fool_loss: {ep_G_fool_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_predictor_x_loss: {ep_predictor_x_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_cycled_x_loss: {ep_cycled_x_loss / len(dataloader):.3f}\",\n",
    "          f\"ep_D_X_loss: {ep_D_X_loss / len(dataloader):.3f}\")\n",
    "    \n",
    "    print(f\"[{epoch}:{batch_no}]\", \n",
    "          f\"ep_loss_idt_y: {ep_loss_idt_y / len(dataloader):.3f}\", \n",
    "          f\"ep_F_fool_loss: {ep_F_fool_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_predictor_y_loss: {ep_predictor_y_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_cycled_y_loss: {ep_cycled_y_loss / len(dataloader):.3f}\",\n",
    "          f\"ep_D_Y_loss: {ep_D_Y_loss / len(dataloader):.3f}\")\n",
    "    \n",
    "    cyclegan.G.eval()\n",
    "    cyclegan.F.eval()\n",
    "    \n",
    "    if vis is not None:\n",
    "        eval_start_time = time.time()\n",
    "\n",
    "        G_eval_forward = cyclegan.apply(test_xs, x_to_y=True)\n",
    "        F_eval_forward = cyclegan.apply(test_ys, x_to_y=False)\n",
    "\n",
    "        G_rev = cyclegan.apply(G_eval_forward, x_to_y=False)\n",
    "        F_rev = cyclegan.apply(F_eval_forward, x_to_y=True)\n",
    "\n",
    "        G_eval_forward = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in G_eval_forward])\n",
    "        F_eval_forward = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in F_eval_forward])\n",
    "        G_rev = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in G_rev])\n",
    "        F_rev = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in F_rev])\n",
    "\n",
    "        G_eval_grid = torchvision.utils.make_grid(G_eval_forward, nrow=4)\n",
    "        F_eval_grid = torchvision.utils.make_grid(F_eval_forward, nrow=4)\n",
    "        G_rev_grid = torchvision.utils.make_grid(G_rev, nrow=4)\n",
    "        F_rev_grid = torchvision.utils.make_grid(F_rev, nrow=4)\n",
    "        \n",
    "        folder = f\"{cyclegan.save_folder}/{epoch}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        \n",
    "        torchvision.utils.save_image(G_eval_grid, f\"{folder}/X_to_Y.png\")\n",
    "        torchvision.utils.save_image(F_eval_grid, f\"{folder}/Y_to_X.png\")\n",
    "        torchvision.utils.save_image(G_rev_grid, f\"{folder}/X_to_Y_to_X.png\")\n",
    "        torchvision.utils.save_image(F_rev_grid, f\"{folder}/Y_to_X_to_Y.png\")\n",
    "\n",
    "        vis.image(G_eval_grid, win=\"G_eval\", opts={\n",
    "            \"caption\": f\"X -> Y evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(F_eval_grid, win=\"F_eval\", opts={\n",
    "            \"caption\": f\"Y -> X evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(G_rev_grid, win=\"G_rev\", opts={\n",
    "            \"caption\": f\"X -> Y -> X evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(F_rev_grid, win=\"F_rev\", opts={\n",
    "            \"caption\": f\"Y -> X -> Y evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "        \n",
    "        print(f\"[{epoch}:END] Completed eval in {time.time() - eval_start_time}s\")\n",
    "\n",
    "    cyclegan.G.train()\n",
    "    cyclegan.F.train()\n",
    "\n",
    "    cyclegan.step_learning_rates()\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"[{epoch}:END] Saving models and training information permanently\")\n",
    "        cyclegan.save(epoch, full_save=True)\n",
    "        cyclegan.save(epoch, full_save=True, folder=\"latest\")\n",
    "    else:\n",
    "        print(f\"[{epoch}:END] Saving models and training information temporarily to latest and saving generators permanently\")\n",
    "        cyclegan.save(epoch, full_save=True, folder=\"latest\")\n",
    "        cyclegan.save(epoch, full_save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d56c9-97ab-4aa0-bafa-b8bcd55b101b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Video Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a35e8cbb-55eb-48b5-9ff8-e81e531a622e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_time = \"1680898964.0020654\"\n",
    "model_parent_directory = f\"./runs/RecycleGAN/{model_time}\"\n",
    "epoch_directory = \"latest\"\n",
    "model_name = \"F.pth\" # F: Y -> X which is the style transfer we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dd980fe3-49a9-4dec-b0b2-9e3bfd6bb698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{model_parent_directory}/info_None.json\", \"r\") as fp:\n",
    "    model_info = json.load(fp)\n",
    "\n",
    "upsample_strategy = model_info[\"upsample_strategy\"]\n",
    "block_count = model_info[\"block_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "13896de5-0cac-4b68-8c59-2171870b0c41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Generator(block_count, upsample_strategy).to(device)\n",
    "model.load_state_dict(torch.load(f\"{model_parent_directory}/{epoch_directory}/{model_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "21ac2f86-ef4a-4b1d-80b1-1ef44178f954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_video_path = \"./original_data/Test/Test Movie.mp4\"\n",
    "output_video_path = f\"./video_transfer/recyclegan_{model_time}.mp4\"\n",
    "batch_size = 4\n",
    "\n",
    "os.makedirs(\"./video_transfer\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1f19d2b0-07b0-4ffd-b297-2273d9527fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef2c9ced0074fa39e293e97b7926704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_video(input_video_path, output_video_path, model, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eec850d-97c4-4f2a-a950-c45cd4d37252",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Frame Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc93aff-fa22-41d2-81ff-c4a80b0de33e",
   "metadata": {},
   "source": [
    "To run this you may need to rename the 'Test Movie.mp4' to 'test_movie.mp4' to avoid issues with whitespace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5d52d22c-6cc0-48ff-a315-c762b413b5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from './original_data/Test/test_movie.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: mp42mp41\n",
      "    creation_time   : 2022-12-02T16:48:53.000000Z\n",
      "  Duration: 00:01:05.80, start: 0.000000, bitrate: 427 kb/s\n",
      "    Stream #0:0(eng): Video: h264 (Main) (avc1 / 0x31637661), yuv420p, 480x360 [SAR 1:1 DAR 4:3], 424 kb/s, 25 fps, 25 tbr, 25k tbn, 50 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2022-12-02T16:48:53.000000Z\n",
      "      handler_name    : Alias Data Handler\n",
      "      encoder         : AVC Coding\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[swscaler @ 0x7ffff7823280] deprecated pixel format used, make sure you did set range correctly\n",
      "Output #0, image2, to './video_transfer/frames/original/%05d.jpg':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: mp42mp41\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0(eng): Video: mjpeg, yuvj420p(pc), 480x360 [SAR 1:1 DAR 4:3], q=2-31, 200 kb/s, 25 fps, 25 tbn, 25 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2022-12-02T16:48:53.000000Z\n",
      "      handler_name    : Alias Data Handler\n",
      "      encoder         : Lavc57.107.100 mjpeg\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
      "frame= 1645 fps=214 q=2.0 Lsize=N/A time=00:01:05.80 bitrate=N/A speed=8.56x    \n",
      "video:36842kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n",
      "ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from './video_transfer/cyclegan_1680981254.7780375_F.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: mp41isom\n",
      "    creation_time   : 2023-04-09T18:01:08.000000Z\n",
      "  Duration: 00:01:05.80, start: 0.000000, bitrate: 4269 kb/s\n",
      "    Stream #0:0(und): Video: h264 (Constrained Baseline) (avc1 / 0x31637661), yuv420p, 480x360 [SAR 1:1 DAR 4:3], 4269 kb/s, 25 fps, 25 tbr, 25k tbn, 50 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-04-09T18:01:08.000000Z\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : AVC Coding\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[swscaler @ 0x7fffb9891720] deprecated pixel format used, make sure you did set range correctly\n",
      "Output #0, image2, to './video_transfer/frames/cyclegan/%05d.jpg':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: mp41isom\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0(und): Video: mjpeg, yuvj420p(pc), 480x360 [SAR 1:1 DAR 4:3], q=2-31, 200 kb/s, 25 fps, 25 tbn, 25 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-04-09T18:01:08.000000Z\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc57.107.100 mjpeg\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
      "frame= 1645 fps=224 q=2.0 Lsize=N/A time=00:01:05.80 bitrate=N/A speed=8.96x    \n",
      "video:34029kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n",
      "ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from './video_transfer/recyclegan_1680898964.0020654_F.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: mp41isom\n",
      "    creation_time   : 2023-04-09T18:02:58.000000Z\n",
      "  Duration: 00:01:05.80, start: 0.000000, bitrate: 4277 kb/s\n",
      "    Stream #0:0(und): Video: h264 (Constrained Baseline) (avc1 / 0x31637661), yuv420p, 480x360 [SAR 1:1 DAR 4:3], 4276 kb/s, 25 fps, 25 tbr, 25k tbn, 50 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-04-09T18:02:58.000000Z\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : AVC Coding\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[swscaler @ 0x7fffd7419e80] deprecated pixel format used, make sure you did set range correctly\n",
      "Output #0, image2, to './video_transfer/frames/recyclegan/%05d.jpg':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: mp41isom\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0(und): Video: mjpeg, yuvj420p(pc), 480x360 [SAR 1:1 DAR 4:3], q=2-31, 200 kb/s, 25 fps, 25 tbn, 25 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-04-09T18:02:58.000000Z\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc57.107.100 mjpeg\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
      "frame= 1645 fps=229 q=2.0 Lsize=N/A time=00:01:05.80 bitrate=N/A speed=9.17x    \n",
      "video:33510kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./video_transfer_frame_comparison.sh \"./original_data/Test/test_movie.mp4\" \"./video_transfer/cyclegan_1680981254.7780375.mp4\" \"./video_transfer/recyclegan_1680898964.0020654.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc78896c-4e0d-49da-a540-7a145ed61871",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f18da-f3bb-4780-8664-652a7aa07c67",
   "metadata": {},
   "source": [
    "#### Segmentation Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a4e1c-9ab0-4c2a-85f4-51df965db759",
   "metadata": {},
   "source": [
    "I begin by cycling the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2417edc7-0dd0-4853-812d-2d9f0b4ccfad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_transfer_folder = \"./extracted_data/recyclegan_transferred\"\n",
    "test_input_folder = lambda base: f\"./extracted_data/recyclegan_training_data/{base}/Test\"\n",
    "\n",
    "model_parent_folder = \"./runs/RecycleGAN/1680898964.0020654\"\n",
    "game_to_movie_path = f\"{model_parent_folder}/latest/G.pth\"\n",
    "movie_to_game_path = f\"{model_parent_folder}/latest/F.pth\"\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "291a2e25-0071-49d9-9e1e-d12fe8552f41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbab10a7b77a4887a81a98630158585a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84d12665c9a45c0ae002fa54f60901e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(f\"{model_parent_folder}/info_None.json\", \"r\") as fp:\n",
    "    model_info = json.load(fp)\n",
    "\n",
    "upsample_strategy = model_info[\"upsample_strategy\"]\n",
    "block_count = model_info[\"block_count\"]\n",
    "\n",
    "for base in [\"Game\", \"Movie\"]:\n",
    "    other_domain = \"Movie\" if base == \"Game\" else \"Game\"\n",
    "    model_path =  game_to_movie_path if base == \"Game\" else movie_to_game_path\n",
    "    test_folder = test_input_folder(base)\n",
    "    transferred_folder = f\"{base_transfer_folder}/{base}_to_{other_domain}\"\n",
    "    \n",
    "    os.makedirs(transferred_folder)\n",
    "    \n",
    "    model = Generator(block_count, upsample_strategy).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    file_names = [file_name for file_name in os.listdir(test_folder) if file_name.endswith(\".jpg\")]\n",
    "    batch_count = len(file_names) // batch_size + 1\n",
    "    \n",
    "    for batch_no in tqdm(range(batch_count)):\n",
    "        selected_files = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            real_i = batch_size * batch_no + i\n",
    "            \n",
    "            if real_i >= len(file_names):\n",
    "                break\n",
    "            \n",
    "            selected_files.append(file_names[real_i])\n",
    "        \n",
    "        if len(selected_files) == 0:\n",
    "            break\n",
    "            \n",
    "        batch_images = [cv2.cvtColor(cv2.imread(f\"{test_folder}/{file_name}\"), cv2.COLOR_BGR2RGB) for file_name in selected_files]\n",
    "        transferred = transfer_style_to_batch(batch_images, model)\n",
    "        \n",
    "        for file_name, transferred_image in zip(selected_files, transferred):\n",
    "            cv2.imwrite(f\"{transferred_folder}/{''.join(file_name.split('.')[:-1])}_transferred.jpg\", cv2.cvtColor((transferred_image.numpy() * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23def448-1850-4cba-960e-27514d53c30b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc68efd9f88a41008bf3a8d983fc181b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823b395d1b9545b2b9f4409c27795e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(f\"{model_parent_folder}/info_None.json\", \"r\") as fp:\n",
    "    model_info = json.load(fp)\n",
    "\n",
    "upsample_strategy = model_info[\"upsample_strategy\"]\n",
    "block_count = model_info[\"block_count\"]\n",
    "\n",
    "for base in [\"Game\", \"Movie\"]:\n",
    "    other_domain = \"Movie\" if base == \"Game\" else \"Game\"\n",
    "    model_path =  movie_to_game_path if base == \"Game\" else game_to_movie_path\n",
    "    test_folder = f\"{base_transfer_folder}/{base}_to_{other_domain}\"\n",
    "    transferred_folder = f\"{base_transfer_folder}/{base}_to_{other_domain}_to_{base}\"\n",
    "    \n",
    "    os.makedirs(transferred_folder)\n",
    "    \n",
    "    model = Generator(block_count, upsample_strategy).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    file_names = [file_name for file_name in os.listdir(test_folder) if file_name.endswith(\".jpg\")]\n",
    "    batch_count = len(file_names) // batch_size + 1\n",
    "    \n",
    "    for batch_no in tqdm(range(batch_count)):\n",
    "        selected_files = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            real_i = batch_size * batch_no + i\n",
    "            \n",
    "            if real_i >= len(file_names):\n",
    "                break\n",
    "            \n",
    "            selected_files.append(file_names[real_i])\n",
    "        \n",
    "        if len(selected_files) == 0:\n",
    "            break\n",
    "            \n",
    "        batch_images = [cv2.cvtColor(cv2.imread(f\"{test_folder}/{file_name}\"), cv2.COLOR_BGR2RGB) for file_name in selected_files]\n",
    "        transferred = transfer_style_to_batch(batch_images, model)\n",
    "        \n",
    "        for file_name, transferred_image in zip(selected_files, transferred):\n",
    "            cv2.imwrite(f\"{transferred_folder}/{''.join(file_name.split('.')[:-1])}_transferred.jpg\", cv2.cvtColor((transferred_image.numpy() * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5669605a-63c6-4d4a-87db-f51e4ada213c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_game_folder = \"./extracted_data/recyclegan_training_data/Game/Test\"\n",
    "cycled_game_folder = \"./extracted_data/recyclegan_transferred/Game_to_Movie_to_Game\"\n",
    "original_game_files = [f\"{original_game_folder}/{file_name}\" for file_name in os.listdir(original_game_folder) if file_name.endswith(\".jpg\")]\n",
    "cycled_game_files = [f\"{cycled_game_folder}/{file_name}\" for file_name in os.listdir(cycled_game_folder) if file_name.endswith(\".jpg\")]\n",
    "\n",
    "original_movie_folder = \"./extracted_data/recyclegan_training_data/Movie/Test\"\n",
    "cycled_movie_folder = \"./extracted_data/recyclegan_transferred/Movie_to_Game_to_Movie\"\n",
    "original_movie_files = [f\"{original_movie_folder}/{file_name}\" for file_name in os.listdir(original_movie_folder) if file_name.endswith(\".jpg\")]\n",
    "cycled_movie_files = [f\"{cycled_movie_folder}/{file_name}\" for file_name in os.listdir(cycled_movie_folder) if file_name.endswith(\".jpg\")]\n",
    "\n",
    "threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "342e684c-7425-45c2-97d1-20cccd2cccb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "maskrcnn.to(device).eval()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e69eb33-36c0-45ba-9c6c-5ee694ff6fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 91 COCO class names, directly from Semantic Segmentation Mask R-CNN.ipynb\n",
    "coco_names = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af9149bd-b607-4338-96d2-6be530d09185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_segmentation_map(output, frame, threshold):\n",
    "    # Get the relevant model output on to the CPU in a usable format\n",
    "    scores = output[\"scores\"].detach().cpu().numpy()\n",
    "    boxes = output[\"boxes\"].detach().cpu().numpy().astype(int)\n",
    "    label_indices = output[\"labels\"].detach().cpu()\n",
    "    masks = (output[\"masks\"] > 0.5).detach().cpu()\n",
    "    \n",
    "    segmentation_maps = {label: np.zeros(frame.shape[:2]).astype(np.uint8) for label in coco_names[1:]}\n",
    "    \n",
    "    # Process each entry found by the model\n",
    "    for i, (confidence, box, label_idx, mask) in enumerate(zip(scores, boxes, label_indices, masks)):\n",
    "        label = coco_names[label_idx.item()]\n",
    "        \n",
    "        # The confidences are sorted high to low so stop once we're below the threshold\n",
    "        if confidence < threshold:\n",
    "            break\n",
    "        \n",
    "        segmentation_maps[label][(mask == 1).squeeze()] = 1\n",
    "    \n",
    "    return segmentation_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44af4403-70e5-4024-a699-fa6a8c3ccf8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def measure_segmentation_map_similarity(original_files, cycled_files):\n",
    "    tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mean_accuracies = []\n",
    "    \n",
    "    for original_file_name, cycled_file_name in tqdm(zip(original_files, cycled_files), total=len(original_files)):\n",
    "        original_file = cv2.imread(original_file_name, cv2.IMREAD_COLOR)\n",
    "        cycled_file = cv2.imread(cycled_file_name, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if original_file.shape != cycled_file.shape:\n",
    "            original_file = cv2.cvtColor(original_file, cv2.COLOR_BGR2RGB)\n",
    "            original_file_pil = Image.fromarray(original_file).resize((256, 256))\n",
    "            original_file = np.asarray(original_file_pil)\n",
    "            original_file = cv2.cvtColor(original_file, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            cycled_file = cv2.cvtColor(cycled_file, cv2.COLOR_BGR2RGB)\n",
    "            cycled_file_pil = Image.fromarray(cycled_file).resize((256, 256))\n",
    "            cycled_file = np.asarray(cycled_file_pil)\n",
    "            cycled_file = cv2.cvtColor(cycled_file, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "        \n",
    "        maskrcnn_batch = torch.stack([tensor_transform(original_file), tensor_transform(cycled_file)]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = maskrcnn(maskrcnn_batch)\n",
    "            \n",
    "        original_seg_maps = retrieve_segmentation_map(outputs[0], original_file, threshold)\n",
    "        cycled_seg_maps = retrieve_segmentation_map(outputs[1], cycled_file, threshold)\n",
    "        \n",
    "        ins_accuracies = []\n",
    "        \n",
    "        for label in coco_names[1:]:\n",
    "            original_seg_map = original_seg_maps[label]\n",
    "            cycled_seg_map = cycled_seg_maps[label]\n",
    "            \n",
    "            assert original_seg_map.shape == cycled_seg_map.shape\n",
    "            \n",
    "            if not original_seg_map.any() and not cycled_seg_map.any():\n",
    "                continue\n",
    "            \n",
    "            correct_pixels = ((original_seg_map == 1) & (cycled_seg_map == 1)).sum()\n",
    "            total_pixels = (original_seg_map == 1).sum()\n",
    "            \n",
    "#             dupe_frame = original_file.copy()\n",
    "#             y = np.stack([original_seg_map, original_seg_map, original_seg_map], axis=2).astype(np.uint8)\n",
    "#             cv2.addWeighted(dupe_frame, 1.0, y * 200, 0.6, 0.0, dupe_frame)\n",
    "#             cv2.imwrite(f\"./test_{time.time()}_{label}_org.png\", dupe_frame)\n",
    "            \n",
    "#             dupe_frame = cycled_file.copy()\n",
    "#             y = np.stack([cycled_seg_map, cycled_seg_map, cycled_seg_map], axis=2).astype(np.uint8)\n",
    "#             cv2.addWeighted(dupe_frame, 1.0, y * 200, 0.6, 0.0, dupe_frame)\n",
    "#             cv2.imwrite(f\"./test_{time.time()}_{label}_cycled.png\", dupe_frame)\n",
    "            \n",
    "            if total_pixels == 0:\n",
    "                continue\n",
    "    \n",
    "            accuracy = correct_pixels / total_pixels\n",
    "            ins_accuracies.append(accuracy)\n",
    "            \n",
    "        if len(ins_accuracies) == 0:\n",
    "            continue\n",
    "        \n",
    "        mean_accuracy = sum(ins_accuracies) / len(ins_accuracies)\n",
    "        mean_accuracies.append(mean_accuracy)\n",
    "    \n",
    "    return sum(mean_accuracies) / len(mean_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a4fdd25-ebc4-42f0-8cc0-ff0b72aa63ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7d01c4ce684cfd83e74baf81a4c146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game_fcn = measure_segmentation_map_similarity(original_game_files, cycled_game_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97afa76f-e60e-4dfa-bfec-cd3117cc5e08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6721227902346468"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7ba3f-cd11-470b-ba80-a62291547da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_fcn = measure_segmentation_map_similarity(original_movie_files, cycled_movie_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa367a8-7fc6-4f86-aa04-aa1753ae1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_fcn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6ba40-7099-44a7-b783-c681fb3927b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2e5a543a-61bb-4790-92b0-3e81ff678cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID between two folders\n",
      "Found 486 images in the folder ./extracted_data/recyclegan_training_data/Movie/Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID Test : 100%|███████████████████████████████████████████████████████████████████████| 16/16 [00:03<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 508 images in the folder ./extracted_data/recyclegan_transferred/Game_to_Movie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID Game_to_Movie : 100%|██████████████████████████████████████████████████████████████| 16/16 [00:06<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "score = fid.compute_fid(test_input_folder(\"Movie\"), \"./extracted_data/recyclegan_transferred/Game_to_Movie\", num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a0944f6b-23d3-4e14-9bb8-9f4e26c309cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172.21562406817506"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7196705e-c26e-4033-ae2d-cbd5bfdec7a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID between two folders\n",
      "Found 508 images in the folder ./extracted_data/recyclegan_training_data/Game/Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID Test : 100%|███████████████████████████████████████████████████████████████████████| 16/16 [00:05<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 486 images in the folder ./extracted_data/recyclegan_transferred/Movie_to_Game\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID Movie_to_Game : 100%|██████████████████████████████████████████████████████████████| 16/16 [00:04<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "score = fid.compute_fid(test_input_folder(\"Game\"), \"./extracted_data/recyclegan_transferred/Movie_to_Game\", num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fe31fa99-dbe6-4c81-b638-ef08d2a9f028",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178.59822213959313"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
