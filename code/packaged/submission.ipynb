{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "011db540-76a2-4754-9535-435c968eb1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.models\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# These imports contain code that is my own work\n",
    "from skeletal_pose import PoseKeypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c425dcb-98ac-415d-885d-4ad3f7b099cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(f\"It is highly recommended to use a GPU! This is likely to run extremely slowly otherwise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5b2fd-9d38-4a0d-ae35-68b280fb9b38",
   "metadata": {},
   "source": [
    "# 1: Human Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d02f9-170e-44fe-9896-7f71b40f5819",
   "metadata": {},
   "source": [
    "General setup instructions:\n",
    "\n",
    "* Please place the original data into a folder called 'original_data' in the same directory as this notebook\n",
    "* Please ensure ffmpeg is available on your system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99938d90-d9a3-497c-b6fb-52cce8f09de7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1: Human Patch Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5827d8b-0888-40fd-8f32-7bc73f226941",
   "metadata": {},
   "source": [
    "The process I follow to extract the human patches is as follows:\n",
    "\n",
    "1. Extract the frames from the training videos using ffmpeg\n",
    "2. Use a pre-trained MaskR-CNN model to segment and extract the human patches\n",
    "3. Save the human patches for future processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d18a58-7545-483f-9490-ee283bcf85dc",
   "metadata": {},
   "source": [
    "Firstly, I use ffmpeg to extract all frames from the videos and store them for later processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73f7e3-fa2b-4119-9520-ac8ad9a7d85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "./frame_extraction.sh ./original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c0b41-8482-4fdf-8f88-c80736f7aead",
   "metadata": {},
   "source": [
    "Next, I use a pre-trained MaskR-CNN model to extract the human patches.\n",
    "This part is heavily inspired by the practical 'Semantic Segmentation Mask R-CNN.ipynb' on Blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a10d4-0cfc-456b-b791-fe53c67114a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "maskrcnn.to(device).eval()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ff6df-fa85-4e8f-abc5-ffbbb4df4417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 91 COCO class names, directly from Semantic Segmentation Mask R-CNN.ipynb\n",
    "coco_names = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d0a54-d0d7-4e41-8b7f-928a57202193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def batch_loader(input_directory, output_directory, batch_size):\n",
    "    # Get all files in the directory that are frames\n",
    "    file_names = [file_name for file_name in os.listdir(input_directory) if file_name.endswith(\".jpg\")]\n",
    "    \n",
    "    for i in tqdm(range(len(file_names) // batch_size + 1)):\n",
    "        selected = []\n",
    "        start_idx = batch_size * i\n",
    "        limit_idx = batch_size * (i + 1)\n",
    "        \n",
    "        # Get the images to put through the model\n",
    "        if limit_idx > len(file_names):\n",
    "            selected = file_names[start_idx:]\n",
    "        else:\n",
    "            selected = file_names[start_idx:limit_idx]\n",
    "            \n",
    "        if len(selected) == 0:\n",
    "            break\n",
    "        \n",
    "        output_files = []\n",
    "        raw_frames = []\n",
    "        usable_frames = []\n",
    "        \n",
    "        for file_name in selected:\n",
    "            # Pre-determine the save location and load the frame with RGB\n",
    "            output_files.append(f\"{output_directory}/Segmented_{file_name.split('.')[0]}\")\n",
    "            input_file = f\"{input_directory}/{file_name}\"\n",
    "            raw_frame = cv2.imread(input_file, cv2.IMREAD_COLOR)\n",
    "            raw_frames.append(raw_frame)\n",
    "            \n",
    "            usable_frame = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2RGB)\n",
    "            usable_frame = tensor_transform(usable_frame)\n",
    "            usable_frames.append(usable_frame)\n",
    "        \n",
    "        # Return a tensor to use with the model and additional info for cropping and saving\n",
    "        stacked_frames = torch.stack(usable_frames).to(device)\n",
    "        yield stacked_frames, raw_frames, output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac26cdf-8cab-4849-859c-c3f9d9d8d5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_maskrcnn_output(maskrcnn_output, original_frame, save_loc, threshold):\n",
    "    # Get the relevant model output on to the CPU in a usable format\n",
    "    scores = output[\"scores\"].detach().cpu().numpy()\n",
    "    boxes = output[\"boxes\"].detach().cpu().numpy().astype(int)\n",
    "    label_indices = output[\"labels\"].detach().cpu()\n",
    "    \n",
    "    # Process each entry found by the model\n",
    "    for i, (confidence, box, label_idx) in enumerate(zip(scores, boxes, label_indices)):\n",
    "        label = coco_names[label_idx.item()]\n",
    "        \n",
    "        if label != \"person\":\n",
    "            continue\n",
    "        \n",
    "        # The confidences are sorted high to low so stop once we're below the threshold\n",
    "        if confidence < threshold:\n",
    "            break\n",
    "          \n",
    "        # Crop and save the patch using its bounding box\n",
    "        x_0, y_0, x_1, y_1 = box\n",
    "        patch = original_frame.copy()[y_0:y_1, x_0:x_1]\n",
    "        cv2.imwrite(f\"{save_loc}_{i}.jpg\", patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eee283-6bbe-4416-8bda-96b696af8279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If everything is setup correctly and the frames have been extracted these settings should be fine\n",
    "base_folders = [\"Train/Game\", \"Train/Movie\"]\n",
    "base_frame_folder = \"./extracted_data/frames\"\n",
    "base_output_folder = \"./extracted_data/unclassified_human_patches\"\n",
    "\n",
    "batch_size = 4\n",
    "threshold = 0.965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b56ffa-92a7-4451-99a5-9c7996c10469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process each folder and every frame in them\n",
    "for base_folder in base_folders:\n",
    "    in_directory = f\"{base_frame_folder}/{base_folder}\"\n",
    "    out_directory = f\"{base_output_folder}/{base_folder}\"\n",
    "    \n",
    "    os.makedirs(out_directory)\n",
    "    \n",
    "    print(f\"{in_directory} -> {out_directory}\")\n",
    "    \n",
    "    batch_generator = batch_loader(in_directory, out_directory, batch_size)\n",
    "\n",
    "    for i, (batch, raw_frames, output_files) in enumerate(batch_generator):\n",
    "        with torch.no_grad():\n",
    "            outputs = maskrcnn(batch)\n",
    "\n",
    "        for output, frame, output_file in zip(outputs, raw_frames, output_files):\n",
    "            process_maskrcnn_output(output, frame, output_file, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff914a2-2095-4728-ac1f-8d0885719cce",
   "metadata": {},
   "source": [
    "In testing, this gave:\n",
    "\n",
    "* 18,454 human patches for the movie clips\n",
    "* 37,530 human patches for the game clips \n",
    "\n",
    "I now sample 50 random images from each domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922652c3-5952-4308-9044-eb26a3e3111a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_submission_folder = \"./data_for_submission/1_1_sampled_human_patches\"\n",
    "sample_count = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c956f58-b4f0-408a-a2e1-e125b026cda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for base_folder in base_folders:\n",
    "    patch_directory = f\"{base_output_folder}/{base_folder}\"\n",
    "    sampled_directory = f\"{base_submission_folder}/{base_folder}\"\n",
    "    \n",
    "    os.makedirs(sampled_directory) \n",
    "    \n",
    "    print(f\"{patch_directory} -> {sampled_directory}\")\n",
    "    \n",
    "    file_names = [file_name for file_name in os.listdir(patch_directory) if file_name.endswith(\".jpg\")]\n",
    "    sampled_file_names = random.sample(file_names, k=sample_count)\n",
    "    \n",
    "    for sampled in sampled_file_names:\n",
    "        shutil.copy(f\"{patch_directory}/{sampled}\", sampled_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f3ffa-c212-4ebb-92a2-e23b455b8f09",
   "metadata": {},
   "source": [
    "## 1.2: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76022d-72a4-4c8a-bb5d-710c8c46a55d",
   "metadata": {},
   "source": [
    "Using the human patches from the previous task, I use the following process to classify them according to pose:\n",
    "1. Use OpenPose to extract the pose keypoints from each human patch\n",
    "2. Load the extracted pose into Python and analyse the data to determine the pose based on joint visibility\n",
    "3. To determine if they are standing or sitting I compute the angle between the hips and knees if they are visible\n",
    "4. Save the images into folders based on their classified pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b1d9e-491e-4641-8b40-f03148222b64",
   "metadata": {},
   "source": [
    "Firstly, I use OpenPose to extract the pose. Depending on your install of OpenPose you may need to adjust the command slightly below to point to the OpenPose executable. I generated normalised keypoints so that image scale is irrelevant for determining the pose and limit the resolution and disable the display to work within my GPU resource constraints. Depending on your operating system use the %%cmd cell for Windows and the %%bash cell for Unix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ffe1a-6ffb-4125-aac5-e10ea1427573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%cmd\n",
    "mkdir -p .\\extracted_data\\human_poses\\Train\\Game\n",
    ".\\openpose\\bin\\OpenPoseDemo.exe --image_dir .\\extracted_data\\unclassified_human_patches\\Train\\Game --write_json .\\extracted_data\\human_poses\\Train\\Game --keypoint_scale 3 --net_resolution \"656x368\" --display 0 --render_pose 0\n",
    "mkdir .\\extracted_data\\human_poses\\Train\\Movie\n",
    ".\\openpose\\bin\\OpenPoseDemo.exe --image_dir .\\extracted_data\\unclassified_human_patches\\Train\\Movie --write_json .\\extracted_data\\human_poses\\Train\\Movie --keypoint_scale 3 --net_resolution \"656x368\" --display 0 --render_pose 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59a809-f067-4548-96a0-469714bad021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ./extracted_data/human_poses/Train/Game\n",
    "./openpose/build/examples/openpose/openpose.bin --image_dir ./extracted_data/unclassified_human_patches/Train/Game --write_json ./extracted_data/human_poses/Train/Game --keypoint_scale 3 --net_resolution \"656x368\" --display 0 --render_pose 0\n",
    "mkdir -p ./extracted_data/human_poses/Train/Movie\n",
    "./openpose/build/examples/openpose/openpose.bin --image_dir ./extracted_data/unclassified_human_patches/Train/Movie --write_json ./extracted_data/human_poses/Train/Movie --keypoint_scale 3 --net_resolution \"656x368\" --display 0 --render_pose 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bec2a-d860-4aef-a339-852705619ea8",
   "metadata": {},
   "source": [
    "Now that I have the extracted pose data saved as .json files in a one-to-one mapping with each human patch (although there may be multiple humans detected by the pose detector!), I process these using Python. Before this I need to define what I count as each class. In connection with Q1.3 I consider those that are facing away to be in the class 'Other' regardless of all other attributes. I group the joints and measure the visibility of each group, the groups are:\n",
    "\n",
    "* Head: Nose, Left Eye, Left Ear, Right Eye, Right Eye\n",
    "* Torso and Arms: Neck, Left Shoulder, Left Elbow, Left Wrist, Right Shoulder, Right Elbow, Right Wrist\n",
    "* Hips: Mid Hip, Left Hip, Right Hip\n",
    "* Legs: Left Knee, Left Ankle, Right Knee, Right Ankle\n",
    "* Feet: Left Ankle, Left Heel, Left Big Toe, Left Small Toe, Right Ankle, Right Heel, Right Big Toe, Right Small Toe\n",
    "\n",
    "Then to classify by pose:\n",
    "\n",
    "* Head Only: At least 60% of the Head group visible, less than 45% of the Torso and Arms group visible and none of the Hips, Legs or Feet groups visible\n",
    "* Half Body: At least 40% of the Head group visible, at least 50% of the Torso and Arms group visible, any amount of Hips, and none of the Legs or Feet groups visible\n",
    "* Full Body: At least 40% of the Head group visible, at least 70% of the Torso and Arms group visible, at least 50% of the Hips and Legs visible, and any amount of Feet visible\n",
    "\n",
    "Finally, to classify sitting and standing I measure the angle between the knees and hips. If this is less than 50 degrees they are sitting otherwise they are standing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e6d0ed4-cbc3-44ca-a12e-d53060ff500f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_folders = [\"Train/Movie\", \"Train/Game\"]\n",
    "base_patch_directory = \"./extracted_data/unclassified_human_patches\"\n",
    "base_pose_directory = \"./extracted_data/human_poses\"\n",
    "base_classification_directory = \"./extracted_data/classified_human_patches\"\n",
    "base_sample_directory = \"./extracted_data/human_patches_validation_set/raw\"\n",
    "base_sample_classified_directory = \"./extracted_data/human_patches_validation_set/hand_classified\"\n",
    "\n",
    "sample_count = 150\n",
    "\n",
    "classes = [\"Full Body Sitting\", \"Full Body Standing\", \"Half Body\", \"Head Only\", \"Other\"]\n",
    "show_skeleton = False\n",
    "enforce_facing = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f8ccf-7604-46d6-907c-336000d2c28e",
   "metadata": {},
   "source": [
    "Prior to classifying them, I sample 150 patches from each domain to classify by hand to evaluate the performance of the pose classification process. I put the classified files in the following directory: ./extracted_data/human_patches_validation_set/{base_folder}/{class_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb21c83-390c-42a4-9416-905d28017f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for base_folder in base_folders:\n",
    "    patch_directory = f\"{base_patch_directory}/{base_folder}\"\n",
    "    sampled_directory = f\"{base_sample_directory}/{base_folder}\"\n",
    "    \n",
    "    os.makedirs(sampled_directory) \n",
    "    \n",
    "    print(f\"{patch_directory} -> {sampled_directory}\")\n",
    "    \n",
    "    file_names = [file_name for file_name in os.listdir(patch_directory) if file_name.endswith(\".jpg\")]\n",
    "    sampled_file_names = random.sample(file_names, k=sample_count)\n",
    "    \n",
    "    for sampled in sampled_file_names:\n",
    "        shutil.copy(f\"{patch_directory}/{sampled}\", sampled_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77eecbfb-557a-4a82-b8e2-e2fe16865af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd467ac042e1442cb3e7d0d4ecaefff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384e857314f040748e6afa2f2700db32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for base_folder in base_folders:\n",
    "    patch_directory = f\"{base_patch_directory}/{base_folder}\"\n",
    "    pose_directory = f\"{base_pose_directory}/{base_folder}\"\n",
    "    output_directory = f\"{base_classification_directory}/{base_folder}\"\n",
    "    \n",
    "    for class_name in classes:\n",
    "        os.makedirs(f\"{output_directory}/{class_name}\")\n",
    "    \n",
    "    file_names = [''.join(file_name.split(\".\")[:-1]) for file_name in os.listdir(patch_directory) if file_name.endswith(\".jpg\")]\n",
    "    \n",
    "    for file_name in tqdm(file_names):\n",
    "        patch_loc = f\"{patch_directory}/{file_name}.jpg\"\n",
    "        keypoints_loc = f\"{pose_directory}/{file_name}_keypoints.json\"\n",
    "        \n",
    "        with open(keypoints_loc, \"r\") as fp:\n",
    "            raw_data = json.load(fp)\n",
    "\n",
    "        if len(raw_data[\"people\"]) != 1:\n",
    "            continue\n",
    "        \n",
    "        kps = raw_data[\"people\"][0][\"pose_keypoints_2d\"]\n",
    "        pose = PoseKeypoints.load_keypoints(kps)\n",
    "        \n",
    "        classification = pose.classify(enforce_facing=enforce_facing)\n",
    "        save_loc = f\"{output_directory}/{classification}/{file_name}.jpg\"\n",
    "        \n",
    "        patch = cv2.imread(patch_loc)\n",
    "    \n",
    "        if show_skeleton:\n",
    "            patch = pose.overlay_pose(patch)\n",
    "        \n",
    "        cv2.imwrite(save_loc, patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54778d69-932a-4d80-9945-a6b6591e4b6e",
   "metadata": {},
   "source": [
    "I now evaluate the results using the hand classified set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c068e3b-00c9-478f-b106-11cf0eeee208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7115384615384616\n",
      "Accuracy: 0.6017699115044248\n"
     ]
    }
   ],
   "source": [
    "for base_folder in base_folders:\n",
    "    classification_data = {}\n",
    "    hand_classified_folder = f\"{base_sample_classified_directory}/{base_folder}\"\n",
    "    pred_classified_folder = f\"{base_classification_directory}/{base_folder}\"\n",
    "    \n",
    "    for class_name in classes:\n",
    "        for file_name in os.listdir(f\"{hand_classified_folder}/{class_name}\"):\n",
    "            classification_data[file_name] = {\"ground_truth\": class_name, \"predicted\": None}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        classified_files = os.listdir(f\"{pred_classified_folder}/{class_name}\")\n",
    "        \n",
    "        for file_name in classification_data.keys():\n",
    "            if file_name in classified_files:\n",
    "                classification_data[file_name][\"predicted\"] = class_name\n",
    "                \n",
    "    correct = 0\n",
    "    \n",
    "    classification_data_rows = [{\"file\": key, **value} for key, value in classification_data.items() if value[\"predicted\"] is not None]\n",
    "    df = pd.DataFrame(classification_data_rows)\n",
    "    \n",
    "    df[\"match\"] = df.apply(lambda row: row[\"ground_truth\"] == row[\"predicted\"], axis=1)\n",
    "    \n",
    "    print(f\"Accuracy:\", df[\"match\"].sum() / len(df))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cad786-35c6-43c3-9014-7285f09ff347",
   "metadata": {},
   "source": [
    "I now sample 10 images from each class per domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78b5fec4-4314-40ab-b085-1cdb5cd2498f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_submission_folder = \"./data_for_submission/1_2_sampled_pose_classified\"\n",
    "sample_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e929c39-1fbb-4940-a7e0-54ff7e85a785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./extracted_data/classified_human_patches/Train/Movie/Full Body Sitting -> ./data_for_submission/1_2_sampled_pose_classified/Train/Movie/Full Body Sitting\n",
      "./extracted_data/classified_human_patches/Train/Movie/Full Body Standing -> ./data_for_submission/1_2_sampled_pose_classified/Train/Movie/Full Body Standing\n",
      "./extracted_data/classified_human_patches/Train/Movie/Half Body -> ./data_for_submission/1_2_sampled_pose_classified/Train/Movie/Half Body\n",
      "./extracted_data/classified_human_patches/Train/Movie/Head Only -> ./data_for_submission/1_2_sampled_pose_classified/Train/Movie/Head Only\n",
      "./extracted_data/classified_human_patches/Train/Movie/Other -> ./data_for_submission/1_2_sampled_pose_classified/Train/Movie/Other\n",
      "./extracted_data/classified_human_patches/Train/Game/Full Body Sitting -> ./data_for_submission/1_2_sampled_pose_classified/Train/Game/Full Body Sitting\n",
      "./extracted_data/classified_human_patches/Train/Game/Full Body Standing -> ./data_for_submission/1_2_sampled_pose_classified/Train/Game/Full Body Standing\n",
      "./extracted_data/classified_human_patches/Train/Game/Half Body -> ./data_for_submission/1_2_sampled_pose_classified/Train/Game/Half Body\n",
      "./extracted_data/classified_human_patches/Train/Game/Head Only -> ./data_for_submission/1_2_sampled_pose_classified/Train/Game/Head Only\n",
      "./extracted_data/classified_human_patches/Train/Game/Other -> ./data_for_submission/1_2_sampled_pose_classified/Train/Game/Other\n"
     ]
    }
   ],
   "source": [
    "for base_folder in base_folders:\n",
    "    for class_name in classes:\n",
    "        sampled_directory = f\"{base_submission_folder}/{base_folder}/{class_name}\"\n",
    "        os.makedirs(sampled_directory)\n",
    "        \n",
    "        class_folder = f\"{base_classification_directory}/{base_folder}/{class_name}\"\n",
    "        print(f\"{class_folder} -> {sampled_directory}\")\n",
    "        \n",
    "        file_names = [file_name for file_name in os.listdir(class_folder) if file_name.endswith(\".jpg\")]\n",
    "        sampled_file_names = random.sample(file_names, k=min(sample_count, len(file_names)))\n",
    "        \n",
    "        for sampled in sampled_file_names:\n",
    "            shutil.copy(f\"{class_folder}/{sampled}\", sampled_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8199a95-1e43-463e-a842-a1fa7cb9363b",
   "metadata": {},
   "source": [
    "## 1.3: Training Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb33ffd-be8f-455a-ba8d-e98f3751ffc8",
   "metadata": {},
   "source": [
    "Now that I have classified the human patches according to pose, I need to select the most appropriate data for training the model. I will use CycleGAN initially as such I need approximately 1200 images from both domains to form a quality dataset. This needs to be diverse (i.e. not just similar patches a couple of frames apart) and representative of the data. To achieve this I group human patches into sequences to reduce oversampling of extremely similar patches which will degrade the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "584e967c-8393-44e6-adc7-183b1653e898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_similar_images(ordered_dir, cutoff, window):\n",
    "    image_locs = [f\"{ordered_dir}/{file_name}\" for file_name in os.listdir(ordered_dir) if file_name.endswith(\".jpg\")]\n",
    "    groups = [[image_locs[0]]]\n",
    "    \n",
    "    for image_loc in tqdm(image_locs):\n",
    "        img_hash = imagehash.average_hash(Image.open(image_loc))\n",
    "        \n",
    "        closest_group_idx = -1\n",
    "        closest_group_diff = 65\n",
    "        \n",
    "        for offset, group in enumerate(groups[-window:][::-1]):\n",
    "            group_idx = len(groups) - offset - 1\n",
    "            last_hash = imagehash.average_hash(Image.open(group[-1]))\n",
    "            diff = img_hash - last_hash\n",
    "            \n",
    "            if diff < closest_group_diff:\n",
    "                closest_group_idx = group_idx\n",
    "                closest_group_diff = diff\n",
    "        \n",
    "        if closest_group_diff <= cutoff:\n",
    "            groups[closest_group_idx].append(image_loc)\n",
    "        else:\n",
    "            groups.append([image_loc])\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdb54f70-c40b-4e91-8411-cd238fe90c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_classes(base_directory, classes, cutoff, window):\n",
    "    groups = []\n",
    "    \n",
    "    for class_name in classes:\n",
    "        pose_directory = f\"{base_directory}/{class_name}\"\n",
    "        groups += group_similar_images(pose_directory, cutoff, window)\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53507d3a-73e2-45cf-8444-c14c5e4c780c",
   "metadata": {},
   "source": [
    "I discard all images from the 'Other' category as I determine that these will likely not be of use to the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "185105af-d72e-47ab-a456-a7a9793cd5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = [\"Full Body Sitting\", \"Full Body Standing\", \"Half Body\", \"Head Only\"]\n",
    "base_pose_directory = \"./extracted_data/classified_human_patches\"\n",
    "cutoff = 8\n",
    "window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8ac4355-ff6f-4c18-92bf-326d37612334",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41097799ecc34e87a4b26d75b12f7a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8ab9b6a427474e9c5889b0d6e51cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae50b62376154e7080ba9f6429b08baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abf87956f7841b0be59c9dca0fc8a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c698a40d899c40459e201478c521b5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b488610d8d4f46c79eae6ff533801448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b180650f72a0408ebf4e753f7498855a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b00713b7c2f4b9ca230d6fd228edcab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game_groups = group_classes(f\"{base_pose_directory}/Train/Game\", classes, cutoff, window) \n",
    "movie_groups = group_classes(f\"{base_pose_directory}/Train/Movie\", classes, cutoff, window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d0840-e205-4723-a4d6-f41bfcd6bee6",
   "metadata": {},
   "source": [
    "Now I sample from these sequences of images. I also discard images that are too small, as I will training on 128x128 images for CycleGAN I discard all images that are smaller than 64x64 pixels as they will be too poor quality to be useful when resized to 128x128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a09e6fbe-96fe-40a6-911b-785857ded481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_test_count = 250\n",
    "target_train_count = 1200\n",
    "output_size = 128\n",
    "min_size = 64\n",
    "sequence_proportion = 0.05\n",
    "\n",
    "train_output_directory = lambda base: f\"./extracted_data/cyclegan_training_data/{base}/Train\"\n",
    "test_output_directory = lambda base: f\"./extracted_data/cyclegan_training_data/{base}/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "526bf0bd-c47e-4be2-b30d-82b29a44eb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_train_test_split(groups, train_dir, test_dir, train_target, test_target, output_size, min_size, sequence_proportion):\n",
    "    os.makedirs(train_dir)\n",
    "    os.makedirs(test_dir)\n",
    "    \n",
    "    for group_no, group in enumerate(tqdm(groups)):\n",
    "        selected_patches = random.sample(range(len(group)), k=max(1, int(sequence_proportion * len(group))))\n",
    "        \n",
    "        for i in selected_patches:\n",
    "            patch = Image.open(group[i])\n",
    "            \n",
    "            if patch.width < min_size or patch.height < min_size:\n",
    "                continue\n",
    "                \n",
    "            patch = patch.resize((output_size, output_size))\n",
    "            patch.save(f\"{train_dir}/{group_no:05d}_{i:05d}.jpg\")\n",
    "    \n",
    "    valid_train_files = [file_name for file_name in os.listdir(train_dir) if file_name.endswith(\".jpg\")]\n",
    "    \n",
    "    if len(valid_train_files) <= train_target:\n",
    "        print(f\"Only found {len(valid_train_files)} valid files, not able to produce a test set\")\n",
    "        return\n",
    "    \n",
    "    available_for_test = min(len(valid_train_files) - train_target, test_target)\n",
    "    selected_for_test = random.sample(valid_train_files, k=available_for_test)\n",
    "    \n",
    "    for file_name in selected_for_test:\n",
    "        os.rename(f\"{train_dir}/{file_name}\", f\"{test_dir}/{file_name}\")\n",
    "    \n",
    "    print(\"Moved\", len(selected_for_test), \"files to test set\")\n",
    "    \n",
    "    valid_train_files = [file_name for file_name in os.listdir(train_dir) if file_name.endswith(\".jpg\")]\n",
    "    \n",
    "    if len(valid_train_files) > train_target:\n",
    "        selected_for_delete = random.sample(valid_train_files, k=len(valid_train_files) - train_target)\n",
    "    \n",
    "        for file_name in selected_for_delete:\n",
    "            os.remove(f\"{train_dir}/{file_name}\")\n",
    "\n",
    "        print(\"Deleted\", len(selected_for_delete), \"files from the train set\")\n",
    "    \n",
    "    print(\"Train Size:\", len([file_name for file_name in os.listdir(train_dir) if file_name.endswith(\".jpg\")]))\n",
    "    print(\"Test Size:\", len([file_name for file_name in os.listdir(test_dir) if file_name.endswith(\".jpg\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0d4ee0e-52a7-4c6c-9df0-2cd654ae99d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a973da479d4f62913c8c2fc98f12da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 250 files to test set\n",
      "Deleted 183 files from the train set\n",
      "Train Size: 1200\n",
      "Test Size: 250\n"
     ]
    }
   ],
   "source": [
    "create_train_test_split(game_groups, train_output_directory(\"Game\"), test_output_directory(\"Game\"), target_train_count, target_test_count, output_size, min_size, sequence_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d4ae863-9d32-4ae0-8a33-74cfb0b873b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1dcdfc1872474aaa7f9af65e9ac231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 106 files to test set\n",
      "Train Size: 1200\n",
      "Test Size: 106\n"
     ]
    }
   ],
   "source": [
    "create_train_test_split(movie_groups, train_output_directory(\"Movie\"), test_output_directory(\"Movie\"), target_train_count, target_test_count, output_size, min_size, sequence_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1beb9e-246f-486c-8258-da8300768d12",
   "metadata": {},
   "source": [
    "Now I sample 50 random images from each training dataset for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4578ac4d-5b07-479b-86f7-a4b3ff9904cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_submission_folder = \"./data_for_submission/1_3_sampled_training_data\"\n",
    "sample_count = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a472cde-0d2c-45e4-867a-41b8e32c30b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./extracted_data/cyclegan_training_data/Game/Train -> ./data_for_submission/1_3_sampled_training_data/Game\n",
      "./extracted_data/cyclegan_training_data/Movie/Train -> ./data_for_submission/1_3_sampled_training_data/Movie\n"
     ]
    }
   ],
   "source": [
    "for base_folder in [\"Game\", \"Movie\"]:\n",
    "    train_directory = train_output_directory(base_folder)\n",
    "    sampled_directory = f\"{base_submission_folder}/{base_folder}\"\n",
    "    \n",
    "    os.makedirs(sampled_directory) \n",
    "    \n",
    "    print(f\"{train_directory} -> {sampled_directory}\")\n",
    "    \n",
    "    file_names = [file_name for file_name in os.listdir(train_directory) if file_name.endswith(\".jpg\")]\n",
    "    sampled_file_names = random.sample(file_names, k=sample_count)\n",
    "    \n",
    "    for sampled in sampled_file_names:\n",
    "        shutil.copy(f\"{train_directory}/{sampled}\", sampled_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41ab59-62ba-465a-8298-5cb4ff6132c1",
   "metadata": {},
   "source": [
    "# Real-world Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c23dfb06-ff13-4870-9204-3bdb763c9e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from joint_dataset import JointDomainImageDataset, JointDomainTripletDataset\n",
    "from generator_model import Generator\n",
    "from cycle_gan import CycleGAN\n",
    "from recycle_gan import RecycleGAN\n",
    "from visdom_utils import MultiLinePlot\n",
    "from image_utils import revert_normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea568e-42df-4803-93fd-100e43829d32",
   "metadata": {},
   "source": [
    "## 2.1: Image Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873d60a-2017-444a-b0b0-d6a65c6ba043",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba5805c7-f918-4c0c-a0f0-2bd1619ff290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using_google_gpus = False\n",
    "\n",
    "train_X_loc = \"./extracted_data/cyclegan_training_data/Game/Train\" \n",
    "test_X_loc = \"./extracted_data/cyclegan_training_data/Game/Test\" \n",
    "\n",
    "train_Y_loc = \"./extracted_data/cyclegan_training_data/Movie/Train\"\n",
    "test_Y_loc = \"./extracted_data/cyclegan_training_data/Movie/Test\"\n",
    "\n",
    "run_data_directory = \"./runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c969af7-5f6e-4c5a-be28-6b7b804d9fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if using_google_gpus:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    vis = None\n",
    "else:\n",
    "    import visdom\n",
    "    vis = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85931a-29ad-4425-9496-fb29bb56aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/cyclegan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff0c67-274c-4e1a-870b-a28a7da84645",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JointDomainImageDataset(train_X_loc, train_Y_loc, train=True, img_size=128)\n",
    "test_dataset = JointDomainImageDataset(test_X_loc, test_Y_loc, train=False, img_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d81a74-7359-4197-9936-623d4e14ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xs = []\n",
    "test_ys = []\n",
    "\n",
    "for i in random.sample(range(0, len(test_dataset)), 16):\n",
    "    x, y = test_dataset[i]\n",
    "    test_xs.append(x)\n",
    "    test_ys.append(y)\n",
    "\n",
    "test_xs = torch.stack(test_xs)\n",
    "test_ys = torch.stack(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9538f2-ad81-4828-80e3-71c2a7356078",
   "metadata": {},
   "outputs": [],
   "source": [
    "if vis is not None:\n",
    "    vis.images(torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in test_xs]), nrow=4, opts={\"title\": \"X_test originals\"})\n",
    "    vis.images(torch.stack([revert_normalisation(y).permute(2, 0, 1) for y in test_ys]), nrow=4, opts={\"title\": \"Y_test originals\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c26f71-e553-41c5-b442-da2a63ede0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = test_dataset[random.randint(0, len(test_dataset))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf14622-23f3-4f77-86a7-167e2ac2ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_img.min(), test_img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72827ab8-0ef3-41bf-b415-c56f6f7a1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2847bf-e756-486a-83e4-a947b9bfc619",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(revert_normalisation(test_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a1a63-e7d0-4d5b-9a5c-4b8c795e2358",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0649aa7-7fe1-4600-8297-5e6e45aafb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 100\n",
    "lambda_weight = 10\n",
    "lambda_idt_X = 0.5\n",
    "lambda_idt_Y = 0.5\n",
    "\n",
    "blocks = 6\n",
    "upsample_strategy = \"upsample\"\n",
    "pool_size = 50\n",
    "opt_scheduler_type = \"linear_decay_with_warmup\"\n",
    "\n",
    "checkpoint_instance_dir = None\n",
    "checkpoint_epoch_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c021f-d634-4c02-a457-8076d2df8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f30f66-d599-4931-8857-09b481b287a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint_instance_dir is not None and checkpoint_epoch_dir is not None:\n",
    "    cyclegan = CycleGAN.load(f\"{run_data_directory}/{checkpoint_instance_dir}\", f\"{checkpoint_epoch_dir}\", device, blocks)\n",
    "else:\n",
    "    cyclegan = CycleGAN(blocks, upsample_strategy, device, pool_size, opt_scheduler_type, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2304f-045b-421e-a2fd-66221c2a1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{cyclegan.save_folder}/info_{checkpoint_epoch_dir}.json\", \"w+\") as fp:\n",
    "    json.dump({\n",
    "        \"block_count\": cyclegan.resnet_block_count,\n",
    "        \"upsample_strategy\": upsample_strategy,\n",
    "        \"pool_size\": pool_size,\n",
    "        \"opt_scheduler_type\": opt_scheduler_type,\n",
    "        \"data_folders\": {\n",
    "            \"train_X\": train_X_loc,\n",
    "            \"test_X\": test_X_loc,\n",
    "            \"train_Y\": train_Y_loc,\n",
    "            \"test_Y\": test_Y_loc\n",
    "        },\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_epochs\": epochs,\n",
    "        \"start_epoch\": cyclegan.start_epoch,\n",
    "        \"lambda_weight\": lambda_weight,\n",
    "        \"lambda_idt_X\": lambda_idt_X,\n",
    "        \"lambda_idt_Y\": lambda_idt_Y,\n",
    "        \"checkpoint\": {\n",
    "            \"instance\": checkpoint_instance_dir,\n",
    "            \"epoch\": checkpoint_epoch_dir\n",
    "        }\n",
    "    }, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e1468-fb3d-4eae-b228-d1c99c545808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could also enable random flipping\n",
    "def generate_noisy_labels(shape, real, device):\n",
    "    # Randomly generated between 0 and 1\n",
    "    labels = torch.rand(shape, device=device)\n",
    "    \n",
    "    if real:\n",
    "        # Now they are between 0.7 and 1.1\n",
    "        labels = (2 * labels / 5) + 0.7\n",
    "    else:\n",
    "        # Now they are between 0 and 0.3\n",
    "        labels = (labels * 3) / 10\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04939a-dc0e-4dee-9322-653cff0a8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_loss(real, fake, pool, discriminator, loss_func):\n",
    "    # Discriminator should give (1) for a real image and (0) for a fake\n",
    "    real_pred = discriminator(real)\n",
    "    real_loss = loss_func(real_pred, generate_noisy_labels(real_pred.shape, True, device)) # Should dampen?\n",
    "    \n",
    "    # We draw from the history buffer\n",
    "    pool_fake = pool.randomise_existing_batch(fake)\n",
    "    fake_pred = discriminator(pool_fake)\n",
    "    # Fake images should not fool the discriminator\n",
    "    fake_loss = loss_func(fake_pred.detach(), generate_noisy_labels(fake_pred.shape, False, device))\n",
    "    \n",
    "    avg_loss = (real_loss + fake_loss) * 0.5\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cd1af-4317-40ec-9a0c-f2eb6342a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cyclegan.start_epoch != 0:\n",
    "    print(f\"Resuming training from epoch {cyclegan.start_epoch + 1}\")\n",
    "else:\n",
    "    print(\"Starting training from scratch\")\n",
    "\n",
    "if vis is not None:\n",
    "    loss_plot = MultiLinePlot(vis, \"Losses\", \"Epoch\", \"Loss\")\n",
    "\n",
    "for epoch in range(cyclegan.start_epoch + 1, epochs + 1):\n",
    "    cyclegan.G.train()\n",
    "    cyclegan.F.train()\n",
    "    \n",
    "    cyclegan.D_X.train()\n",
    "    cyclegan.D_Y.train()\n",
    "\n",
    "    batch_start_time = time.time()\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    cum_loss_idt_x = 0\n",
    "    cum_loss_idt_y = 0\n",
    "    cum_G_fool_loss = 0\n",
    "    cum_F_fool_loss = 0\n",
    "    cum_cycled_x_loss = 0\n",
    "    cum_cycled_y_loss = 0\n",
    "    cum_D_X_loss = 0\n",
    "    cum_D_Y_loss = 0\n",
    "    \n",
    "    ep_loss_idt_x = 0\n",
    "    ep_loss_idt_y = 0\n",
    "    ep_G_fool_loss = 0\n",
    "    ep_F_fool_loss = 0\n",
    "    ep_cycled_x_loss = 0\n",
    "    ep_cycled_y_loss = 0\n",
    "    ep_D_X_loss = 0\n",
    "    ep_D_Y_loss = 0\n",
    "    \n",
    "    for batch_no, (x, y) in enumerate(dataloader):\n",
    "        # Load the sequence of frames to the GPU\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Firstly, generate fake images in domain Y using the x_t images\n",
    "        fake_y = cyclegan.G(x)\n",
    "        \n",
    "        # Then generate the fake images in domain X using the y_t images\n",
    "        fake_x = cyclegan.F(y)\n",
    "        \n",
    "        ### GENERATOR TRAINING\n",
    "        \n",
    "        # Freeze discriminator weights\n",
    "        for param in cyclegan.D_X.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in cyclegan.D_Y.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Zero the gradients for the generators and predictors\n",
    "        cyclegan.G_opt.zero_grad()\n",
    "        cyclegan.F_opt.zero_grad()\n",
    "        \n",
    "        # Calculate the identity loss, this tries to enforce that G(y) = I(y) = y i.e. the identity\n",
    "        idt_x = cyclegan.F(x)\n",
    "        loss_idt_x = cyclegan.identity_loss(idt_x, x)\n",
    "        \n",
    "        idt_y = cyclegan.G(y)\n",
    "        loss_idt_y = cyclegan.identity_loss(idt_y, y)\n",
    "        \n",
    "        # Now we try and fool the discriminators\n",
    "        # D_X tries to tell if an image is from X (1) or from F(Y) (0) so it takes input fake_x_t\n",
    "        # D_X is supposed to be 1 if the image is from X but here we are trying to get it to be 1 if it is from G(X) -> Y which is incorrect\n",
    "        G_fool = cyclegan.D_Y(fake_y) \n",
    "        # We want to tell if fake_y_0 has fooled D_X, so we measure how far from the output 1 it is \n",
    "        G_fool_loss = cyclegan.gan_loss(G_fool, generate_noisy_labels(G_fool.shape, True, device))\n",
    "        \n",
    "        # D_Y tries to tell if an image is from Y (1) or from G(X) (0) so it takes input fake_y_t\n",
    "        F_fool = cyclegan.D_X(fake_x)\n",
    "        # We want to tell if fake_x_0 has fooled D_Y, so we measure how far from the output 1 it is \n",
    "        F_fool_loss = cyclegan.gan_loss(F_fool, generate_noisy_labels(F_fool.shape, True, device))\n",
    "        \n",
    "        # Now do the cycle loss\n",
    "        cycled_x = cyclegan.F(fake_y)\n",
    "        cycled_loss_x = cyclegan.cycle_loss(cycled_x, x)\n",
    "        \n",
    "        cycled_y = cyclegan.G(fake_x)\n",
    "        cycled_loss_y = cyclegan.cycle_loss(cycled_y, y)\n",
    "        \n",
    "        # Backpropagate and step the gradients\n",
    "        generator_loss = G_fool_loss + F_fool_loss + lambda_weight * (cycled_loss_x + cycled_loss_y + lambda_idt_X * loss_idt_x + lambda_idt_Y * loss_idt_y)\n",
    "        generator_loss.backward()\n",
    "        \n",
    "        cyclegan.G_opt.step()\n",
    "        cyclegan.F_opt.step()\n",
    "        \n",
    "        ### DISCRIMINATOR TRAINING\n",
    "        # Unfreeze discriminator weights\n",
    "        for param in cyclegan.D_X.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in cyclegan.D_Y.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Zero the gradients\n",
    "        cyclegan.D_X_opt.zero_grad()\n",
    "        cyclegan.D_Y_opt.zero_grad()\n",
    "        \n",
    "        D_X_loss = get_discriminator_loss(x, fake_x, cyclegan.fake_X_buffer, cyclegan.D_X, cyclegan.gan_loss)\n",
    "        \n",
    "        D_Y_loss = get_discriminator_loss(y, fake_y, cyclegan.fake_Y_buffer, cyclegan.D_Y, cyclegan.gan_loss)\n",
    "        \n",
    "        # Backpropagate and step the gradients\n",
    "        D_loss = D_X_loss + D_Y_loss\n",
    "        D_loss.backward()\n",
    "        \n",
    "        cyclegan.D_X_opt.step()\n",
    "        cyclegan.D_Y_opt.step()\n",
    "        \n",
    "        ### UPDATE POOLS\n",
    "        cyclegan.fake_X_buffer.add(fake_x.detach())\n",
    "        cyclegan.fake_Y_buffer.add(fake_y.detach())\n",
    "        \n",
    "        ### TRACK LOSS\n",
    "        cum_loss_idt_x += loss_idt_x.item()\n",
    "        cum_loss_idt_y += loss_idt_y.item()\n",
    "        cum_G_fool_loss += G_fool_loss.item()\n",
    "        cum_F_fool_loss += F_fool_loss.item()\n",
    "        cum_cycled_x_loss += cycled_loss_x.item()\n",
    "        cum_cycled_y_loss += cycled_loss_y.item()\n",
    "        cum_D_X_loss += D_X_loss.item()\n",
    "        cum_D_Y_loss += D_Y_loss.item()\n",
    "        \n",
    "        ep_loss_idt_x += loss_idt_x.item()\n",
    "        ep_loss_idt_y += loss_idt_y.item()\n",
    "        ep_G_fool_loss += G_fool_loss.item()\n",
    "        ep_F_fool_loss += F_fool_loss.item()\n",
    "        ep_cycled_x_loss += cycled_loss_x.item()\n",
    "        ep_cycled_y_loss += cycled_loss_y.item()\n",
    "        ep_D_X_loss += D_X_loss.item()\n",
    "        ep_D_Y_loss += D_Y_loss.item()\n",
    "        \n",
    "        if epoch == 0 and batch_no < 55:\n",
    "            print(f\"[{epoch}:{batch_no}] fake_X_buffer: {len(cyclegan.fake_X_buffer)}, fake_Y_buffer: {len(cyclegan.fake_Y_buffer)}\")\n",
    "        \n",
    "        if batch_no % 100 == 0 and batch_no != 0: \n",
    "            duration = time.time() - batch_start_time\n",
    "            \n",
    "            updated_losses = {\n",
    "                \"loss_idt_x\": cum_loss_idt_x / 100,\n",
    "                \"loss_idt_y\": cum_loss_idt_y / 100,\n",
    "                \"G_fool_loss\": cum_G_fool_loss / 100,\n",
    "                \"F_fool_loss\": cum_F_fool_loss / 100,\n",
    "                \"cycled_x_loss\": cum_cycled_x_loss / 100,\n",
    "                \"cycled_y_loss\": cum_cycled_y_loss / 100,\n",
    "                \"D_X_loss\": cum_D_X_loss / 100,\n",
    "                \"D_Y_loss\": cum_D_Y_loss / 100\n",
    "            }\n",
    "            \n",
    "            x_loss_str = f\"[{epoch}:{batch_no}]\"\n",
    "            y_loss_str = f\"[{epoch}:{batch_no}]\"\n",
    "            \n",
    "            for i, (key, value) in enumerate(updated_losses.items()):\n",
    "                if i % 2 == 0:\n",
    "                    x_loss_str = f\"{x_loss_str} {key}: {value},\"\n",
    "                else:\n",
    "                    y_loss_str = f\"{y_loss_str} {key}: {value},\"\n",
    "            \n",
    "            x_loss_str = x_loss_str[:-1]\n",
    "            y_loss_str = y_loss_str[:-1]\n",
    "\n",
    "            print(f\"[{epoch}:{batch_no}] Took {duration:.2f}s\")\n",
    "            print(x_loss_str)\n",
    "            print(y_loss_str)\n",
    "            print(f\"[{epoch}:{batch_no}] fake_X_buffer: {len(cyclegan.fake_X_buffer)}, fake_Y_buffer: {len(cyclegan.fake_Y_buffer)}\")\n",
    "            \n",
    "            if vis is not None:\n",
    "                loss_plot.append_values(epoch + batch_no / len(dataloader), updated_losses)\n",
    "            \n",
    "            cum_loss_idt_x = 0\n",
    "            cum_loss_idt_y = 0\n",
    "            cum_G_fool_loss = 0\n",
    "            cum_F_fool_loss = 0\n",
    "            cum_cycled_x_loss = 0\n",
    "            cum_cycled_y_loss = 0\n",
    "            cum_D_X_loss = 0\n",
    "            cum_D_Y_loss = 0\n",
    "\n",
    "            batch_start_time = time.time()\n",
    "    \n",
    "    print(f\"[{epoch}:END] Completed epoch in {time.time() - epoch_start_time}s\")\n",
    "    \n",
    "    print(f\"[{epoch}:{batch_no}]\", \n",
    "          f\"ep_loss_idt_x: {ep_loss_idt_x / len(dataloader):.3f}\", \n",
    "          f\"ep_G_fool_loss: {ep_G_fool_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_cycled_x_loss: {ep_cycled_x_loss / len(dataloader):.3f}\",\n",
    "          f\"ep_D_X_loss: {ep_D_X_loss / len(dataloader):.3f}\")\n",
    "    \n",
    "    print(f\"[{epoch}:{batch_no}]\", \n",
    "          f\"ep_loss_idt_y: {ep_loss_idt_y / len(dataloader):.3f}\", \n",
    "          f\"ep_F_fool_loss: {ep_F_fool_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_cycled_y_loss: {ep_cycled_y_loss / len(dataloader):.3f}\",\n",
    "          f\"ep_D_Y_loss: {ep_D_Y_loss / len(dataloader):.3f}\")\n",
    "    \n",
    "    cyclegan.G.eval()\n",
    "    cyclegan.F.eval()\n",
    "    \n",
    "    if vis is not None:\n",
    "        eval_start_time = time.time()\n",
    "\n",
    "        G_eval_forward = cyclegan.apply(test_xs, x_to_y=True)\n",
    "        F_eval_forward = cyclegan.apply(test_ys, x_to_y=False)\n",
    "\n",
    "        G_rev = cyclegan.apply(G_eval_forward, x_to_y=False)\n",
    "        F_rev = cyclegan.apply(F_eval_forward, x_to_y=True)\n",
    "\n",
    "        G_eval_forward = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in G_eval_forward])\n",
    "        F_eval_forward = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in F_eval_forward])\n",
    "        G_rev = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in G_rev])\n",
    "        F_rev = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in F_rev])\n",
    "\n",
    "        G_eval_grid = torchvision.utils.make_grid(G_eval_forward, nrow=4)\n",
    "        F_eval_grid = torchvision.utils.make_grid(F_eval_forward, nrow=4)\n",
    "        G_rev_grid = torchvision.utils.make_grid(G_rev, nrow=4)\n",
    "        F_rev_grid = torchvision.utils.make_grid(F_rev, nrow=4)\n",
    "        \n",
    "        folder = f\"{cyclegan.save_folder}/{epoch}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        \n",
    "        torchvision.utils.save_image(G_eval_grid, f\"{folder}/X_to_Y.png\")\n",
    "        torchvision.utils.save_image(F_eval_grid, f\"{folder}/Y_to_X.png\")\n",
    "        torchvision.utils.save_image(G_rev_grid, f\"{folder}/X_to_Y_to_X.png\")\n",
    "        torchvision.utils.save_image(F_rev_grid, f\"{folder}/Y_to_X_to_Y.png\")\n",
    "\n",
    "        vis.image(G_eval_grid, win=\"G_eval\", opts={\n",
    "            \"caption\": f\"X -> Y evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(F_eval_grid, win=\"F_eval\", opts={\n",
    "            \"caption\": f\"Y -> X evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(G_rev_grid, win=\"G_rev\", opts={\n",
    "            \"caption\": f\"X -> Y -> X evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(F_rev_grid, win=\"F_rev\", opts={\n",
    "            \"caption\": f\"Y -> X -> Y evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        print(f\"[{epoch}:END] Completed eval in {time.time() - eval_start_time}s\")\n",
    "\n",
    "    cyclegan.G.train()\n",
    "    cyclegan.F.train()\n",
    "\n",
    "    cyclegan.step_learning_rates()\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"[{epoch}:END] Saving models and training information permanently\")\n",
    "        cyclegan.save(epoch, full_save=True)\n",
    "        cyclegan.save(epoch, full_save=True, folder=\"latest\")\n",
    "    else:\n",
    "        print(f\"[{epoch}:END] Saving models and training information temporarily to latest and saving generators permanently\")\n",
    "        cyclegan.save(epoch, full_save=True, folder=\"latest\")\n",
    "        cyclegan.save(epoch, full_save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d6d6b-fae0-4c73-b560-f026ed9904cf",
   "metadata": {},
   "source": [
    "### Video Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83fd8001-fc8a-4e29-ba96-c86ef6b2112e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_preprocess_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4e931fd-95ca-43ec-96f6-6554dfa275e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transfer_style_to_batch(cv2_images, model):\n",
    "    imgs = [video_preprocess_transform(img) for img in cv2_images]\n",
    "    imgs = torch.stack(imgs).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        imgs_transferred = model(imgs)\n",
    "    \n",
    "    imgs_transferred = [revert_normalisation(img_t.cpu()) for img_t in imgs_transferred]\n",
    "    return imgs_transferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0163f7f0-9dcc-49fe-b606-2101bea4e60a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_video(video_path, save_loc, model, batch_size=4):\n",
    "    # Try to delete the existing saved file\n",
    "    try:\n",
    "        os.remove(save_loc)\n",
    "    except OSError:\n",
    "        pass \n",
    "    \n",
    "    video = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    success, frame = video.read()\n",
    "    height, width, _ = frame.shape\n",
    "    \n",
    "    frame_buffer = []\n",
    "    video_out = cv2.VideoWriter(save_loc, -1, fps, (width, height))\n",
    "    \n",
    "    with tqdm(total=total_frames) as progress:\n",
    "        while success:\n",
    "            frame_buffer.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            if len(frame_buffer) == batch_size:\n",
    "                transferred_frames = transfer_style_to_batch(frame_buffer, model)\n",
    "\n",
    "                for out_frame in transferred_frames:\n",
    "                    coloured_out_frame = cv2.cvtColor((out_frame.numpy() * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "                    video_out.write(coloured_out_frame)\n",
    "\n",
    "                frame_buffer = []\n",
    "\n",
    "            success, frame = video.read()\n",
    "            progress.update(1)\n",
    "    \n",
    "    if len(frame_buffer) != 0:\n",
    "        # Process any additional final frames that don't make a full batch\n",
    "        transferred_frames = transfer_style_to_batch(frame_buffer, model)\n",
    "\n",
    "        for out_frame in transferred_frames:\n",
    "            coloured_out_frame = cv2.cvtColor((out_frame.numpy() * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "            video_out.write(coloured_out_frame)\n",
    "    \n",
    "    video_out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8ab9e22d-f9c6-4f4e-bd32-866e43d164cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_time = \"1680981254.7780375\"\n",
    "model_parent_directory = f\"./runs/CycleGAN/{model_time}\"\n",
    "epoch_directory = \"latest\"\n",
    "model_name = \"G.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0265f3d6-ba8b-45e7-b26a-60668fee48e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{model_parent_directory}/info_None.json\", \"r\") as fp:\n",
    "    model_info = json.load(fp)\n",
    "\n",
    "upsample_strategy = model_info[\"upsample_strategy\"]\n",
    "block_count = model_info[\"block_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "81de7445-e038-41ed-90e9-3bba84193397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Generator(block_count, upsample_strategy).to(device)\n",
    "model.load_state_dict(torch.load(f\"{model_parent_directory}/{epoch_directory}/{model_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0abb0ca0-fb9e-4f1b-a5ed-4c5c169c44d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_video_path = \"./original_data/Test/Test Movie.mp4\"\n",
    "output_video_path = f\"./video_transfer/cyclegan_{model_time}.mp4\"\n",
    "batch_size = 4\n",
    "\n",
    "os.makedirs(\"./video_transfer\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "58bae42e-83e9-4026-b02b-03bdee9bfffb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e4971085d94f63b4054db1d7085101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_video(input_video_path, output_video_path, model, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627c870-ef8e-4dc9-9cab-73ce48b3db99",
   "metadata": {},
   "source": [
    "## 2.2: Local (temporal) Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44cf89-80a9-4d27-9cf1-d7945b9fa747",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Generation\n",
    "Unlike CycleGAN, the data generated in Question 1 cannot be used directly with RecycleGAN. I firstly need to process the data from Q1.2 to create triplets. These are 3 consecutive human patches concatenated together. There is no overlap between triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2b10ce9-f7cf-4309-a96c-f72b0744ea21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236705e1b5794347b71f7d56339f3d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36480e700be84eb49e2e14a3ff8a6606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f56ec7e57c446c4a700caa7db9437ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb790c74f1b241de83e9cda23bb5b8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4feb2120f447bca9186968261fffa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f04c7ec7c54ce3974a3a37034b0eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557288e84edd40a0af8aa913c6a93abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429996ef59e94818b615e8bd595be1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = [\"Full Body Sitting\", \"Full Body Standing\", \"Half Body\", \"Head Only\"]\n",
    "base_pose_directory = \"./extracted_data/classified_human_patches\"\n",
    "cutoff = 8\n",
    "window = 5\n",
    "\n",
    "game_groups = group_classes(f\"{base_pose_directory}/Train/Game\", classes, cutoff, window) \n",
    "movie_groups = group_classes(f\"{base_pose_directory}/Train/Movie\", classes, cutoff, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "816db814-e3b7-4949-bb49-c7cc7ce93d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_test_count = 250\n",
    "target_train_count = 600\n",
    "min_triplets_from_clip = 3\n",
    "max_triplets_from_clip = 5\n",
    "output_size = 256\n",
    "min_size = 96\n",
    "\n",
    "train_output_directory = lambda base: f\"./extracted_data/recyclegan_training_data/{base}/Train\"\n",
    "test_output_directory = lambda base: f\"./extracted_data/recyclegan_training_data/{base}/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a036bb7-8a95-4ae4-9c5b-d8dbec54431f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_available_triplet_lengths(groups, min_triplets_from_clip):\n",
    "    lengths = {}\n",
    "    min_length = min_triplets_from_clip * 3\n",
    "    excluded_count = 0\n",
    "\n",
    "    for group in groups:\n",
    "        l = len(group)\n",
    "\n",
    "        if l < min_length:\n",
    "            excluded_count += l\n",
    "            continue\n",
    "\n",
    "        if l not in lengths.keys():\n",
    "            lengths[l] = 0\n",
    "\n",
    "        lengths[l] += 1\n",
    "    \n",
    "    return lengths, excluded_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c34b137-5f3b-4d4f-919a-946dc7db3d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_recyclegan_train_test_split(groups, train_dir, test_dir, train_count, test_count, min_triplets_from_clip, max_triplets_from_clip):\n",
    "    os.makedirs(train_dir)\n",
    "    os.makedirs(test_dir)\n",
    "    \n",
    "    lengths, excluded_count = compute_available_triplet_lengths(groups, min_triplets_from_clip)\n",
    "    test_save_prob = test_count / excluded_count\n",
    "    min_length = min_triplets_from_clip * 3\n",
    "    \n",
    "    for group_no, group in enumerate(tqdm(groups)):\n",
    "        if len(group) < min_length:\n",
    "            for i in range(len(group)):\n",
    "                if random.uniform(0, 1) < test_save_prob:\n",
    "                    Image.open(group[i]).save(f\"{test_dir}/{group_no:05d}_{i:05d}.jpg\")\n",
    "\n",
    "            continue \n",
    "        \n",
    "        triplet_count = len(group) // 3\n",
    "        selection_count = min(triplet_count, max_triplets_from_clip)\n",
    "        selected_triplets = random.sample(range(triplet_count), k=selection_count)\n",
    "        \n",
    "        for i in selected_triplets:\n",
    "            triplet_image = Image.new(\"RGB\", (3 * output_size, output_size))\n",
    "\n",
    "            x = Image.open(group[3 * i])\n",
    "            y = Image.open(group[3 * i + 1])\n",
    "            z = Image.open(group[3 * i + 2])\n",
    "\n",
    "            if x.width < min_size or x.height < min_size:\n",
    "                continue\n",
    "\n",
    "            if y.width < min_size or y.height < min_size:\n",
    "                continue\n",
    "\n",
    "            if z.width < min_size or z.height < min_size:\n",
    "                continue\n",
    "\n",
    "            triplet_image.paste(x.resize((output_size, output_size)), (0, 0))\n",
    "            triplet_image.paste(y.resize((output_size, output_size)), (output_size, 0))\n",
    "            triplet_image.paste(z.resize((output_size, output_size)), (2 * output_size, 0))\n",
    "\n",
    "            triplet_image.save(f\"{train_dir}/{group_no:05d}_{i:05d}.jpg\")\n",
    "    \n",
    "    valid_input_files = [file_name for file_name in os.listdir(train_dir) if file_name.endswith(\".jpg\")]\n",
    "    print(\"Uncapped Train Size:\", len(valid_input_files))\n",
    "    \n",
    "    if len(valid_input_files) > train_count:\n",
    "        selected_for_delete = random.sample(valid_input_files, k=len(valid_input_files) - train_count)\n",
    "\n",
    "        for file_name in selected_for_delete:\n",
    "            os.remove(f\"{train_dir}/{file_name}\")\n",
    "\n",
    "        print(\"Deleted\", len(selected_for_delete), \"files\")\n",
    "    \n",
    "    print(\"Capped Train Size:\", len([file_name for file_name in os.listdir(train_dir) if file_name.endswith(\".jpg\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "114cec97-7ac9-48e7-86d1-050b7d99db04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: './extracted_data/recyclegan_training_data/Game/Train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcreate_recyclegan_train_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_output_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_output_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_train_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_test_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_triplets_from_clip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_triplets_from_clip\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 2\u001b[0m, in \u001b[0;36mcreate_recyclegan_train_test_split\u001b[1;34m(groups, train_dir, test_dir, train_count, test_count, min_triplets_from_clip, max_triplets_from_clip)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_recyclegan_train_test_split\u001b[39m(groups, train_dir, test_dir, train_count, test_count, min_triplets_from_clip, max_triplets_from_clip):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(test_dir)\n\u001b[0;32m      5\u001b[0m     lengths, excluded_count \u001b[38;5;241m=\u001b[39m compute_available_triplet_lengths(groups, min_triplets_from_clip)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: './extracted_data/recyclegan_training_data/Game/Train'"
     ]
    }
   ],
   "source": [
    "create_recyclegan_train_test_split(game_groups, train_output_directory(\"Game\"), test_output_directory(\"Game\"), target_train_count, target_test_count, min_triplets_from_clip, max_triplets_from_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2c27f2d7-cfe1-4787-a439-b941fdbd4729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9125c773c432403daec37d8ba5d50164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncapped Train Size: 689\n",
      "Deleted 89 files\n",
      "Capped Train Size: 600\n"
     ]
    }
   ],
   "source": [
    "create_recyclegan_train_test_split(movie_groups, train_output_directory(\"Movie\"), test_output_directory(\"Movie\"), target_train_count, target_test_count, min_triplets_from_clip, max_triplets_from_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a0033-fdea-409f-8969-977ba273d586",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e26624e2-4f41-4674-b40e-2b08380f0325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using_google_gpus = False\n",
    "\n",
    "train_X_loc = \"./extracted_data/recyclegan_training_data/Game/Train\" \n",
    "test_X_loc = \"./extracted_data/recyclegan_training_data/Game/Test\" \n",
    "\n",
    "train_Y_loc = \"./extracted_data/recyclegan_training_data/Movie/Train\" \n",
    "test_Y_loc = \"./extracted_data/recyclegan_training_data/Movie/Test\" \n",
    "\n",
    "run_data_directory = \"./runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49bb616-39f9-49d2-9f69-ec617524b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_google_gpus:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    vis = None\n",
    "else:\n",
    "    import visdom\n",
    "    vis = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cb104-c5c2-45f8-828e-190864efe112",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/cyclegan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a0d6745-5b4c-4393-b14f-f785d696ce17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "JointDomainImageDataset.__init__() missing 1 required positional argument: 'load_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m JointDomainTripletDataset(train_X_loc, train_Y_loc, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mJointDomainImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_X_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_Y_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: JointDomainImageDataset.__init__() missing 1 required positional argument: 'load_size'"
     ]
    }
   ],
   "source": [
    "train_dataset = JointDomainTripletDataset(train_X_loc, train_Y_loc, train=True, img_size=256)\n",
    "test_dataset = JointDomainImageDataset(test_X_loc, test_Y_loc, train=False, img_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f3307-79a8-4b5a-8a59-509326530f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xs = []\n",
    "test_ys = []\n",
    "\n",
    "for i in random.sample(range(0, len(test_dataset)), 16):\n",
    "    x, y = test_dataset[i]\n",
    "    test_xs.append(x)\n",
    "    test_ys.append(y)\n",
    "\n",
    "test_xs = torch.stack(test_xs)\n",
    "test_ys = torch.stack(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53467f-44bb-44a5-b5cc-d0044cff8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "if vis is not None:\n",
    "    vis.images(torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in test_xs]), nrow=4, opts={\"title\": \"X_test originals\"})\n",
    "    vis.images(torch.stack([revert_normalisation(y).permute(2, 0, 1) for y in test_ys]), nrow=4, opts={\"title\": \"Y_test originals\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04515947-c0d7-4c18-9bef-e13c1c346751",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = test_dataset[random.randint(0, len(test_dataset))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8e894-58f5-4ffc-9e57-8b696be0d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_img.min(), test_img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e79b33-6f23-483e-99ac-d9e5b0255d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e8195-d39d-4d75-9a5e-9afbb456fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(revert_normalisation(test_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a713bdd-a544-45a1-988c-d2449219011d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeadc15-172b-4213-8dd7-595c6c1e04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 100\n",
    "lambda_weight = 5\n",
    "lambda_idt_X = 0.5\n",
    "lambda_idt_Y = 0.5\n",
    "\n",
    "blocks = 6\n",
    "upsample_strategy = \"upsample\"\n",
    "pool_size = 20\n",
    "opt_scheduler_type = \"linear_decay_with_warmup\"\n",
    "\n",
    "checkpoint_instance_dir = None\n",
    "checkpoint_epoch_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658f697-ca61-4a6f-ba1f-abd9cc485f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c31e2-fe43-420a-ac7f-094a3559ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint_instance_dir is not None and checkpoint_epoch_dir is not None:\n",
    "    cyclegan = RecycleGAN.load(f\"{run_data_directory}/{checkpoint_instance_dir}\", f\"{checkpoint_epoch_dir}\", device, blocks)\n",
    "else:\n",
    "    cyclegan = RecycleGAN(blocks, upsample_strategy, device, pool_size, opt_scheduler_type, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442bcbf-8ac7-4a62-a1a1-31bc35b7d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{cyclegan.save_folder}/info_{checkpoint_epoch_dir}.json\", \"w+\") as fp:\n",
    "    json.dump({\n",
    "        \"block_count\": cyclegan.resnet_block_count,\n",
    "        \"upsample_strategy\": upsample_strategy,\n",
    "        \"pool_size\": pool_size,\n",
    "        \"opt_scheduler_type\": opt_scheduler_type,\n",
    "        \"data_folders\": {\n",
    "            \"train_X\": train_X_loc,\n",
    "            \"test_X\": test_X_loc,\n",
    "            \"train_Y\": train_Y_loc,\n",
    "            \"test_Y\": test_Y_loc\n",
    "        },\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_epochs\": epochs,\n",
    "        \"start_epoch\": cyclegan.start_epoch,\n",
    "        \"lambda_weight\": lambda_weight,\n",
    "        \"lambda_idt_X\": lambda_idt_X,\n",
    "        \"lambda_idt_Y\": lambda_idt_Y,\n",
    "        \"checkpoint\": {\n",
    "            \"instance\": checkpoint_instance_dir,\n",
    "            \"epoch\": checkpoint_epoch_dir\n",
    "        }\n",
    "    }, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d092986-28a4-48d2-8f29-a8eacb55eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could also enable random flipping\n",
    "def generate_noisy_labels(shape, real, device):\n",
    "    # Randomly generated between 0 and 1\n",
    "    labels = torch.rand(shape, device=device)\n",
    "    \n",
    "    if real:\n",
    "        # Now they are between 0.7 and 1.1\n",
    "        labels = (2 * labels / 5) + 0.7\n",
    "    else:\n",
    "        # Now they are between 0 and 0.3\n",
    "        labels = (labels * 3) / 10\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8713a-f86c-4c3a-894b-c12a4a5e97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_loss(real, fake, pool, discriminator, loss_func):\n",
    "    # Discriminator should give (1) for a real image and (0) for a fake\n",
    "    real_pred = discriminator(real)\n",
    "    real_loss = loss_func(real_pred, generate_noisy_labels(real_pred.shape, True, device)) # Should dampen?\n",
    "    \n",
    "    # We draw from the history buffer\n",
    "    pool_fake = pool.randomise_existing_batch(fake)\n",
    "    fake_pred = discriminator(pool_fake)\n",
    "    # Fake images should not fool the discriminator\n",
    "    fake_loss = loss_func(fake_pred.detach(), generate_noisy_labels(fake_pred.shape, False, device))\n",
    "    \n",
    "    avg_loss = (real_loss + fake_loss) * 0.5\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a462990-62f3-4551-9681-8866c0d5f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cyclegan.start_epoch != 0:\n",
    "    print(f\"Resuming training from epoch {cyclegan.start_epoch + 1}\")\n",
    "else:\n",
    "    print(\"Starting training from scratch\")\n",
    "\n",
    "if vis is not None:\n",
    "    loss_plot = MultiLinePlot(vis, \"Losses\", \"Epoch\", \"Loss\")\n",
    "\n",
    "for epoch in range(cyclegan.start_epoch + 1, epochs + 1):\n",
    "    cyclegan.G.train()\n",
    "    cyclegan.F.train()\n",
    "    \n",
    "    cyclegan.D_X.train()\n",
    "    cyclegan.D_Y.train()\n",
    "    \n",
    "    cyclegan.P_X.train()\n",
    "    cyclegan.P_Y.train()\n",
    "\n",
    "    batch_start_time = time.time()\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    cum_loss_idt_x = 0\n",
    "    cum_loss_idt_y = 0\n",
    "    cum_G_fool_loss = 0\n",
    "    cum_F_fool_loss = 0\n",
    "    cum_predictor_x_loss = 0\n",
    "    cum_predictor_y_loss = 0\n",
    "    cum_cycled_x_loss = 0\n",
    "    cum_cycled_y_loss = 0\n",
    "    cum_D_X_loss = 0\n",
    "    cum_D_Y_loss = 0\n",
    "    \n",
    "    ep_loss_idt_x = 0\n",
    "    ep_loss_idt_y = 0\n",
    "    ep_G_fool_loss = 0\n",
    "    ep_F_fool_loss = 0\n",
    "    ep_predictor_x_loss = 0\n",
    "    ep_predictor_y_loss = 0\n",
    "    ep_cycled_x_loss = 0\n",
    "    ep_cycled_y_loss = 0\n",
    "    ep_D_X_loss = 0\n",
    "    ep_D_Y_loss = 0\n",
    "    \n",
    "    for batch_no, ((x_0, x_1, x_2), (y_0, y_1, y_2)) in enumerate(dataloader):\n",
    "        # Load the sequence of frames to the GPU\n",
    "        x_0 = x_0.to(device)\n",
    "        x_1 = x_1.to(device)\n",
    "        x_2 = x_2.to(device)\n",
    "        \n",
    "        y_0 = y_0.to(device)\n",
    "        y_1 = y_1.to(device)\n",
    "        y_2 = y_2.to(device)\n",
    "        \n",
    "        # Firstly, generate fake images in domain Y using the x_t images\n",
    "        fake_y_0 = cyclegan.G(x_0)\n",
    "        fake_y_1 = cyclegan.G(x_1)\n",
    "        \n",
    "        # For the final frame we use the predictor model, we are doing this in domain Y space so use P_Y not P_X\n",
    "        fake_y_2 = cyclegan.P_Y(torch.cat((fake_y_0, fake_y_1), 1))\n",
    "        \n",
    "        # Then generate the fake images in domain X using the y_t images\n",
    "        fake_x_0 = cyclegan.F(y_0)\n",
    "        fake_x_1 = cyclegan.F(y_1)\n",
    "        # For the final frame we use the predictor model, we are doing this in domain X space so use P_X not P_Y\n",
    "        fake_x_2 = cyclegan.P_X(torch.cat((fake_x_0, fake_x_1), 1))\n",
    "        \n",
    "        ### GENERATOR TRAINING\n",
    "        \n",
    "        # Freeze discriminator weights\n",
    "        for param in cyclegan.D_X.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in cyclegan.D_Y.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Zero the gradients for the generators and predictors\n",
    "        cyclegan.G_opt.zero_grad()\n",
    "        cyclegan.F_opt.zero_grad()\n",
    "        cyclegan.P_X_opt.zero_grad()\n",
    "        cyclegan.P_Y_opt.zero_grad()\n",
    "        \n",
    "        # Calculate the identity loss, this tries to enforce that G(y) = I(y) = y i.e. the identity\n",
    "        idt_x_0 = cyclegan.F(x_0)\n",
    "        idt_x_1 = cyclegan.F(x_1)\n",
    "        loss_idt_x = cyclegan.identity_loss(idt_x_0, x_0) + cyclegan.identity_loss(idt_x_1, x_1)\n",
    "        \n",
    "        idt_y_0 = cyclegan.G(y_0)\n",
    "        idt_y_1 = cyclegan.G(y_1)\n",
    "        loss_idt_y = cyclegan.identity_loss(idt_y_0, y_0) + cyclegan.identity_loss(idt_y_1, y_1)\n",
    "        \n",
    "        # Now we try and fool the discriminators\n",
    "        # D_X tries to tell if an image is from X (1) or from F(Y) (0) so it takes input fake_x_t\n",
    "        # D_X is supposed to be 1 if the image is from X but here we are trying to get it to be 1 if it is from G(X) -> Y which is incorrect\n",
    "        G_fool_0 = cyclegan.D_Y(fake_y_0) \n",
    "        # We want to tell if fake_y_0 has fooled D_X, so we measure how far from the output 1 it is \n",
    "        G_fool_loss_0 = cyclegan.gan_loss(G_fool_0, generate_noisy_labels(G_fool_0.shape, True, device))\n",
    "        \n",
    "        G_fool_1 = cyclegan.D_Y(fake_y_1)\n",
    "        G_fool_loss_1 = cyclegan.gan_loss(G_fool_1, generate_noisy_labels(G_fool_1.shape, True, device))\n",
    "        \n",
    "        G_fool_2 = cyclegan.D_Y(fake_y_2)\n",
    "        G_fool_loss_2 = cyclegan.gan_loss(G_fool_2, generate_noisy_labels(G_fool_2.shape, True, device))\n",
    "        \n",
    "        G_fool_loss = G_fool_loss_0 + G_fool_loss_1 + G_fool_loss_2\n",
    "        \n",
    "        # D_Y tries to tell if an image is from Y (1) or from G(X) (0) so it takes input fake_y_t\n",
    "        F_fool_0 = cyclegan.D_X(fake_x_0)\n",
    "        # We want to tell if fake_x_0 has fooled D_Y, so we measure how far from the output 1 it is \n",
    "        F_fool_loss_0 = cyclegan.gan_loss(F_fool_0, generate_noisy_labels(F_fool_0.shape, True, device))\n",
    "        \n",
    "        F_fool_1 = cyclegan.D_X(fake_x_1)\n",
    "        F_fool_loss_1 = cyclegan.gan_loss(F_fool_1, generate_noisy_labels(F_fool_1.shape, True, device))\n",
    "        \n",
    "        F_fool_2 = cyclegan.D_X(fake_x_2)\n",
    "        F_fool_loss_2 = cyclegan.gan_loss(F_fool_2, generate_noisy_labels(F_fool_2.shape, True, device))\n",
    "        \n",
    "        F_fool_loss = F_fool_loss_0 + F_fool_loss_1 + F_fool_loss_2\n",
    "        \n",
    "        # Now we train the predictors on the real images using the recurrent loss\n",
    "        # x_2 first\n",
    "        predicted_x_2 = cyclegan.P_X(torch.cat((x_0, x_1), 1))\n",
    "        predicted_x_loss = cyclegan.recurrent_loss(predicted_x_2, x_2)\n",
    "        \n",
    "        # So recycled_x_2 = F(P_Y(G(x_0), G(x_1)))\n",
    "        recycled_x_2 = cyclegan.F(fake_y_2)\n",
    "        recycled_x_loss = cyclegan.recycle_loss(recycled_x_2, x_2)\n",
    "        \n",
    "        predictor_x_loss = predicted_x_loss + recycled_x_loss\n",
    "        \n",
    "        # now y_2\n",
    "        predicted_y_2 = cyclegan.P_Y(torch.cat((y_0, y_1), 1))\n",
    "        predicted_y_loss = cyclegan.recurrent_loss(predicted_y_2, y_2)\n",
    "        \n",
    "        # So recycled_y_2 = G(P_X(F(y_0), F(y_1)))\n",
    "        recycled_y_2 = cyclegan.G(fake_x_2)\n",
    "        recycled_y_loss = cyclegan.recycle_loss(recycled_y_2, y_2)\n",
    "        \n",
    "        predictor_y_loss = predicted_y_loss + recycled_y_loss\n",
    "        \n",
    "        # Now do the cycle loss\n",
    "        cycled_x_0 = cyclegan.F(fake_y_0)\n",
    "        cycled_loss_x_0 = cyclegan.cycle_loss(cycled_x_0, x_0)\n",
    "        \n",
    "        cycled_x_1 = cyclegan.F(fake_y_1)\n",
    "        cycled_loss_x_1 = cyclegan.cycle_loss(cycled_x_1, x_1)\n",
    "        \n",
    "        cycled_loss_x = cycled_loss_x_0 + cycled_loss_x_1\n",
    "        \n",
    "        cycled_y_0 = cyclegan.G(fake_x_0)\n",
    "        cycled_loss_y_0 = cyclegan.cycle_loss(cycled_y_0, y_0)\n",
    "        \n",
    "        cycled_y_1 = cyclegan.G(fake_x_1)\n",
    "        cycled_loss_y_1 = cyclegan.cycle_loss(cycled_y_1, y_1)\n",
    "        \n",
    "        cycled_loss_y = cycled_loss_y_0 + cycled_loss_y_1\n",
    "        \n",
    "        # Backpropagate and step the gradients\n",
    "        generator_loss = G_fool_loss + F_fool_loss + lambda_weight * (cycled_loss_x + cycled_loss_y + predictor_x_loss + predictor_y_loss + lambda_idt_X * loss_idt_x + lambda_idt_Y * loss_idt_y)\n",
    "        generator_loss.backward()\n",
    "        \n",
    "        cyclegan.G_opt.step()\n",
    "        cyclegan.F_opt.step()\n",
    "        cyclegan.P_X_opt.step()\n",
    "        cyclegan.P_Y_opt.step()\n",
    "        \n",
    "        del G_fool_0\n",
    "        del G_fool_1\n",
    "        del G_fool_2\n",
    "        del F_fool_0\n",
    "        del F_fool_1\n",
    "        del F_fool_2\n",
    "        \n",
    "        ### DISCRIMINATOR TRAINING\n",
    "        # Unfreeze discriminator weights\n",
    "        for param in cyclegan.D_X.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in cyclegan.D_Y.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Zero the gradients\n",
    "        cyclegan.D_X_opt.zero_grad()\n",
    "        cyclegan.D_Y_opt.zero_grad()\n",
    "        \n",
    "        D_X_loss_0 = get_discriminator_loss(x_0, fake_x_0, cyclegan.fake_X_buffer, cyclegan.D_X, cyclegan.gan_loss)\n",
    "        D_X_loss_1 = get_discriminator_loss(x_1, fake_x_1, cyclegan.fake_X_buffer, cyclegan.D_X, cyclegan.gan_loss)\n",
    "        D_X_loss_fake_pred = get_discriminator_loss(x_2, fake_x_2, cyclegan.fake_X_buffer, cyclegan.D_X, cyclegan.gan_loss)\n",
    "        D_X_loss_real_pred = get_discriminator_loss(x_2, predicted_x_2, cyclegan.fake_X_buffer, cyclegan.D_X, cyclegan.gan_loss)\n",
    "        D_X_loss = D_X_loss_0 + D_X_loss_1 + D_X_loss_fake_pred + D_X_loss_real_pred\n",
    "        \n",
    "        D_Y_loss_0 = get_discriminator_loss(y_0, fake_y_0, cyclegan.fake_Y_buffer, cyclegan.D_Y, cyclegan.gan_loss)\n",
    "        D_Y_loss_1 = get_discriminator_loss(y_1, fake_y_1, cyclegan.fake_Y_buffer, cyclegan.D_Y, cyclegan.gan_loss)\n",
    "        D_Y_loss_fake_pred = get_discriminator_loss(y_2, fake_y_2, cyclegan.fake_Y_buffer, cyclegan.D_Y, cyclegan.gan_loss)\n",
    "        D_Y_loss_real_pred = get_discriminator_loss(y_2, predicted_y_2, cyclegan.fake_Y_buffer, cyclegan.D_Y, cyclegan.gan_loss)\n",
    "        D_Y_loss = D_Y_loss_0 + D_Y_loss_1 + D_Y_loss_fake_pred + D_Y_loss_real_pred\n",
    "        \n",
    "        # Backpropagate and step the gradients\n",
    "        D_loss = D_X_loss + D_Y_loss\n",
    "        D_loss.backward()\n",
    "        \n",
    "        cyclegan.D_X_opt.step()\n",
    "        cyclegan.D_Y_opt.step()\n",
    "        \n",
    "        ### UPDATE POOLS\n",
    "        cyclegan.fake_X_buffer.add(fake_x_0.detach())\n",
    "        cyclegan.fake_X_buffer.add(fake_x_1.detach())\n",
    "        cyclegan.fake_X_buffer.add(fake_x_2.detach())\n",
    "        \n",
    "        cyclegan.fake_Y_buffer.add(fake_y_0.detach())\n",
    "        cyclegan.fake_Y_buffer.add(fake_y_1.detach())\n",
    "        cyclegan.fake_Y_buffer.add(fake_y_2.detach())\n",
    "        \n",
    "        ### TRACK LOSS\n",
    "        cum_loss_idt_x += loss_idt_x.item()\n",
    "        cum_loss_idt_y += loss_idt_y.item()\n",
    "        cum_G_fool_loss += G_fool_loss.item()\n",
    "        cum_F_fool_loss += F_fool_loss.item()\n",
    "        cum_predictor_x_loss += predictor_x_loss.item()\n",
    "        cum_predictor_y_loss += predictor_y_loss.item()\n",
    "        cum_cycled_x_loss += cycled_loss_x.item()\n",
    "        cum_cycled_y_loss += cycled_loss_y.item()\n",
    "        cum_D_X_loss += D_X_loss.item()\n",
    "        cum_D_Y_loss += D_Y_loss.item()\n",
    "        \n",
    "        ep_loss_idt_x += loss_idt_x.item()\n",
    "        ep_loss_idt_y += loss_idt_y.item()\n",
    "        ep_G_fool_loss += G_fool_loss.item()\n",
    "        ep_F_fool_loss += F_fool_loss.item()\n",
    "        ep_predictor_x_loss += predictor_x_loss.item()\n",
    "        ep_predictor_y_loss += predictor_y_loss.item()\n",
    "        ep_cycled_x_loss += cycled_loss_x.item()\n",
    "        ep_cycled_y_loss += cycled_loss_y.item()\n",
    "        ep_D_X_loss += D_X_loss.item()\n",
    "        ep_D_Y_loss += D_Y_loss.item()\n",
    "        \n",
    "        if epoch == 0 and batch_no < 55:\n",
    "            print(f\"[{epoch}:{batch_no}] fake_X_buffer: {len(cyclegan.fake_X_buffer)}, fake_Y_buffer: {len(cyclegan.fake_Y_buffer)}\")\n",
    "        \n",
    "        if batch_no % 100 == 0 and batch_no != 0: \n",
    "            duration = time.time() - batch_start_time\n",
    "            \n",
    "            updated_losses = {\n",
    "                \"loss_idt_x\": cum_loss_idt_x / 100,\n",
    "                \"loss_idt_y\": cum_loss_idt_y / 100,\n",
    "                \"G_fool_loss\": cum_G_fool_loss / 100,\n",
    "                \"F_fool_loss\": cum_F_fool_loss / 100,\n",
    "                \"predictor_x_loss\": cum_predictor_x_loss / 100,\n",
    "                \"predictor_y_loss\": cum_predictor_y_loss / 100,\n",
    "                \"cycled_x_loss\": cum_cycled_x_loss / 100,\n",
    "                \"cycled_y_loss\": cum_cycled_y_loss / 100,\n",
    "                \"D_X_loss\": cum_D_X_loss / 100,\n",
    "                \"D_Y_loss\": cum_D_Y_loss / 100\n",
    "            }\n",
    "            \n",
    "            x_loss_str = f\"[{epoch}:{batch_no}]\"\n",
    "            y_loss_str = f\"[{epoch}:{batch_no}]\"\n",
    "            \n",
    "            for i, (key, value) in enumerate(updated_losses.items()):\n",
    "                if i % 2 == 0:\n",
    "                    x_loss_str = f\"{x_loss_str} {key}: {value},\"\n",
    "                else:\n",
    "                    y_loss_str = f\"{y_loss_str} {key}: {value},\"\n",
    "            \n",
    "            x_loss_str = x_loss_str[:-1]\n",
    "            y_loss_str = y_loss_str[:-1]\n",
    "\n",
    "            print(f\"[{epoch}:{batch_no}] Took {duration:.2f}s\")\n",
    "            print(x_loss_str)\n",
    "            print(y_loss_str)\n",
    "            print(f\"[{epoch}:{batch_no}] fake_X_buffer: {len(cyclegan.fake_X_buffer)}, fake_Y_buffer: {len(cyclegan.fake_Y_buffer)}\")\n",
    "            \n",
    "            if vis is not None:\n",
    "                loss_plot.append_values(epoch + batch_no / len(dataloader), updated_losses)\n",
    "            \n",
    "            cum_loss_idt_x = 0\n",
    "            cum_loss_idt_y = 0\n",
    "            cum_G_fool_loss = 0\n",
    "            cum_F_fool_loss = 0\n",
    "            cum_predictor_x_loss = 0\n",
    "            cum_predictor_y_loss = 0\n",
    "            cum_cycled_x_loss = 0\n",
    "            cum_cycled_y_loss = 0\n",
    "            cum_D_X_loss = 0\n",
    "            cum_D_Y_loss = 0\n",
    "\n",
    "            batch_start_time = time.time()\n",
    "    \n",
    "    print(f\"[{epoch}:END] Completed epoch in {time.time() - epoch_start_time}s\")\n",
    "    \n",
    "    print(f\"[{epoch}:{batch_no}]\", \n",
    "          f\"ep_loss_idt_x: {ep_loss_idt_x / len(dataloader):.3f}\", \n",
    "          f\"ep_G_fool_loss: {ep_G_fool_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_predictor_x_loss: {ep_predictor_x_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_cycled_x_loss: {ep_cycled_x_loss / len(dataloader):.3f}\",\n",
    "          f\"ep_D_X_loss: {ep_D_X_loss / len(dataloader):.3f}\")\n",
    "    \n",
    "    print(f\"[{epoch}:{batch_no}]\", \n",
    "          f\"ep_loss_idt_y: {ep_loss_idt_y / len(dataloader):.3f}\", \n",
    "          f\"ep_F_fool_loss: {ep_F_fool_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_predictor_y_loss: {ep_predictor_y_loss / len(dataloader):.3f}\", \n",
    "          f\"ep_cycled_y_loss: {ep_cycled_y_loss / len(dataloader):.3f}\",\n",
    "          f\"ep_D_Y_loss: {ep_D_Y_loss / len(dataloader):.3f}\")\n",
    "    \n",
    "    cyclegan.G.eval()\n",
    "    cyclegan.F.eval()\n",
    "    \n",
    "    if vis is not None:\n",
    "        eval_start_time = time.time()\n",
    "\n",
    "        G_eval_forward = cyclegan.apply(test_xs, x_to_y=True)\n",
    "        F_eval_forward = cyclegan.apply(test_ys, x_to_y=False)\n",
    "\n",
    "        G_rev = cyclegan.apply(G_eval_forward, x_to_y=False)\n",
    "        F_rev = cyclegan.apply(F_eval_forward, x_to_y=True)\n",
    "\n",
    "        G_eval_forward = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in G_eval_forward])\n",
    "        F_eval_forward = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in F_eval_forward])\n",
    "        G_rev = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in G_rev])\n",
    "        F_rev = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in F_rev])\n",
    "\n",
    "        G_eval_grid = torchvision.utils.make_grid(G_eval_forward, nrow=4)\n",
    "        F_eval_grid = torchvision.utils.make_grid(F_eval_forward, nrow=4)\n",
    "        G_rev_grid = torchvision.utils.make_grid(G_rev, nrow=4)\n",
    "        F_rev_grid = torchvision.utils.make_grid(F_rev, nrow=4)\n",
    "        \n",
    "        folder = f\"{cyclegan.save_folder}/{epoch}\"\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        \n",
    "        torchvision.utils.save_image(G_eval_grid, f\"{folder}/X_to_Y.png\")\n",
    "        torchvision.utils.save_image(F_eval_grid, f\"{folder}/Y_to_X.png\")\n",
    "        torchvision.utils.save_image(G_rev_grid, f\"{folder}/X_to_Y_to_X.png\")\n",
    "        torchvision.utils.save_image(F_rev_grid, f\"{folder}/Y_to_X_to_Y.png\")\n",
    "\n",
    "        vis.image(G_eval_grid, win=\"G_eval\", opts={\n",
    "            \"caption\": f\"X -> Y evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(F_eval_grid, win=\"F_eval\", opts={\n",
    "            \"caption\": f\"Y -> X evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(G_rev_grid, win=\"G_rev\", opts={\n",
    "            \"caption\": f\"X -> Y -> X evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "\n",
    "        vis.image(F_rev_grid, win=\"F_rev\", opts={\n",
    "            \"caption\": f\"Y -> X -> Y evaluation, epoch {epoch}\",\n",
    "            \"store_history\": True\n",
    "        })\n",
    "        \n",
    "        print(f\"[{epoch}:END] Completed eval in {time.time() - eval_start_time}s\")\n",
    "\n",
    "    cyclegan.G.train()\n",
    "    cyclegan.F.train()\n",
    "\n",
    "    cyclegan.step_learning_rates()\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"[{epoch}:END] Saving models and training information permanently\")\n",
    "        cyclegan.save(epoch, full_save=True)\n",
    "        cyclegan.save(epoch, full_save=True, folder=\"latest\")\n",
    "    else:\n",
    "        print(f\"[{epoch}:END] Saving models and training information temporarily to latest and saving generators permanently\")\n",
    "        cyclegan.save(epoch, full_save=True, folder=\"latest\")\n",
    "        cyclegan.save(epoch, full_save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d56c9-97ab-4aa0-bafa-b8bcd55b101b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Video Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a35e8cbb-55eb-48b5-9ff8-e81e531a622e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_time = \"1680898964.0020654\"\n",
    "model_parent_directory = f\"./runs/RecycleGAN/{model_time}\"\n",
    "epoch_directory = \"latest\"\n",
    "model_name = \"G.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dd980fe3-49a9-4dec-b0b2-9e3bfd6bb698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{model_parent_directory}/info_None.json\", \"r\") as fp:\n",
    "    model_info = json.load(fp)\n",
    "\n",
    "upsample_strategy = model_info[\"upsample_strategy\"]\n",
    "block_count = model_info[\"block_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "13896de5-0cac-4b68-8c59-2171870b0c41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Generator(block_count, upsample_strategy).to(device)\n",
    "model.load_state_dict(torch.load(f\"{model_parent_directory}/{epoch_directory}/{model_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "21ac2f86-ef4a-4b1d-80b1-1ef44178f954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_video_path = \"./original_data/Test/Test Movie.mp4\"\n",
    "output_video_path = f\"./video_transfer/recyclegan_{model_time}.mp4\"\n",
    "batch_size = 4\n",
    "\n",
    "os.makedirs(\"./video_transfer\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1f19d2b0-07b0-4ffd-b297-2273d9527fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89d2adad7504ec38d85357f290fe604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_video(input_video_path, output_video_path, model, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
