{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BUd-Sn6W5DpW",
        "JyctLauF6Uvs",
        "Cj0VbbNc6cBJ",
        "a6YY_RtbnVup"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-km0NX3Q4PIf"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "\n",
        "from torchvision.transforms import transforms\n",
        "import torch.utils.data\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "from pytorchCycleGANandpix2pix.models import networks as offnets\n",
        "from pytorchCycleGANandpix2pix.util.image_pool import ImagePool\n",
        "from pytorchCycleGANandpix2pix.util.util import tensor2im\n",
        "\n",
        "from joint_dataset import JointDomainImageDataset\n",
        "\n",
        "from dotmap import DotMap\n",
        "\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Configuration (Local vs. Hosted)"
      ],
      "metadata": {
        "id": "BUd-Sn6W5DpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using_google_gpus = False\n",
        "\n",
        "train_X_loc = \"./selected_data/experiment_random_1679337142.6036053/X/train\" \n",
        "test_X_loc = \"./selected_data/experiment_random_1679337142.6036053/X/test\"\n",
        "\n",
        "train_Y_loc = \"./selected_data/experiment_random_1679337142.6036053/Y/train\"\n",
        "test_Y_loc = \"./selected_data/experiment_random_1679337142.6036053/Y/test\"\n",
        "\n",
        "# train_Y_loc = \"./selected_data/experiment_random_1679337142.6036053/X/train\" # \"./datasets/horse2zebra/trainA\" if using_google_gpus else \"./cyclegan_original_data/horse2zebra/trainA\" #  # \n",
        "# test_Y_loc = \"./selected_data/experiment_stratified_selection_1679231577.167669/X/test\" # \"./datasets/horse2zebra/testA\" if using_google_gpus else \"./cyclegan_original_data/horse2zebra/testA\" # \"./selected_data/experiment_stratified_selection_1678204517.945494/X/test\" # \n",
        "\n",
        "# train_X_loc = \"./selected_data/experiment_stratified_selection_1679231577.167669/Y/train\" # \"./datasets/horse2zebra/trainB\" if using_google_gpus else \"./cyclegan_original_data/horse2zebra/trainB\" # \"./selected_data/experiment_stratified_selection_1678204517.945494/Y/train\" # \n",
        "# test_X_loc = \"./selected_data/experiment_stratified_selection_1679231577.167669/Y/test\" # \"./datasets/horse2zebra/testB\" if using_google_gpus else \"./cyclegan_original_data/horse2zebra/testB\" # \"./selected_data/experiment_stratified_selection_1678204517.945494/Y/test\" # \n",
        "\n",
        "run_data_directory = \"./runs\""
      ],
      "metadata": {
        "id": "q9gvhH8N4T4J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if using_google_gpus:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    vis = None\n",
        "else:\n",
        "    import visdom\n",
        "    vis = visdom.Visdom()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLu2XCHZ4mzV",
        "outputId": "f58865ea-7045-4bc6-9a65-cc11b95a137d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting up a new session...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/cyclegan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AfRxszI4sd0",
        "outputId": "b49f64c4-713f-40dd-dd00-5ae5806a6679"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WinError 3] The system cannot find the path specified: 'drive/MyDrive/cyclegan'\n",
            "F:\\Documents\\Development\\GitHub\\advanced-computer-vision-y4\\code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator Architecture"
      ],
      "metadata": {
        "id": "JyctLauF6Uvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CINR = Convolution Instance Norm ReLU\n",
        "# 3x3 convolutions with stride 1/2, 1 or 2 depending on position\n",
        "# Uses reflection padding\n",
        "# In the paper:\n",
        "# dk denotes a k filter stride 2 with 3x3 conv\n",
        "# c7s1-k denotes a k filter stride 1 with 7x7 conv\n",
        "# uk denotes a k filter stride 1/2 with 3x3 conv\n",
        "# Instead of using a nn.Module, this has much less overhead\n",
        "def createGeneratorCINRLayer(in_ch, out_ch, stride, kernel_size, reflect_pad):\n",
        "    layers = []\n",
        "        \n",
        "    padding = 1\n",
        "    \n",
        "    if reflect_pad:\n",
        "        layers.append(nn.ReflectionPad2d(kernel_size // 2))\n",
        "        padding = 0\n",
        "\n",
        "    if stride < 1:\n",
        "        layers.append(\n",
        "            nn.ConvTranspose2d(in_ch, out_ch, kernel_size=kernel_size, stride=int(1 / stride), padding=padding, output_padding=padding, bias=True)\n",
        "        )\n",
        "        # layers += [\n",
        "        #     nn.Upsample(scale_factor=2, mode=\"bilinear\"), ## nn instead of bi maybe try pixelshuffle\n",
        "        #     nn.ReflectionPad2d(1),\n",
        "        #     nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=1, padding=0)\n",
        "        # ]\n",
        "    else:\n",
        "        layers.append(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
        "        )\n",
        "    \n",
        "    layers += [\n",
        "        nn.InstanceNorm2d(out_ch),\n",
        "        nn.ReLU(True)\n",
        "    ]\n",
        "\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "kns8C2ps6Wsw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Contains 2 3x3 convolutional layers with the same number of filters on both layers\n",
        "# Use reflect padding in these\n",
        "# Don't use dropout\n",
        "# Use instancenorm\n",
        "class GeneratorResidualBlock(nn.Module):\n",
        "    def __init__(self, feature_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        layers = [\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(feature_size, feature_size, kernel_size=3, bias=True),\n",
        "            nn.InstanceNorm2d(feature_size),\n",
        "            nn.ReLU(True),\n",
        "            # Dropout would go here if I want it\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(feature_size, feature_size, kernel_size=3, bias=True),\n",
        "            nn.InstanceNorm2d(feature_size)\n",
        "        ]\n",
        "        \n",
        "        self.seq = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, batch):\n",
        "        return batch + self.seq(batch)"
      ],
      "metadata": {
        "id": "zSag7JDP6ZBC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the 128x128 case:\n",
        "# c7s1-64, d128, d256, R256 x 6, u128, u64, c7s1-3\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, resnet_blocks):\n",
        "        super().__init__()\n",
        "        \n",
        "        layers = [\n",
        "            createGeneratorCINRLayer(in_ch=3, out_ch=64, stride=1, kernel_size=7, reflect_pad=True),\n",
        "            createGeneratorCINRLayer(in_ch=64, out_ch=128, stride=2, kernel_size=3, reflect_pad=False),\n",
        "            createGeneratorCINRLayer(in_ch=128, out_ch=256, stride=2, kernel_size=3, reflect_pad=False)\n",
        "        ]\n",
        "        \n",
        "        # same dim all the way through\n",
        "        for _ in range(resnet_blocks):\n",
        "            layers.append(GeneratorResidualBlock(feature_size=256))\n",
        "\n",
        "        layers += [ \n",
        "            createGeneratorCINRLayer(in_ch=256, out_ch=128, kernel_size=3, stride=0.5, reflect_pad=False),\n",
        "            createGeneratorCINRLayer(in_ch=128, out_ch=64, kernel_size=3, stride=0.5, reflect_pad=False),\n",
        "\n",
        "            # Last one is a bit different without the ReLU and instance norm\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_channels=64, out_channels=3, stride=1, kernel_size=7, bias=True),\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "        \n",
        "        self.seq = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        return self.seq(batch)"
      ],
      "metadata": {
        "id": "h3Vj64aD6Z8W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator Architecture"
      ],
      "metadata": {
        "id": "Cj0VbbNc6cBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CINR = Convolution Instance Normalisation Layer\n",
        "# Denoted Ck where k = #filters\n",
        "def createDiscriminatorCINRLayer(in_ch, out_ch, stride, apply_norm):\n",
        "    layers = [\n",
        "        nn.Conv2d(in_ch, out_ch, kernel_size=4, stride=stride, padding=1),\n",
        "    ]\n",
        "\n",
        "    if apply_norm:\n",
        "        layers.append(\n",
        "            nn.InstanceNorm2d(out_ch)\n",
        "        )\n",
        "    \n",
        "    layers.append(\n",
        "        nn.LeakyReLU(0.2, True)\n",
        "    )\n",
        "\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "ZMrmutIx6d2x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [\n",
        "            createDiscriminatorCINRLayer(in_ch=3, out_ch=64, stride=2, apply_norm=False),\n",
        "            createDiscriminatorCINRLayer(in_ch=64, out_ch=128, stride=2, apply_norm=True),\n",
        "            createDiscriminatorCINRLayer(in_ch=128, out_ch=256, stride=2, apply_norm=True),\n",
        "            createDiscriminatorCINRLayer(in_ch=256, out_ch=512, stride=1, apply_norm=True),\n",
        "            # In the source of CycleGAN they use k=4, s=1, p=1 despite not saying this in the paper\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "        ]\n",
        "\n",
        "        self.seq = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        return self.seq(batch)"
      ],
      "metadata": {
        "id": "GfTdrjNg6fE7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Architecture"
      ],
      "metadata": {
        "id": "a6YY_RtbnVup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_generator_arch = Generator().to(device)\n",
        "summary(my_generator_arch, (3, 256, 256))"
      ],
      "metadata": {
        "id": "P__W1G8VnLft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb4a61cd-ee07-4ab3-a44e-8c1a54cc78db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   ReflectionPad2d-1          [-1, 3, 262, 262]               0\n",
            "            Conv2d-2         [-1, 64, 256, 256]           9,472\n",
            "    InstanceNorm2d-3         [-1, 64, 256, 256]               0\n",
            "              ReLU-4         [-1, 64, 256, 256]               0\n",
            "            Conv2d-5        [-1, 128, 128, 128]          73,856\n",
            "    InstanceNorm2d-6        [-1, 128, 128, 128]               0\n",
            "              ReLU-7        [-1, 128, 128, 128]               0\n",
            "            Conv2d-8          [-1, 256, 64, 64]         295,168\n",
            "    InstanceNorm2d-9          [-1, 256, 64, 64]               0\n",
            "             ReLU-10          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-11          [-1, 256, 66, 66]               0\n",
            "           Conv2d-12          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-13          [-1, 256, 64, 64]               0\n",
            "             ReLU-14          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-15          [-1, 256, 66, 66]               0\n",
            "           Conv2d-16          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-17          [-1, 256, 64, 64]               0\n",
            "GeneratorResidualBlock-18          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-19          [-1, 256, 66, 66]               0\n",
            "           Conv2d-20          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-21          [-1, 256, 64, 64]               0\n",
            "             ReLU-22          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-23          [-1, 256, 66, 66]               0\n",
            "           Conv2d-24          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-25          [-1, 256, 64, 64]               0\n",
            "GeneratorResidualBlock-26          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-27          [-1, 256, 66, 66]               0\n",
            "           Conv2d-28          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-29          [-1, 256, 64, 64]               0\n",
            "             ReLU-30          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-31          [-1, 256, 66, 66]               0\n",
            "           Conv2d-32          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-33          [-1, 256, 64, 64]               0\n",
            "GeneratorResidualBlock-34          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-35          [-1, 256, 66, 66]               0\n",
            "           Conv2d-36          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-37          [-1, 256, 64, 64]               0\n",
            "             ReLU-38          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-39          [-1, 256, 66, 66]               0\n",
            "           Conv2d-40          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-41          [-1, 256, 64, 64]               0\n",
            "GeneratorResidualBlock-42          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-43          [-1, 256, 66, 66]               0\n",
            "           Conv2d-44          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-45          [-1, 256, 64, 64]               0\n",
            "             ReLU-46          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-47          [-1, 256, 66, 66]               0\n",
            "           Conv2d-48          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-49          [-1, 256, 64, 64]               0\n",
            "GeneratorResidualBlock-50          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-51          [-1, 256, 66, 66]               0\n",
            "           Conv2d-52          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-53          [-1, 256, 64, 64]               0\n",
            "             ReLU-54          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-55          [-1, 256, 66, 66]               0\n",
            "           Conv2d-56          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-57          [-1, 256, 64, 64]               0\n",
            "GeneratorResidualBlock-58          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-59          [-1, 256, 66, 66]               0\n",
            "           Conv2d-60          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-61          [-1, 256, 64, 64]               0\n",
            "             ReLU-62          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-63          [-1, 256, 66, 66]               0\n",
            "           Conv2d-64          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-65          [-1, 256, 64, 64]               0\n",
            "GeneratorResidualBlock-66          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-67          [-1, 256, 66, 66]               0\n",
            "           Conv2d-68          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-69          [-1, 256, 64, 64]               0\n",
            "             ReLU-70          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-71          [-1, 256, 66, 66]               0\n",
            "           Conv2d-72          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-73          [-1, 256, 64, 64]               0\n",
            "GeneratorResidualBlock-74          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-75          [-1, 256, 66, 66]               0\n",
            "           Conv2d-76          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-77          [-1, 256, 64, 64]               0\n",
            "             ReLU-78          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-79          [-1, 256, 66, 66]               0\n",
            "           Conv2d-80          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-81          [-1, 256, 64, 64]               0\n",
            "GeneratorResidualBlock-82          [-1, 256, 64, 64]               0\n",
            "  ConvTranspose2d-83        [-1, 128, 128, 128]         295,040\n",
            "   InstanceNorm2d-84        [-1, 128, 128, 128]               0\n",
            "             ReLU-85        [-1, 128, 128, 128]               0\n",
            "  ConvTranspose2d-86         [-1, 64, 256, 256]          73,792\n",
            "   InstanceNorm2d-87         [-1, 64, 256, 256]               0\n",
            "             ReLU-88         [-1, 64, 256, 256]               0\n",
            "  ReflectionPad2d-89         [-1, 64, 262, 262]               0\n",
            "           Conv2d-90          [-1, 3, 256, 256]           9,411\n",
            "             Tanh-91          [-1, 3, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 11,378,179\n",
            "Trainable params: 11,378,179\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 935.23\n",
            "Params size (MB): 43.40\n",
            "Estimated Total Size (MB): 979.38\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "their_generator_arch = offnets.define_G(\n",
        "    input_nc=3,\n",
        "    output_nc=3,\n",
        "    ngf=64,\n",
        "    netG=\"resnet_9blocks\",\n",
        "    norm=\"instance\"\n",
        ").to(device)\n",
        "summary(their_generator_arch, (3, 256, 256))"
      ],
      "metadata": {
        "id": "rOzu0PORnlAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2fd2ff3-0da4-465e-b6ca-14368c2f0c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initialize network with normal\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   ReflectionPad2d-1          [-1, 3, 262, 262]               0\n",
            "            Conv2d-2         [-1, 64, 256, 256]           9,472\n",
            "    InstanceNorm2d-3         [-1, 64, 256, 256]               0\n",
            "              ReLU-4         [-1, 64, 256, 256]               0\n",
            "            Conv2d-5        [-1, 128, 128, 128]          73,856\n",
            "    InstanceNorm2d-6        [-1, 128, 128, 128]               0\n",
            "              ReLU-7        [-1, 128, 128, 128]               0\n",
            "            Conv2d-8          [-1, 256, 64, 64]         295,168\n",
            "    InstanceNorm2d-9          [-1, 256, 64, 64]               0\n",
            "             ReLU-10          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-11          [-1, 256, 66, 66]               0\n",
            "           Conv2d-12          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-13          [-1, 256, 64, 64]               0\n",
            "             ReLU-14          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-15          [-1, 256, 66, 66]               0\n",
            "           Conv2d-16          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-17          [-1, 256, 64, 64]               0\n",
            "      ResnetBlock-18          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-19          [-1, 256, 66, 66]               0\n",
            "           Conv2d-20          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-21          [-1, 256, 64, 64]               0\n",
            "             ReLU-22          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-23          [-1, 256, 66, 66]               0\n",
            "           Conv2d-24          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-25          [-1, 256, 64, 64]               0\n",
            "      ResnetBlock-26          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-27          [-1, 256, 66, 66]               0\n",
            "           Conv2d-28          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-29          [-1, 256, 64, 64]               0\n",
            "             ReLU-30          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-31          [-1, 256, 66, 66]               0\n",
            "           Conv2d-32          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-33          [-1, 256, 64, 64]               0\n",
            "      ResnetBlock-34          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-35          [-1, 256, 66, 66]               0\n",
            "           Conv2d-36          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-37          [-1, 256, 64, 64]               0\n",
            "             ReLU-38          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-39          [-1, 256, 66, 66]               0\n",
            "           Conv2d-40          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-41          [-1, 256, 64, 64]               0\n",
            "      ResnetBlock-42          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-43          [-1, 256, 66, 66]               0\n",
            "           Conv2d-44          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-45          [-1, 256, 64, 64]               0\n",
            "             ReLU-46          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-47          [-1, 256, 66, 66]               0\n",
            "           Conv2d-48          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-49          [-1, 256, 64, 64]               0\n",
            "      ResnetBlock-50          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-51          [-1, 256, 66, 66]               0\n",
            "           Conv2d-52          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-53          [-1, 256, 64, 64]               0\n",
            "             ReLU-54          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-55          [-1, 256, 66, 66]               0\n",
            "           Conv2d-56          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-57          [-1, 256, 64, 64]               0\n",
            "      ResnetBlock-58          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-59          [-1, 256, 66, 66]               0\n",
            "           Conv2d-60          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-61          [-1, 256, 64, 64]               0\n",
            "             ReLU-62          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-63          [-1, 256, 66, 66]               0\n",
            "           Conv2d-64          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-65          [-1, 256, 64, 64]               0\n",
            "      ResnetBlock-66          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-67          [-1, 256, 66, 66]               0\n",
            "           Conv2d-68          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-69          [-1, 256, 64, 64]               0\n",
            "             ReLU-70          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-71          [-1, 256, 66, 66]               0\n",
            "           Conv2d-72          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-73          [-1, 256, 64, 64]               0\n",
            "      ResnetBlock-74          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-75          [-1, 256, 66, 66]               0\n",
            "           Conv2d-76          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-77          [-1, 256, 64, 64]               0\n",
            "             ReLU-78          [-1, 256, 64, 64]               0\n",
            "  ReflectionPad2d-79          [-1, 256, 66, 66]               0\n",
            "           Conv2d-80          [-1, 256, 64, 64]         590,080\n",
            "   InstanceNorm2d-81          [-1, 256, 64, 64]               0\n",
            "      ResnetBlock-82          [-1, 256, 64, 64]               0\n",
            "  ConvTranspose2d-83        [-1, 128, 128, 128]         295,040\n",
            "   InstanceNorm2d-84        [-1, 128, 128, 128]               0\n",
            "             ReLU-85        [-1, 128, 128, 128]               0\n",
            "  ConvTranspose2d-86         [-1, 64, 256, 256]          73,792\n",
            "   InstanceNorm2d-87         [-1, 64, 256, 256]               0\n",
            "             ReLU-88         [-1, 64, 256, 256]               0\n",
            "  ReflectionPad2d-89         [-1, 64, 262, 262]               0\n",
            "           Conv2d-90          [-1, 3, 256, 256]           9,411\n",
            "             Tanh-91          [-1, 3, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 11,378,179\n",
            "Trainable params: 11,378,179\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 935.23\n",
            "Params size (MB): 43.40\n",
            "Estimated Total Size (MB): 979.38\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_discriminator_arch = PatchDiscriminator().to(device)\n",
        "summary(my_discriminator_arch, (3, 256, 256))"
      ],
      "metadata": {
        "id": "l6GPGTgprW7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76823ce3-1370-42ab-ae72-0da4b91e464f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "            Conv2d-3          [-1, 128, 64, 64]         131,200\n",
            "    InstanceNorm2d-4          [-1, 128, 64, 64]               0\n",
            "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
            "            Conv2d-6          [-1, 256, 32, 32]         524,544\n",
            "    InstanceNorm2d-7          [-1, 256, 32, 32]               0\n",
            "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
            "            Conv2d-9          [-1, 512, 31, 31]       2,097,664\n",
            "   InstanceNorm2d-10          [-1, 512, 31, 31]               0\n",
            "        LeakyReLU-11          [-1, 512, 31, 31]               0\n",
            "           Conv2d-12            [-1, 1, 30, 30]           8,193\n",
            "================================================================\n",
            "Total params: 2,764,737\n",
            "Trainable params: 2,764,737\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 45.27\n",
            "Params size (MB): 10.55\n",
            "Estimated Total Size (MB): 56.57\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "their_discriminator_arch = offnets.define_D(\n",
        "    input_nc=3,\n",
        "    ndf=64,\n",
        "    netD=\"basic\",\n",
        "    norm=\"instance\"\n",
        ").to(device)\n",
        "summary(their_discriminator_arch, (3, 256, 256))"
      ],
      "metadata": {
        "id": "gA_xkksbraNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c227ba-27f5-4e49-adb7-634fd06666b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initialize network with normal\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
            "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
            "            Conv2d-3          [-1, 128, 64, 64]         131,200\n",
            "    InstanceNorm2d-4          [-1, 128, 64, 64]               0\n",
            "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
            "            Conv2d-6          [-1, 256, 32, 32]         524,544\n",
            "    InstanceNorm2d-7          [-1, 256, 32, 32]               0\n",
            "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
            "            Conv2d-9          [-1, 512, 31, 31]       2,097,664\n",
            "   InstanceNorm2d-10          [-1, 512, 31, 31]               0\n",
            "        LeakyReLU-11          [-1, 512, 31, 31]               0\n",
            "           Conv2d-12            [-1, 1, 30, 30]           8,193\n",
            "================================================================\n",
            "Total params: 2,764,737\n",
            "Trainable params: 2,764,737\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 45.27\n",
            "Params size (MB): 10.55\n",
            "Estimated Total Size (MB): 56.57\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CycleGAN"
      ],
      "metadata": {
        "id": "TnGUYCHQyAiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HistoryBuffer:\n",
        "    def __init__(self, max_size):\n",
        "        self.max_size = max_size\n",
        "        self.buffer = []\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def _make_space(self, max_del_size):\n",
        "        current_available_space = self.max_size - len(self)\n",
        "        del_size = max(0, max_del_size - current_available_space)\n",
        "\n",
        "        if del_size == 0:\n",
        "            return\n",
        "\n",
        "        del_indexes = random.sample(range(0, len(self)), del_size)\n",
        "\n",
        "        for del_idx in del_indexes:\n",
        "            del self.buffer[del_idx]\n",
        "\n",
        "    def add(self, batch):\n",
        "        self._make_space(len(batch))\n",
        "\n",
        "        for item in batch:\n",
        "            self.buffer.append(item.detach())\n",
        "\n",
        "    def sample_batch(self, batch_size):\n",
        "        return torch.stack(random.sample(self.buffer, batch_size))\n",
        "\n",
        "    def randomise_existing_batch(self, existing_batch):\n",
        "        if len(self) < existing_batch.shape[0] / 2:\n",
        "            return existing_batch\n",
        "        \n",
        "        new_batch = []\n",
        "\n",
        "        for item in existing_batch:\n",
        "            if random.uniform(0, 1) < 0.5:\n",
        "                new_batch.append(item.detach())\n",
        "            else:\n",
        "                new_batch.append(self.buffer[random.randint(0, len(self) - 1)])\n",
        "\n",
        "        return torch.stack(new_batch)"
      ],
      "metadata": {
        "id": "1548gLVZolP8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGAN:\n",
        "    def __init__(self, device, blocks, init=True, start_epoch=0, save_folder=None):\n",
        "        self.block_count = blocks\n",
        "\n",
        "        self.G = Generator(blocks).to(device)\n",
        "        self.F = Generator(blocks).to(device)\n",
        "        \n",
        "        self.D_X = PatchDiscriminator().to(device)\n",
        "        self.D_Y = PatchDiscriminator().to(device)\n",
        "\n",
        "        if init:\n",
        "            self._initialise_weights()\n",
        "\n",
        "        self.fake_X_buffer = HistoryBuffer(50)\n",
        "        self.fake_Y_buffer = HistoryBuffer(50)\n",
        "\n",
        "        self.gan_X_loss = nn.MSELoss().to(device)\n",
        "        self.gan_Y_loss = nn.MSELoss().to(device)\n",
        "\n",
        "        self.cycle_X_loss = nn.L1Loss().to(device)\n",
        "        self.cycle_Y_loss = nn.L1Loss().to(device)\n",
        "\n",
        "        self.identity_loss = nn.L1Loss().to(device)\n",
        "\n",
        "        self.G_opt = torch.optim.Adam(self.G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        self.F_opt = torch.optim.Adam(self.F.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "        self.D_X_opt = torch.optim.Adam(self.D_X.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        self.D_Y_opt = torch.optim.Adam(self.D_Y.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "        def lambda_rule(epoch):\n",
        "            lr_l = 1.0 - max(0, epoch - 99) / float(101)\n",
        "            return lr_l\n",
        "\n",
        "        self.G_opt_scheduler = torch.optim.lr_scheduler.LambdaLR(self.G_opt, lr_lambda=lambda_rule)\n",
        "        self.F_opt_scheduler = torch.optim.lr_scheduler.LambdaLR(self.F_opt, lr_lambda=lambda_rule)\n",
        "        self.D_X_opt_scheduler = torch.optim.lr_scheduler.LambdaLR(self.D_X_opt, lr_lambda=lambda_rule)\n",
        "        self.D_Y_opt_scheduler = torch.optim.lr_scheduler.LambdaLR(self.D_Y_opt, lr_lambda=lambda_rule)\n",
        "\n",
        "        self.save_folder = save_folder if save_folder is not None else f\"./runs/{time.time()}\"\n",
        "        os.makedirs(self.save_folder, exist_ok=True)\n",
        "        self.start_epoch = start_epoch\n",
        "\n",
        "    def _initialise_weights(self):\n",
        "        def applicator(m):\n",
        "            classname = m.__class__.__name__\n",
        "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "                torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "                if hasattr(m, 'bias') and m.bias is not None:\n",
        "                    torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "            \n",
        "            elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
        "                torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "                torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "        \n",
        "        self.G.apply(applicator)\n",
        "        self.F.apply(applicator)\n",
        "        self.D_X.apply(applicator)\n",
        "        self.D_Y.apply(applicator)\n",
        "\n",
        "    def _step_learning_rate(self, name, scheduler, optimiser):\n",
        "        previous_lr = optimiser.param_groups[0][\"lr\"]\n",
        "        scheduler.step()\n",
        "        new_lr = optimiser.param_groups[0][\"lr\"]\n",
        "        print(f\"Updated {name} learning rate from {previous_lr} to {new_lr}\")\n",
        "\n",
        "    def step_learning_rates(self):\n",
        "        self._step_learning_rate(\"G_opt\", self.G_opt_scheduler, self.G_opt)\n",
        "        self._step_learning_rate(\"F_opt\", self.F_opt_scheduler, self.F_opt)\n",
        "        self._step_learning_rate(\"D_X_opt\", self.D_X_opt_scheduler, self.D_X_opt)\n",
        "        self._step_learning_rate(\"D_Y_opt\", self.D_Y_opt_scheduler, self.D_Y_opt)\n",
        "\n",
        "    def apply(self, tensors, x_to_y):\n",
        "        model = self.G if x_to_y else self.F\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            processed_tensors = model(tensors.to(device).detach())\n",
        "        \n",
        "        model.train()\n",
        "        return processed_tensors.detach().cpu()\n",
        "\n",
        "    def save(self, epoch):\n",
        "        folder = f\"{self.save_folder}/{epoch}\"\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"G\": self.G.state_dict(),\n",
        "            \"F\": self.F.state_dict(),\n",
        "            \"D_X\": self.D_X.state_dict(),\n",
        "            \"D_Y\": self.D_Y.state_dict(),\n",
        "            \"G_opt\": self.G_opt.state_dict(),\n",
        "            \"F_opt\": self.F_opt.state_dict(),\n",
        "            \"D_X_opt\": self.D_X_opt.state_dict(),\n",
        "            \"D_Y_opt\": self.D_Y_opt.state_dict(),\n",
        "            \"fake_X_buffer\": self.fake_X_buffer.buffer,\n",
        "            \"fake_Y_buffer\": self.fake_Y_buffer.buffer\n",
        "        }, f\"{folder}/checkpoint.tar\")\n",
        "\n",
        "        torch.save(self.G.state_dict(), f\"{folder}/G.pth\")\n",
        "        torch.save(self.F.state_dict(), f\"{folder}/F.pth\")\n",
        "    \n",
        "    @staticmethod\n",
        "    def load(save_folder, epoch_dir, device, blocks):\n",
        "        checkpoint_path = f\"{save_folder}/{epoch_dir}/checkpoint.tar\"\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "        cyclegan = CycleGAN(device, blocks, init=False, start_epoch=checkpoint[\"epoch\"], save_folder=save_folder)\n",
        "\n",
        "        cyclegan.G.load_state_dict(checkpoint[\"G\"])\n",
        "        cyclegan.D_X.load_state_dict(checkpoint[\"D_X\"])\n",
        "        cyclegan.F.load_state_dict(checkpoint[\"F\"])\n",
        "        cyclegan.D_Y.load_state_dict(checkpoint[\"D_Y\"])\n",
        "\n",
        "        cyclegan.G_opt.load_state_dict(checkpoint[\"G_opt\"])\n",
        "        cyclegan.G_opt_scheduler.last_epoch = cyclegan.start_epoch\n",
        "\n",
        "        cyclegan.D_X_opt.load_state_dict(checkpoint[\"D_X_opt\"])\n",
        "        cyclegan.D_X_opt_scheduler.last_epoch = cyclegan.start_epoch\n",
        "\n",
        "        cyclegan.F_opt.load_state_dict(checkpoint[\"F_opt\"])\n",
        "        cyclegan.F_opt_scheduler.last_epoch = cyclegan.start_epoch\n",
        "\n",
        "        cyclegan.D_Y_opt.load_state_dict(checkpoint[\"D_Y_opt\"])\n",
        "        cyclegan.D_Y_opt_scheduler.last_epoch = cyclegan.start_epoch\n",
        "\n",
        "        cyclegan.fake_X_buffer.buffer = [x.to(device) for x in checkpoint[\"fake_X_buffer\"]]\n",
        "        cyclegan.fake_Y_buffer.buffer = [y.to(device) for y in checkpoint[\"fake_Y_buffer\"]]\n",
        "\n",
        "        cyclegan.G.train()\n",
        "        cyclegan.D_X.train()\n",
        "        cyclegan.F.train()\n",
        "        cyclegan.D_Y.train()\n",
        "\n",
        "        print(f\"Models and buffers loaded from {checkpoint_path}\")\n",
        "\n",
        "        return cyclegan"
      ],
      "metadata": {
        "id": "eIUNOtblyAJ3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "kprZWRluQ8zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = JointDomainImageDataset(train_X_loc, train_Y_loc, train=True)\n",
        "test_dataset = JointDomainImageDataset(test_X_loc, test_Y_loc, train=False)"
      ],
      "metadata": {
        "id": "nef4ZQruRQWo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def revert_normalisation(tensor):\n",
        "    return (tensor.permute(1, 2, 0) + 1) / 2"
      ],
      "metadata": {
        "id": "BraLmO8mRXBe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_xs = []\n",
        "test_ys = []\n",
        "\n",
        "for i in random.sample(range(0, len(test_dataset)), 16):\n",
        "    x, y = test_dataset[i]\n",
        "    test_xs.append(x)\n",
        "    test_ys.append(y)\n",
        "\n",
        "test_xs = torch.stack(test_xs)\n",
        "test_ys = torch.stack(test_ys)"
      ],
      "metadata": {
        "id": "5bCo5qbosJkA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if vis is not None:\n",
        "    vis.images(torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in test_xs]), nrow=4, opts={\"title\": \"X_test originals\"})\n",
        "    vis.images(torch.stack([revert_normalisation(y).permute(2, 0, 1) for y in test_ys]), nrow=4, opts={\"title\": \"Y_test originals\"})"
      ],
      "metadata": {
        "id": "mgZqZlyGslb5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img = test_dataset[random.randint(0, len(test_dataset))][0]"
      ],
      "metadata": {
        "id": "zDZxE2PqRV5Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img.permute(1, 2, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "LYq_e-frRkHy",
        "outputId": "159ff32a-f2ab-4840-f820-494ccc981faf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1d2afc9c040>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGhCAYAAADbf0s2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABkZklEQVR4nO2df7AeVX3/3+fsc+9NBHJjYrk3KYmmlhmgoiJIjDitrZmCOiqVanHSFpWRqkFFWoVUwfpVDNpWLYpQnRZ1KlKdEVqZFocGhDrGAEGsqPxwyEAUb9DS3MsPc3Pvs+f7x+7ZPXv27K/n2efHPs/7BZtnn92zZ/e5z3n2vZ/P+ZzPEUopBUIIIWQIkYO+AEIIISQLihQhhJChhSJFCCFkaKFIEUIIGVooUoQQQoYWihQhhJChhSJFCCFkaKFIEUIIGVooUoQQQoYWihQhhJChZWAideWVV+I5z3kOVqxYgc2bN+OOO+4Y1KUQQggZUgYiUv/6r/+KCy+8EB/60Idw99134wUveAFOP/10PPbYY4O4HEIIIUOKGESC2c2bN+PFL34xPvvZzwIAfN/Hhg0b8K53vQsXX3xx4fG+7+PRRx/FUUcdBSFEry+XEEJIzSil8MQTT2D9+vWQMtteavXxmgAAhw8fxt69e7Fjx45om5QSW7duxe7du53HLC4uYnFxMXr/85//HCeccELPr5UQQkhv2b9/P4455pjM/X0XqV/96ldot9uYmZlJbJ+ZmcF9993nPGbnzp348Ic/3I/LI4SQxiIQ3NSFXkTwqqwy2v8kBAxvVFBKKaDtB+8EACmDcq5z6RW936wvrlcAUobvBeAFFSqlcHDh1zjqqKNyP1Mjovt27NiB+fn5aNm/f/+gL4kQQoYOYSxSCnieDBcB6QHSE+Eiw8ULF2m8CnheoCWeZy2teAnqAzzPPI8XLVLKYIn2SXgtGZ1beIH8FHXZ9N2SetazngXP83DgwIHE9gMHDmB2dtZ5zNTUFKampvpxeYQQ0liEtSIS6yL1KoSwLKl4XQgYS2iPpfQk3mDWZQuPFk7lOK6IvltSk5OTOPnkk7Fr165om+/72LVrF7Zs2dLvyyGEkJFCu/ikAKQQkIEcJcukBEokFimFIVCBq05IYxEi2i4jV54+v3UufR5hn7McfbekAODCCy/EOeecg1NOOQWnnnoqPv3pT+Opp57CW97ylkFcDiGEjARCBOIkIAKB0oKgAD8qIxwiBWjrRggF31cQQoUipEVGBQsAKBEsqfOnRcgUKYhYCMtK1UBE6k/+5E/wy1/+Epdeeinm5ubwwhe+EDfddFMqmIIQQkh13IEOSXdfsny8TSkVHh8LWcr6sf2KVj0ua0kIEbn7hBCR3hUxkHFS3bKwsIDp6elBXwYhhAwVEwKYkLEV5YXjj3yl4Ie3elNI3KKioFQbSvlR8ENQCKElpUIrSiTDBuEWqcBVKCNLSgkJBQHfV/jf/1vA/Pw8Vq1alfmZBmJJEUII6QHa8oElQigOVUi7/iwrKlFJ6O7LqTQlhpbLr2zXFEWKEEJGDWEIiHbd5ZWNVoPxS67tgAoMJ0P4iqQvIXy2dUWRIoSQ8UIhMHD8MGzcl1nipGLrBggsMMTrSkhAKSirTFBExQEQZtS6Seo4x3pJKFKEEDIiKGiBCrUgFKkgMM8cDwVLMMyIu2BdCWOsFJIBfcIIa9fdU0lEbC0JEYgd4jB0uvsIIWRM0caNMt4DtgFjOOsS/VZCO/WCLaYACT2YV0TKFBlUTmtKR/QZVlQU5l7eoKJIEULIiKCgoPSYKKUglIoEK7KV9MBawMipB0TWj1KQoVQJnWUidPNFBpEyAtotSyqqLRE9aCYAFBQpQggZV/xQmIReF3G/lIKKMkUoIzQ8FQmoFKBUMDA4DDsXRjCG7e4Tlkjp6MKor8sM5IhKlRv9RJEihJARxBwCazjwovemxRMRWlKxG9Bw8VnRgqbrLu6rQqKuyGITSJlOtKQIIYRE2NN1ALZLLmkSRbpiuPPiIVPCEKsMwbGjBx3pkspAkSKEkDEhO2LcEhSElpgQYRpylSwblsukQKCqQJEihJAxw8zJZ6cwUon0SclUSvbxZc7jWne9z4IiRQghI0IY7wBXUIKdpsgWqaxEsq5IvCyRyhKlrOzoZaBIEULICKHjJczU4blCZG3Xr0pHBobh53ZdeXWU2V4WihQhhIwhZdx90VDcCu6+sq5BWlKEEEIKybOyIldfjruvjFuPlhQhhJDK5M8tpYxBuG53n6ufy3UOWlKEEEJKk+eqiwcBl4vuKxsUkSpTcgJ5ihQhhIwRWVaPud33/XAj4LKiCqMCS5WnSBFCCOkAc4yUe1/+uvk+S9DKuvtk6asmhBDSWLoNYOj0nK71KtCSIoSQMSAvuKGX57PXq0KRIoSQEacTgVBGor8yR+vBv65zu1185abqoLuPEEJGCD22KViqWzNKqcQ0H679rvXkNdRnrdGSIoSQEcEWp6wcfS604CRFSiWSTeSJl6676DxVAycoUoQQMuKUFQbbStLFze126qRk1nSk1nMuqtS1091HCCEjhMt60mS58sztrvVgAvmwrPFq1qQQTFcfveo6co4pAy0pQggZGdwuPgBOi8clWra7LzoOgBIiMYu8S3B8fawQuVZQWbGiJUUIISOCy4NW1cUXrSO2oLQ1BWM906JK1IHC9SJoSRFCyAhR63ioQKEAAWc/VNb7OqFIEULIyFD3gN38cPTorKFQ9WKgMN19hBAyguQJRpn8e0HvUzx3fNWkskXXUEb8gBG3pKqPbSaEkOZSZgBv1hxQ7rIAPC/q68o6xnwvpcysD0iOxyrDyIqUmQheZ/egUBFCRp/sEPS8Ab72WCqlFKSUEJ7IFb+6cvRlMbIiRQgh40zZAby2e84OVReOMlnrVbNJlGGkRYqWEyFknCmybLKsoLKuuKrXkvc+CwZOEELIiJLl7nOV6+UUHt3UPbKWFK0oQsj4IRJBDuZrolRGVgrzmJQ1FXb0K4QBFdF6EAGojDpc99+gamFvKGRkRYoQQsYNM8Chm+AJGQqNWZ8SgC/M8yAsq/+xdiROmjYcVEnjiiJFCCEjRXYAQxl3n3Bt19nQwwI6YlqEwiVF0qKyz+CyrMp6uyhShBAywlR19wVCpBPJmulkjZeiSL4yrjxaUoQQQmzKuPuUlEHOPglDmMIsFCLbIusFFClCCBlzEqIjDKtJAKZjTggkrKR+CBVD0AkhZAwRxn/J7Y5IvAFCS4oQQsYYW6Sg3XqRCVPPgJ5OB/NSpAghZEyJwyNiwdARfEK4Jy/s6DyuIAtmnCCEENJ0aEkRQgjJxopC7ze1W1I7d+7Ei1/8Yhx11FE4+uijceaZZ+L+++9PlDl06BC2b9+OtWvX4sgjj8RZZ52FAwcO1H0phBAy9iiloiWzTPiffhfMyKvXMNAYitpF6rbbbsP27dvxve99DzfffDOWlpbwh3/4h3jqqaeiMu9973vxzW9+E1//+tdx22234dFHH8XrX//6ui+FEELGEFV6YkFl9Tbp94mteuxURh9SVv6/uhCqFznZDX75y1/i6KOPxm233Ybf/d3fxfz8PH7jN34D1157Lf74j/8YAHDffffh+OOPx+7du/GSl7yksM6FhQVMT0/38rIJIaRxtFoeJic9CCHQarXgeV5qwK6UMngVEp7nBTPpCkNcBOBruRIAPGXnSorq0kvRbLzxofG+dtvHQ/fuw/z8PFatWpV5TM8DJ+bn5wEAa9asAQDs3bsXS0tL2Lp1a1TmuOOOw8aNG7F7925nHYuLi1hYWEgshBBCbAI3HVDOikq6+fR2O5pPxIljS85NlShvLLruKpZRT0XK931ccMEFOO200/C85z0PADA3N4fJyUmsXr06UXZmZgZzc3POenbu3Inp6elo2bBhQy8vmxBCGkzS3Wf2R3XtONNZKKTOKhusK9G7/queitT27dtx77334rrrruuqnh07dmB+fj5a9u/fX9MVEkLI6KBUUpiylo4wxclc9NxSHSxl6FkI+vnnn48bb7wRt99+O4455pho++zsLA4fPoyDBw8mrKkDBw5gdnbWWdfU1BSmpqZ6damEEDJy2GKklCoX1KDdhdB9UVZOCseAXGVsTk3VUWbejhxqt6SUUjj//PNx/fXX45ZbbsGmTZsS+08++WRMTExg165d0bb7778fjzzyCLZs2VL35RBCyBiRbz0Vbdd1JJVEGIuxxSxWtaOpArVbUtu3b8e1116Lf/u3f8NRRx0V9TNNT09j5cqVmJ6exrnnnosLL7wQa9aswapVq/Cud70LW7ZsKRXZRwghJBvTgnK59nzfBxBH+qUsLjOcQukEtCLaq0kkSBdIiVTCaDPfhAOwRElRq12krrrqKgDAy1/+8sT2a665Bm9+85sBAJ/61KcgpcRZZ52FxcVFnH766fjc5z5X96UQQsgYYoiK7d4TSYuoTF2xSClrq5mKQiT2JU5nvomqKG929XycVC/gOClCCEnjtTxMTLYghIDXaqFljJMSUobTcATrUohgnJTnxduFgFIKPtpBhUIE+3WAhBduQ7AIMyxdj5kKJ0YMN8URf0IEVpSvIBTQbrfxwA9/WjhOirn7CCFkRBBSQngtSCkgvRZkywtsHilTA26FEBCeFxwjACHC/VCQClDKjwb+BvETMlgAeELCC9fN8VNSCMiU9YYoRF3o7OoVbCOKFCGEjByGKy5jinh3eWOLPgaGCy8Vupc6Wyhoyak/rIlAEtdUBEWKEEKIgUAQ+G1ZOwqAClx1EApKhAEYpoVm1GDWpszMShU7mChShBBCLBxWjlIQPsK+JR9KW0NKxUHqQriOTAhU1SAIihQhhJAUSrlT9aU2OQqKxO7uYvMoUoQQMsa4RCQe7FtfIr5OxYrTxxNCyJjiyjiRXq/vPJ1AS4oQQsYYM6dft645XVdWPUXZMFxQpAghhNRGGfGpko2d7j5CCCG1UTjZYkVrjSJFCCEjhA4CLztYdtihSBFCyIiQSHlkLE2GIkUIIaOEQCoNUpOFiiJFCCFjTtbEiOb+Muu9mFSD0X2EEDKm5A3kDdaLE5ab5XthsVGkCCFkjMkbJ6UH85pjn1zrZh2uelznZAg6IYSQvlKnOGloSRFCCKmNPBGy+77KQEuKEEJIik5SGFUtWwaKFCGEkASx0Ci4ZoAyw9p7Hd5Odx8hhJAUOmjCNIxMQerX2CtaUoQQQiKqhKD3Q6hG25LSf7/6x5cRQshY4wpJ1+/rZHRFSsCaw3hQF0IIIaOFHRyhhYqDeSsgon8CVPQPISMEvQWkZnRQRGwp5Zd3ZZuoM8Kv0SIlJCBl/Ec0I05UIipFROnrgXg6ZN9X8P1wa7J4DH/8ZNiQyeShgDVIUim2W9I1QfsCgPJJal3i1K111WiRkl4gUlqohBCQUkIILUCBAkkpIIUEhJk8EVheBrScwQ+Fim5CMswIAcjgkUtICSmD2CffDxqwUgrwUZxwjZAcTPddtyLTrRuw2SIlQwGSwhAp/Uf1AWPyL2E8fWqhkjJUfi1OOtbR/HsKugrJcBE83CZDgaObCoR+7CIEQOciEY+DKmcNZbn4xtqSmpycgOdJyFCApPVk6ft+KEbxdtMtImVcxvcVlB+GXZr/htt8P1gSY9wI6TNCBBaUa2I7GTx1wVcUqrHFmGbDTPzauVAB2t2Xf9p0doqsY6oOAm62SE20ID0ZuvjiH6oQAr7vo91uZ4pUsF2XAZTvw1e+UQbhevgk4gNqGVA+KFBkcAgBGTzapkb9RzcjnxI1rgRen6RIAd1YM9XcfVlRf91cQ6NFSnpeaEnJSKC0GNkjo10iZXY6+wIQSiS+2GTGXgUpFYQIrCtlWlWE9BX31OCBu4+MO7r3oiz2vbJK8ENeFJ9twdnW3FhYUitXroDnefA8z2lJudx9eptSCu12O3rfbrfRbrcBuGep9H2F5eV2EJDR9rG8HFhgvh9aV4T0AYHklODOhy/pBxGAjPIjIS4LR7+61u3BuS5BqSJQ3dBokZqcmERrohWJlO3WyxIpHfWnhUkpheXl5QKR8iFlIH7Ly4CvfPh+8ORKkSL9QwT/O9x9UVsVghGqJEWe2y1LiELPckfnqmtgb6NFSoZWVJZISSmjV/0Hk1JG4qWxOxhdImU+GSgFeJ4PIXxAAUrEfViEDBzzzsKGOfaUDWQw38duv2BMTtZA3U4j+qoIWKNFasWKKUxMTDhFSmNHkpjuPm096SCLONLPd1pSS0tL8H0fnrccuRSXRBvKX04OCiZkgOjfQtB22Xk6Vjii+9JFil155j47A4X7tPnn0fXZ9Zah0SI1MTGRKVJFftTAfSexvLyc6pMyXYKmSOnXuIyA39YD3mhNkeEhbvsMRx8nXNF9dj9TkdvPPMY0youEKnEdlkCO7WBez2tluvvyREp/CVqobPeePt5+ItF1m+cJ1rWVRqEiPSby4vUmmScZPcqEgGfv610bGwtLampqEhMTE5kh6HkiZab8UErB87xEpJ9tQelySUvKh98GWi0/DKiILTBC6kQ/0QoEE/wolEziSR0ba6qMUcoKnBh0I2q0SLVaLbRaLedgXsD9Rzfdfe12G57nRSKkgyp0Odu60u7AtCWlhZFmFOkNkdtFBNkmRZUbhwAj/MYQ291XlmT5wT/lNFqktEC4LCkb059qipIdwmuLlekO1OuplDRSQCj9nv3UpH+YVr69PWiIbIzjRvzdZ9MkV3GjRarVmsDk5KTTqrFddYlxJIYAaUtKH+MSLduScvZPKUAKHxKIJgnh7YH0krzoLRUO5OUD03ihEw8I+LkP7FWi6wZNo0XK87yEJWWOwNdC5Er5YQqRaTWZ+01R0/vsuqIFsRVF1woZFhjVN4YoFXzvqmSf5QApe32NFqmU261kTH/vLwwUKtIzhJBRgln3fsMbwIY4VpQOqKlca7V7qn0/7oZGi5TdJ2X2Mw0kyo4TJpIeE/3oM1w5ZrlghS6/sUPV92gSeJOKy2VlrXAJlR2QVkSjRQrI/2N0w7CbymSMKdHOk2ltFB+exgVjDtdO7mHuIJyw4ozynVDl2hovUnbnsTmOyRWCaQ/QdeXwcyl95h9Vj19JiCQH9ZJeoftBEb0m9hp9qQKALwRU1P7NO1g/r5kMCldQWBFmhopOn/nrDMrI9xnUwOWXXw4hBC644IJo26FDh7B9+3asXbsWRx55JM466ywcOHCgct222OixTzoPn2txiVSZJQt76oRo0GUzAmdIwxAimIVDimDyQ9PlbS8iWsKDhIhd0myfI0+Z+5eJ2yuVzrxjJ0PI8mbVJVQ9Fak777wT//iP/4jnP//5ie3vfe978c1vfhNf//rXcdttt+HRRx/F61//+sr1lxWXIsFxbw+CyLMsq0z44yc9JzvS1N4WPkUZh5rrfbhU0ljMppJ1/6wiVLZolRXPnonUk08+iW3btuELX/gCnvnMZ0bb5+fn8U//9E/45Cc/iT/4gz/AySefjGuuuQbf/e538b3vfa/SObTVZC6+rxff+KMq57pSPlQ0H7y92OT8QYUChB++Ivir8gZAasQcm6ublxQSnpCQEJBhDgpz3VwCyysoK4SMLSvJhjpKKFSznkrV2WVUX7exAj0Tqe3bt+PVr341tm7dmti+d+9eLC0tJbYfd9xx2LhxI3bv3l3pHO3lZSwbS7sdL77fttTfB+By9+UJVZFwwRAoH5AqXECRIrUSJxEIsptICHgQ8CyhsgUqcgtGi4QUMilUbKujRUU3X0FlsD1KLkxhsiOuXcODqlxfTwInrrvuOtx999248847U/vm5uYwOTmJ1atXJ7bPzMxgbm7OWd/i4iIWFxej9wsLCwDynxpiq8nenu8mNI/Xdegvqbi3WSVeCKmLRHeSQCp3nxEakdimg3qSqLC8qDVcmQwHOjam3/Qqg0XtIrV//3685z3vwc0334wVK1bUUufOnTvx4Q9/OLVdqTZ8X0ApafhPTaExwydNMQpEyLS4gmk2kmmTgmALHYShsLwcT5CoLTBz8dsKfhtQPjilPOkKIQBPmk+nwavnefCkCIdJKUC144PCBi8FILxwVmolAM/I56YAXym0fT94yBNAO+siSPMIv2MgyJgvhAofVJLrgWcpDoyIAyQ6Exv7AT9voHnVwIraRWrv3r147LHH8KIXvSja1m63cfvtt+Ozn/0svvWtb+Hw4cM4ePBgwpo6cOAAZmdnnXXu2LEDF154YfR+YWEBGzZsgO8vw/cFgj84EHw72nUnob2ZWmgAleir0v1XQCxoWqS0KzAQskCk2u3gNdhvLwrKV/CX474DQjpFCoGWF8xVJqWE9DxIEfQnSU/EUxmq+FW7DWQUXpro+Ybygzbth23T9wEl4l8MGRFUmBopXGyRCr7xIMtoIEoSSgUP5TqiL0gvV204jUuozHFXdlq6stQuUq94xSvwwx/+MLHtLW95C4477jhcdNFF2LBhAyYmJrBr1y6cddZZAID7778fjzzyCLZs2eKsc2pqClNTU6ntwe8yfiKIf256CfZpwQmOUZFgJSP33K5A08IKxMiOBoxfo6+Iv3hSE/oHLc0hDnp7KFDmsCeh/3HdCERQOh42Ed6kwCY7qmTrQewUNIUjKSL1tgpboAZmSR111FF43vOel9h2xBFHYO3atdH2c889FxdeeCHWrFmDVatW4V3vehe2bNmCl7zkJZXO1W4vh6ar+UfVqq2fFmBYPioxNXxsYSHl7tOiFLv7/DB6MK7DtMbi4zv6sxGSSSIhsgiCJXRePv1oFLju9DpSdycFAFJChE9jnlLho50PXwoIP35YI8SkjJhkiU8d/VQDyTjxqU99ClJKnHXWWVhcXMTpp5+Oz33uc5XrCcTBvU9nfgAQio1vWUS26y+9Hvc/qTDEfTkhdIGJHPdZ8QdOekUsVDKwqqAzSYRtWuhwCGU8qiWP19slEBwrBCAkpPChwnsJ2zDplLoH8Ub1qjoD6vvEwsICpqensXnLCWi1PGcZs4POFiB7Cnh73Zw+3sxesby8nBAwpRSWlpaxuLgcBFYsAUuH+/AHICONACClwMSEhBeG8+oZqD0h0ZJeIpQXgGVVZYz0M9zYZrs+vNw2Hrr69jFJLzDGH0xNTWJqxWQUcGNODmsn5Dbvl2YIee6pHGOhygiVeV++794HMD8/j1WrVmWWb3TuvuWlpaivySZPpMoKVjmR8rG0pOD7gM8wKdIlyUG47nEmWe4+X5QXKfPZVErdd6vL9PhDkkbRq9DysjRapNp+Ozd+1uwMzHLnVXH3LS+n3X3LywrLYUQfw85Jt+jbgTm+KTFQMlxEmNlYRf8JQMSiVRQMEdUpA/ehis5FkSLDRaNFyvdVGCCRxgx9LOPiSwZDpK0t3e+k+558X4sfguEq/GGTOjDS7bkH4gY7hDEJXdA/pcJjwwcz5A/ojDwNQQyyFSrM9kySFLnvemltNVqkAsvGndnJdGl0IkxJ370KB/OaIhW+thEJFSHdEKQxClPqyXSG88DyMXz/hl/PzG6kgyRszIlA3SlrkkGBFCpi4xKjsiLVqZA1WqSCwWfp7UUuvLz9aXef7ptC5NbzfSQtKP6YSQ3YFpQ7UScgwucyLVRKKYQOwCBKT6TnmYrKIznoUhhl9bkpTqQsZYXHLldFsBotUu12G1kKYVtJ2ZZUoDKxK88ULKDdNtx7PuAbwsTfMqmDtDi5LZ24tKMC117jRmAH8druPkLKkGVJZe0rc3wRjRapxcVFSGOqAfPHrKfuyBIpHclku+9S620zGwVicaJCkZqQUULyOEef09UXClhmb1PFp9pe9yWQ0aJX46CKaLRIBSLkNiPNaLxsdx8MYcpY19GDFCXSA5JWVNqCSo9jcTrywlejg6rovPqGU8eHIGNDXuLYXtFokVpaWk5YUoApUj6Wl43Er1HePQSJNhG471RoGWWtA6BAkd6RECbkzsMTW1J5lfXrwsm40E1/Uh00WqQWFw+FobPpfUGgg2ExhYFNylAf021nChLFifQac9CuFAKeF4iT53mRBeXMEkAVIn1kGFzDjRap5WU/ikayhcqMxosG2lJ0yJBhjoXKcvElFoTGUs1t2RwnRYjJoPsuGy5Sxsh66/dl9ivlzf5OyCAQIgiWCMZD6UCJZOCE+74QJkwSccReHHqedz7hFiJ9AzI6x4RUgM9QdJKEllQHHD4Ujutw7WQUHhlipAQ8Gb56sUgFbj8ZipXDoor+S2IPxHXhSgAaDRAO8lYEU07p3wwf7gjKWVIMnMjA5eYjZNiJjRY724O2ooJS6Yi/ZF+W/leZ2zLuFebsqMlXRK/xzciYwDNyVZBxppcZJYpotEgR0lS0GJnh58n95jtt0iSFKioLwC9lSaXXg4jCoM/WvA4hAaFdfhSqsWXQVhRAkSKk72gxkNK0olyLcq7LMIAi4Uko6JPS5cxjdKCrlIaHzxi4HrnSKVBjDQMnCBkjhPUabRe2e88tXGZ5rR4KiCys/OCJ9HrynCrcHqyHs39QowhFipBxwBSo7ASy+TcDqSP/rHr9MFowS1B0vTo9WHxOGS7KupbAelOh+LHvlwwKihQhfUQHTSQzTWRPwx0dp7frMHUkp4+HCPulSlxDNOW8UqG7T8Yz/Boz9wrRDkPd6/nshHQCRYqQPhK52TKmhg/K2AJlWll6nJSjDNBRdJ8daRgeEe1XZZSPNAqz/WStm2QN9O6HG5AiRUif0NnO47FROuQ8HtCr0yOZgqHHUOmynqctKR3koKCn/jRvJeZ6uk8rDo4IxkkFrkQFAeUH1+f7IpjKRgFtkKaT517WA8hd665pXvoJRYqQPiEE4HlxZF+cry+ejsMUpLQLMMxQ4clAUIzEyX4oVFGSZLiCMwxh0guCUHglRJg6TAZT0SPIeanjM9oMQ280+UmLsxcACbdyVQus6JrKQJEipM/YVk28PZ3lPDXYF4b4GMIRPfGKeJdJ1HUlkuvJkyXrJONJVr9omeN6UZ4iRciQkAiEwGDDfgFrsk/SeAY93qlTZHERQki/GJZM5CmBGo7LIh3icuM1BVpShAwZwyJUpMEYfmGXKHXbn9RPKFKEDADTtWe7+QjpikSWkqxhDe4IP3O/K7JvENDdR8gA0KHj5kJI16Qia9wDw/VrlojZZQcJRYoQkgvls1norCbpsXHFgjMMomRDdx8hxEliZmvSGOwMImUDJoY1sIKWFCEkEz04mDQDV2qsqkmMh02oaEkRMgjCqTCSU+0auSKi95o4kWxqW7ReRk5UeskYwMvw8wYi3HkhgWZF9JlQpAgZFJFQqXjdFJuEkBn7hakefrwu8tREAcKP6xcOsTKWIJiDA3mbR3GaozIBE8MERYqQQSAMMUpl2zOUwba2EscZdRWJSSLqyz6VS6jC3RSqRmG7+9JZ75slUABFipD+YQtFgtCCgTL0QwuGhFuFLKsrT6mU22JKHJM6nOpEBg9FipB+khAqLQI+AKFncAplKn4fdBmJ8DCXBWavu8gRKTOMTznEi5ABQpEipN9kBUWElpRNbFHpCAe7TJeCoiyhokCRIYIh6IT0CXNwZUO6A0jTsKL7gOb0PWVBS4qQPuKaS4rBCaROOhnIO8zQkiJkCBi4SDX4JkZM3Hn6co8Y4rx9AEWKkKEiSjSbiEIXUdhET8jwQQ5cOEllRJi4zzUuypXlvEomikFBdx8hQ4I5dUd08+ilOGnszjJjIC9dkc2j7GDevH4rc9+gM/RTpAgZIhJCZfw3uOsZ2KlJhyTGbOeIj/PYIbSm6O4jpEHoREhKAH64RNsGe2mk4QxrNCAtKUIahIIhUAheAWYrJ/UwjEJFkSKkD6TGSCVi0UMXTLjku2TiTBRxSqRw3c5wpBLvgjVl5JpwjQmmf6/hBBn04+Zj5oQ0SmUEUcRtz32c84w9DrqgSBHSS4wZUoUUkJ6AEICUXnxTkDISp2i7lBCyFWyXMljCdRV56YPtAAClIKADHoJoB5XIZp4MhvDD5Om+ElChr9D3FZaXFZTy0W6raDtNtOYgBSClCBcgdgZrAYr3B+1N5oylUhAiCOLRwTx5ARhFQmUGYFQJxuhJn9TPf/5z/Omf/inWrl2LlStX4sQTT8Rdd92VuMBLL70U69atw8qVK7F161Y8+OCDvbgUQgZKIE7BIj0B6UlIz4OQHoRsJRapF28C0mslygTbJgDhQUEE4iI8SC/eJ2RwnC6jINH2geW2QtvXSyBQvgrr0MLlA35bYXnZx9KSj/ayIVKkMQgBeFLA8wSE0ALlh2Kjgv2eQKsl0WpJTEx4mJxsYWLCi7Ylj1XhQ1a+QGmxk1LC87zUIqWMlqoDjGsXqf/7v//DaaedhomJCfznf/4nfvzjH+Pv//7v8cxnPjMq84lPfAJXXHEFrr76auzZswdHHHEETj/9dBw6dKjuyyFkaElNQme5ACM3oFEmeG9mqbXHuSS3a8xQ8lh3wkS2Vpg5+7caTG6mfSDrm00Lh0rtd6276nDX53YxlqF2d9/HP/5xbNiwAddcc020bdOmTdG6Ugqf/vSn8cEPfhCve93rAABf/vKXMTMzgxtuuAFnn3123ZdEyNCRN2bFLDMKaW0I6YbaLal///d/xymnnII3vOENOProo3HSSSfhC1/4QrR/3759mJubw9atW6Nt09PT2Lx5M3bv3u2sc3FxEQsLC4mFkKZTJEDDGGlFSL+pXaQeeughXHXVVTj22GPxrW99C+94xzvw7ne/G1/60pcAAHNzcwCAmZmZxHEzMzPRPpudO3dieno6WjZs2FD3ZRPSM3Qwn35XRXQoUKRJmIPR9dIttYuU7/t40YtehI997GM46aSTcN555+Ftb3sbrr766o7r3LFjB+bn56Nl//79NV4xIb1DhP/Enc/hdooPGVFMgapDqGoXqXXr1uGEE05IbDv++OPxyCOPAABmZ2cBAAcOHEiUOXDgQLTPZmpqCqtWrUoshDQCa3wU+5cIqUbtInXaaafh/vvvT2x74IEH8OxnPxtAEEQxOzuLXbt2RfsXFhawZ88ebNmype7LIWTgJIL2UBwlRUgTybKcXNurWFi1R/e9973vxUtf+lJ87GMfwxvf+Ebccccd+PznP4/Pf/7zAIIf5gUXXICPfvSjOPbYY7Fp0yZccsklWL9+Pc4888y6L4eQgSJEMMBSSIQDKN1jTghpMlpw8tpyp/1VtYvUi1/8Ylx//fXYsWMH/t//+3/YtGkTPv3pT2Pbtm1Rmfe///146qmncN555+HgwYN42ctehptuugkrVqyo+3IIGSjRYF4BChQZSToRqCoINejJQjpgYWEB09PTg74MQnIRAmhNBIsQAhMTLbRarWhkvjn6PiifHLk/MTGReO8aRyVl7LHXN4B2uw3f91PrrhuF7/tYWlqC7/tot9s4fHgJ7baPdhtYPsxUfk1jcoWHqZUTkFKg1WphYmICQgTrnuel1nU2CLNt+b4ftpngvVLxCGHXgF1z3fXwldX22u02fvw/92F+fj43zoC5+wipG8N6kp6A52mh8VLCRAjJhyJFSM2YGY4CcRKJV4oUIeXhpIeE9IBkKj6KEmkuUWb96H1n2cw7hZYUITUjBCC1u4/WE2kwKpyYLE+Let2uKVKE9ABzbBS1iTQXLVAiEcVnr9tCVWxhlZ+ojCJFSE3omTKkEXJuW1JlpjwgZDhQ1npee9VRe1keA7uu8lCkCKmBYLCujujT63qG1PREb3T/keZgiopbgGLLqWhCK9OCoiVFSF9JDto13X3ZAkWhIs0m7q/SbTlw/xklHFo00IwThIwjptTYmSUoSqS5BJZPIDwSsSVkt2OB2DLKdvfFkYLskyKkv1hjo/IWQpqEUr4RKKG3mm694jathckUKaX8UuenSBFSE8nJDR37aU2RRmNaUabVZFtQtqVl90MxcIKQviNgpEKygiUIGRWywtB7CUWKkDqIpuTQ6Y9k5PpzFrcScBZWbwheA3NCkxHAbndZY6TqhmmRCKmBIPuROT28O8w8b5xUN31XnUyBUMfU3qRZ2O2vSjszI1YBBeUMJVfGXrOMXa48tKQIqQEhJVotCSmD6Q88zwOAaCoEAIkpEexX/URaVaR834+eaF2TyZn16Sk79HFKqWiKjuDVh++rTroNyJCgAxPMsPBux+hJL4jqcz9gidDXrQDhjvtTUHHXlQjC01XR2GADihQhNaDn7/G8YJ4oPV+PnjcKKHdzMMua2MJjbs+b6VRfgxZBXU7PGeT7PpaXl8P1sB5Qo5pMnkiVfRDS7SQYnC5LpvaKrSgnRh1CBLpWBooUIbUggv9yxkeVfYLNKpvl/7ezUldx45mT0cXbQJUaI4rapChRprCC9GpJ4aNIEVIPBQN4swb0ujqjXeLUSSSVLVjsfxoPOnkgytxWlOWoDzBwgpBuEWbgRL5rJS+QIm97FaFxuQApUONBVsBOtTrM9lrXlXUOLSlCusHqP84tmmEldYrLzUdIJ7is+2EZ40eRIqRT7FRIstjNUuaHn3eTcFlHVfuhyIgSmPMdD2MYVujuI6QLhIwzTQD5IuTa1+mYKHOdFhXRmA9NowItKUI6xJX5PFWmoA+qU9dKljVl7ydjQiLAITtHZBPFiyJFSKcIY6JDmR3dlzjEIVq+Xy4btE2Wu4/W1JghAOi5zBwzQVcZHzWM0N1HSIforOexQOWnNiobyZdX1obuPgIgtqQsd1+TxUlDS4qQLigb0ZcVgt4NFCcCIM5MlMitl0691R2Da1sUKUI6JfHkamw2hMmcssPM3ZeoRohUH1MeLjcfRWp8EQiteZlsh0DdQgXUK1acmZeQnqOfYFPbczJOOOvJyULhgv1PJMLxoBTtarCbT0ORIqSH2DeJoqSeeWUoRsRF7OrrR4AELSlCRg7borKpYjlVPY6MAWFUXzBNjIwy79eSFDZRRd2ZhylShAwNZRPMliXrOArX+GFm3wdqdPENiZeQIeiEdEOUacLdSV3k7uvJzcUBgytGmIyhDx0/DBkDg4ehO4uWFCEdktcX4JrosKoI1ZV8Vr9nJOBootublMloUpsqEaTDIE4aihQhnWKMT4k2ZVhNRU+4Zhi6uW5SdSLDTveT5uFy+dlU/d6HRajo7iOkGxxWUl74edksFGVgKiQCRMnPM7NMVBkKMYzQkiKkC7LSIRXdMOw6qgiMS5goUmNKaMpn5ejrT1h6b6FIEdIhUZRujosvUb5H0X0UJ6LJEqEmipOG7j5COkZA5EyL4DyiILqPkNJIxKm5BnIBRlbbHh5HS4qQTrEyThcWz3H3mUJFy4gUYueN7LtKCcQ2Tpn2apapdsEUKUK6oryfP1d7hCgokBcBKKBvAkX6Zu6nFjacQoOku+EO5rZEE08VFYmXdBlXQxOlk1dQpAjpgiBRjIA5j7yCgK/C36xSQNsHhICn4t+ltp6CgAddV1gPVHBTcARDJLfHi4rqVs7fvlKAH5Yx131rXR9bdwIcUj/x8Adt1Rj+PyQDekzMSTaVUoYVD0NolEPj9HmUca6wbPSg5BJGvU8ZdZSHIkVIxwSCpJdAYGTwXguCD/gi+IH6CpBKu2hkNOBX1xW8iPipVam0uaPFSQSVK1//+H1D7JTzMD+szhQqc9HbSAMQsUAFIqNFwxaqdFSfUgrtdhsAolehm6+IBwe7+1sN802ZFpRK7tObosMMkdLHKZegpaFIEdIN9u8s/EFnPVMGFpAIvXv6KTb+sYtEOUckoH6vzKdXROvKWM8itrqSH0QfTZ1qDkkjSVivdtlsUYgM81SdukCOoChhNDnXNSjHtuzrtKFIEVIV43foelId1FgVXa8Wv7IBGPF1UZ5GidranCoR6FBkFZW0mlxQpAipiilSGYETeXn8eiFUdn1lhSotrgyoGCWy3H0VakBvQgcZ3UdIbykZ+puXXLYuobKj/jQMZR9dapcNI6S9/jN2d7UUKUKqIAAZBlIJ2Z1br3zoerbYuCwoMh50apXrgIgoDjSO2Um33y7cdHVRe8aJdruNSy65BJs2bcLKlSvx3Oc+Fx/5yEdSaVwuvfRSrFu3DitXrsTWrVvx4IMP1n0phNSOjoKS4eL6YZe5eZQRsKJMFFk5Asl40O33LcxInSiUPa4byU0Do3aR+vjHP46rrroKn/3sZ/GTn/wEH//4x/GJT3wCn/nMZ6Iyn/jEJ3DFFVfg6quvxp49e3DEEUfg9NNPx6FDh+q+HEJqR4/y1wIVbCvOJqHfu/ZX7a9yWVBVAyXcdYlSbkwyvET64mqPxn/Zxwuj7OCbQu3uvu9+97t43eteh1e/+tUAgOc85zn46le/ijvuuANA8GP69Kc/jQ9+8IN43eteBwD48pe/jJmZGdxwww04++yz674kQmolCIZAONGcTEw0lyc6dVk6ZhSfiUukssQoSxCH6AGalKSfEaSDoHZL6qUvfSl27dqFBx54AADwgx/8AN/5znfwyle+EgCwb98+zM3NYevWrdEx09PT2Lx5M3bv3u2sc3FxEQsLC4mFkEHgcu/lCVV8XO/EyiTLoqp88xIUqiaR/H5HS6Rqt6QuvvhiLCws4LjjjoPneWi327jsssuwbds2AMDc3BwAYGZmJnHczMxMtM9m586d+PCHP1z3pRLSEbZQpfdnC1JXfQglI/d0tF9R+bquiwyC6i7hxHurOURdUsLaOARxOLVbUl/72tfwla98Bddeey3uvvtufOlLX8Lf/d3f4Utf+lLHde7YsQPz8/PRsn///hqvmJAKGJZT1pLlfnGVcZ/CHa5eRUiqlC8TpEGGi6IHpbicw7rXYqTH+YWx56ZQxWUH3x5qt6Te97734eKLL476lk488UQ8/PDD2LlzJ8455xzMzs4CAA4cOIB169ZFxx04cAAvfOELnXVOTU1hamqq7kslpDLBD1kkhMYWqahsTh9QliVki4W2isz9dYaZ05pqMCUeRDKFzLSStGtXGG9EutigqN2Sevrpp63EmYDneVHm3U2bNmF2dha7du2K9i8sLGDPnj3YsmVL3ZdDSO1EUbs1ue7KlK1qFVFwSBFRW7ETw8YlMrb3l9otqde85jW47LLLsHHjRvzO7/wOvv/97+OTn/wk3vrWtwII/jAXXHABPvrRj+LYY4/Fpk2bcMkll2D9+vU488wz674cQmomHbHXcU01C0mRlUZIE6ldpD7zmc/gkksuwTvf+U489thjWL9+Pf7iL/4Cl156aVTm/e9/P5566imcd955OHjwIF72spfhpptuwooVK+q+HEJ6QjfWSpnjqgoNBYrUwTDa30I1sFUvLCxgenp60JdBxpDWhMSKZ0ygNeHB8zxMTExASolWq4WpqalUUIQreKIMSin4vp/K1KJf9aLd6GboubnffK/r0/MJ+b4P3/exvLyMdrsdbVdKob0MtA+n56Uiw4H0BLxJD1IKeJ6HyclJCCEwMdHC5OQEpBRotVpotbz0A5VyTI0pAOGFARlSwJMeBGRQ1kdPaLfb+PH//BTz8/NYtWpVZjnm7iOkCkJAlIzqA5C5PQs7SMK13dyfNVYqq5wWtazricWV6tQUsqJJc4/R+fv09xxF+sX7hgWKFCFlMSKesm4Mrmg5e1/WDaQTp4Y9JqpI5MzoQAZXjBZxG8sOdjAFKGVNGWWGSahqj+4jZCSRgNBLRmi5awBvryLt7Dqr5O4jo0HHrUqYqyLaNDyylISWFCFFiFigpDFFRxl3Xy9EKssNSJEaJ7rM02cJ1bAKFECRIqQU0eB77fLLGARbZr3zayhOi0SLakxwtMMqxwWrwyxNMXT3EVICLVJSCkjLkipy95U/RzNuGoT0E1pShBQQC5QWqfLRfem6itPY0BIiJIYiRUgJUgljHGLjiu7LivZzkRVm3g/Rsq8tSOOmBp+4jXRM1vCC/GOS63U0vTzXdBkoUoR0gdkHlBU44Xp14TrONUA3K8y8TJ+VeR6lFKSU8DwvIYZKKfiyRyM4SS3YwyCyrHk9eNxsW8HX7BjM62iauh92kMY9RYqQDrEzPuibQpa7r0wklpTSOeOu3ub7PtrtdlSfWcY1VioL81o8z4OUMvFZ/DYghJ85loYMASIeMK7bnS1ULpHyfSOTiDGYF0KErm3LZV1ggJVpb64ytKQIqRNR3hpyrZvvi9x9ZcTHdgN24hZ01cE+seagranofdS+rHLOhyOBLF9uclCw3ua+Bt0+e9lmKFKElEAIGQVN5EX29f46ioWkikWly+m8fna+QEIGDUPQCSnA9P8XDdzty/WUFMay15Pq71LhtjoulpAuoSVFSBGGrz54W06gXO64sq6RojJmPfZ62Tqyzkl5GhU6e2jSDyvJJj24MXwUKUJKELj7ssdF2WQlce1UOJLX4h40rDvJdQBElXOanyUqT60aXhwPSvEuvT3j0PA1FeRnWM9xGxj8AHO6+wgpwHb3udarjH0qS9H0HC6Rcll5RfTbXUm6JxjHljXVSrnvPlFCZYWZD/5JhZYUISWwQ3PLiFIdN/0skXO5+BjwQIA4Ei+v+WXF9illHDckzYkiRUgRQkTuPtdgSdtN1guLxCV+yQGa2WLmWifNx26L2sWnl1GB7j5CSmCLUtY2oH8WTdnIwk5cgGTYEZmTbwIUKULGElolhPQfihQhBdj9UWPDGH1UMrxQpAgpJDuclxDSWxg4QUgWwnrNIGs23Ly8e1n19AtzcLH93uyEV0MW6UVCunxoUoinYVFQUX0CQYSfUnqbMXXLgKBIEZKFzrEpkfkrNYWlKBN5GVeheZw9KLcu7GAL/aozsEspIaSCkAhuZHohI4EK/4vWHYO3Ew9VA/b7UqQIySIUqTKuvqKJ3apG//XaqjLF1BU+L4SIRZoCNVzUpBkpgQqNq9Q4qQG7utknRUgGQgRTxpuBE91Qt/CYbsYsl2OZJLTpbBUyGmsjR2zMTeMxXdAFww86bbNRsmFTuQYILSlCXAhbpOqJ8KtDqFzClCVSQPJmpV2I5mfQbj5zW7sdTE0CpeAP/j5FNEY2CXdbFIml07YatyUBIRQGaU5RpAjJIPqphx3IZQQqK0Cim8CJImGz+8XyXIxZg3/NwAmdAVsk73dUqmHBCHJwtyWB6qJifrkiY/tgoLuPEAcCgJACQuo5pFwpaHo/l1SeQJW1pIqwM2gk0z4ZYkUGjyOIxYzGDN5np0YK2ok7mezg5cgNLSlCXAhASgHPk/C89DQdZh+Opmh6jqrRfUXbq4pUVsYM13ogUEFooy8VIFTwSMtIv8GTK1SihEApd8ZzY6qOYRq0TpEiJAMBAZlhPeWlSLL7d+qiU3egicvyyxrLJYTu1wjcf0yyPgTkdg9lDzqPv9uCL3Gw3U9O6O4jxIGAiNx96ei34UjYaltSRRS5Jt3uPuPpPGe8GOkTiX7C3rqahwVaUoS4EIjEKWtJFO/jjaKbfqg8S8rerz+jtAb20qAaEMbA8uDhafQFCqBIEZImGsSbDo7o1oIqe1xhRF+JdevEZuVBFF+0r+BgIRjdN0TEX6XZFjvv70xWXv3Ysta8vb/s8xVFihADIYNFekHghLaaPM9LBEwA+dZIVpm88HMz84O9L7oRAMEI2+hiESZbg6VW1g3BWFEC8EXyMIVgm2+ui6BsVEYYufzIwDGbkh4moQMjguFwCkIkU2ul21f49KEfRBK5kcK2oHxo9XK13zLi5BKysl4AihQhBsILBMqTAl4rECdzcfVNpepwWF/2uokdsGD/oBNuPSnCEcYAfKMz3IwrdkRvKbM+JAXI168A2iI4lw8VlTHLkuFAu/psS0opwPdVKFr6kSPd9pLNUCSGVkWBNGGFfvg0020GC3u9bG5KihQhGvO3Gv5Txt2XF4hgrrusKzM8PcuKcl1n9KrdcLpOHUZsX5Oux3Tb6UNgCZAhSEq/p7tvyHBZNukoU6WS0ZvmeCr3oF1X7HpcX/qcvYfRfYSYRP1R8X1+GCL5iojyrdUUJ57qj6ulVtItWWOgRhlaUoQY6MjefmeWqIM6BKrOIBFSP/GD02Cvo59QpAixMOfPKePW6xdm8EOdpo1rMC8hwwJFihCT0J9ij5HKs6yy+pN6QtCbHUVx1YX5uShWw4sRG9MfIvfi4B7Q2CdFSIiOQyhy92Wt94f6+p00TXBlkgCVMKf7xIDbBS0pQkxKRvAN+obuClOvi0F/NpJPv6yoZJT64NoERYoQjYjTAeXdqIclmKCqQGWJrmuuq2H4fMTCGCswDB7ZojZSNIda2TZGdx8hBnFiVbdQDYtAaTrN30cxajhDIFKd0Mnvh5YUIYA1Pqp4EG6WJTKs5A0grnTToKYRFFvcyVme3W2vbLujSBESZZZGqk8qK4Ci6bgENy9/YEczkpORo2r7r2N4Q2V33+23347XvOY1WL9+PYQQuOGGGxL7lVK49NJLsW7dOqxcuRJbt27Fgw8+mCjz+OOPY9u2bVi1ahVWr16Nc889F08++WTHH4KQrgnD+uwsE9HuEREnm7KfUVCkyICoLFJPPfUUXvCCF+DKK6907v/EJz6BK664AldffTX27NmDI444AqeffjoOHToUldm2bRt+9KMf4eabb8aNN96I22+/Heedd17nn4KQMaIu5+KoCu9YMMCvroxXoU7Pg1Bd2GFCCFx//fU488wzAQRW1Pr16/GXf/mX+Ku/+isAwPz8PGZmZvDFL34RZ599Nn7yk5/ghBNOwJ133olTTjkFAHDTTTfhVa96FX72s59h/fr1heddWFjA9PR0p5dNSBIZLFIKTE21MDERZDyfmppCq9WC53lYsWJFlAVdv2pcUXZZiWn1e/NY83jf9xN5+BLZoxEGeMk4q3nV2Xld52y329G2drsN3/exvLyMw4cPh+ttLC8vBdM/LIUL6T8CkWt6anICzzjiGWi1WmGbbEEICc8TaLX04HMYMysXBcooQMRJiIUQceYVFZvRnQhPVhb0dtvHAz/eh/n5eaxatSrz+Fqj+/bt24e5uTls3bo12jY9PY3Nmzdj9+7dAIDdu3dj9erVkUABwNatWyGlxJ49e5z1Li4uYmFhIbEQMq7UnW2CNItxc73WKlJzc3MAgJmZmcT2mZmZaN/c3ByOPvroxP5Wq4U1a9ZEZWx27tyJ6enpaNmwYUOdl02IE9cEbfYTYS8G01a5Ptf1uJYqpINFgLG6Kw4xwhGB2oezhuceTBtoxDipHTt2YH5+Plr2798/6EsiI07Zm3+d02N0ep11Xpc7mpECNQxo952ZS7IPZ80UxX5FwdYagj47OwsAOHDgANatWxdtP3DgAF74whdGZR577LHEccvLy3j88cej422mpqYwNTVV56USUojLWipKwNrbG4f7vL0WScZXDBH9HAphTI4oMh5U8sZJ1UWtltSmTZswOzuLXbt2RdsWFhawZ88ebNmyBQCwZcsWHDx4EHv37o3K3HLLLfB9H5s3b652wnBsS2wCJxdC6sI5lfuAGZbrIP2jIBFRn66iOq6kzGWpbEk9+eST+OlPfxq937dvH+655x6sWbMGGzduxAUXXICPfvSjOPbYY7Fp0yZccsklWL9+fRQBePzxx+OMM87A2972Nlx99dVYWlrC+eefj7PPPrtUZJ+J54Vi5PhufD945e+Y1EFWhJJJP55uo6i+np+JDB257jRlvQ4fnSZprixSd911F37/938/en/hhRcCAM455xx88YtfxPvf/3489dRTOO+883Dw4EG87GUvw0033YQVK1ZEx3zlK1/B+eefj1e84hWQUuKss87CFVdcUfVSAivKsV2pQLz0ay+hCI4PZooXW6T62amshaou6sgKQHqM0byaNr4tOy1SyeO7GSc1KPQ4qYmp8LvLsKS0NdVLgnDg3p+H9JBonBQwOdnC5GQw9mRycjIahzI1NZUYJ+V6KnSNjcrap6k2TkrBB9CG26rLs+5cNwp9rHlOPWYqOU5qGYcPLwVllgAc7uaPTSqjs6CEY6CkJ9FqtZxt0vMkWi2vnnFSYdCMgEz1SZXJ3We+mtvjcVJt3HfvQ4XjpBqdu0/KbJHqF8oHfDXMRjapSlGEXNZzXVOfcO3USG6xi9fZ1geAsF4HxCBy9zVapITId/f1456h9FNOmbI9vRLSDeb3Z4+Pco2XysJ2Cdo/0iwRKHWOEg3IznpR9rqLyej8Jb1HGEsFlXIJSmY7qHAfK1VfQdngd1XuuEaLFID4r2p8YP3dyD6MApMCUFIACoi6s03LSuk+BOtGCLoJhwUhAekFP2p7HIpe7B+YfQPQbjPX9qIkrnljssztQNDGVNjSioRNH+d6ms0aV6Vdf77vG0u436dM9Z0oglmEbj0JKQN3n07ZJaWM2qznBW0YqDjoN7xBKSTvUwKA7HCsXJZLOs897aLRIpVnLfXL8yL0I47WJy1WKu6vUirdP6ZFqg/dZqQAKXSkaCxSrgGTZawoLQqduv6KRMqHgh8/DuXWY6+7xChVf9hQfd+P8vj5vg/lByJFleo/UgoIKeCF4qRFamJiIuqL0u1VyvjhPKsdZrqrbYUKKoEQMvGgU0Zk7DKudu2XDBpovkhFbwZ4HdBuHCBQJxF3QiIjylDFTy383Q+Y0I2SN7IecFtQLkzXXhVcfWAJsdEWVMnzu+rO6sjOsspUeMJoExtrXwltoqAtGQENRVkezLZc5UHL2WRzmnGRUFW1mlw0W6QwHIN2RWhJxY1BQSkBIfTNKuM4Q6H42x8cwhCoXqR1GTayAkJcaE+P0M6C0f2zDCdCW/YiZTUli8V5Fk0PU5W2HB9j1AcAqty4vOwHne7ubo0WqcRsoQO6y+sgTQj7STsWKj1mK3Gc/uErWlMDJboJZ4eMjwp5EYsuEn+H0fpTNIYgjDzui3K5oZNtN1+gssf5mQ8k5nEi1U/loi6ryUWjRWoY3H0iNsgBuIUKUClrSn+PLgEj/UWY/2b8qF37COkHLis/T4D0Ma42XcUVHZUxg8IKqFuggFESqYz9PUcZwRN6kyVUZYI7BMdaDRC3u6/MjYGQnhIGLmgryram3O0xv42WGbsU1W0Egg2KRosUgEx3X17kX60nFyIIlFDpJxbTBA+sqvT1KUZPDBxToIL3ro5oWlSk/+igLL3EUXwy0V47qltkZ/RP1Dnge1OjRSpPiLoTqQoH6qmVRdqUjsORVSrCz3T3DboREMB0jWT5+/X7YSPrRlMmZL4IszNeB66SPhFGhrkeoqIime1ROO+B2joCsl14Zn+UpmwUX9b+bmi0SHkSEBJwxeXqL7Yc9pdSRaS8YAGi8Sd6Pfnl6O2ILtZw+bJvapCIYndfEygSq6xMGlnrtmWph1WwmfYBGfZlSBG2z/z26Iruc94Du3Tf5Q1v6OTYMjRapIQeuOYUqSo3F7NPKdm/VHycBJQMx5IkbwbanNbmuQoHndhRf824BY4mul/TvgE0SZzyKIq4cgmU3X4jzJ8Ilaq3RFZUdtLiqGiGQGXWW+N3V1Z4xnecVKG7r+pNRqCaSCH2fxg/6LS7Lxar8CBn1B9/+30muukmv/Nhd+/1gl5EZZFuSLc7O81VmWMAd3Rfp2RZUr2k0SIlCy2pKsn7BDoTKRm5+/R588ajxPsAIMyTpgAZZlNH+qOQXhAawRDxA03ZqD7zwYOQflNlrFvd5zVf+0WjRSrLkhLhk3Ff3H0iju6TUibyUbndfaYbRcD3AQkFX0Q1UqT6gY53cQgVkO1aSVRBoSJ9pmxqrl6ev9+MiEhldySWrAmdiVSyvN2Astx95vYg8i92/RlxFaTH6K9KP9QUlx+w60/FK3YTKfuUW+UpPLMc22cfyM7zkJfjMQrGMobEOD06OV9i7FZ0Oqn6TqNFKshYHVsyMTpzb1l3Xz2BE0KIxCyn2qrSlhTgbmBCKPi+vsmES8krIJ0hRDhppsOKcpcflkg/FbaP6uHlVVLXmC5r7Z5mu+w3OtCqeBLO+HstjhKOJ3vJKWPXp4rbTK9otEjFNw2R2t43d59V3raWbGtKlzHdgL7vQ0rDdUSfX8+xw3TLNpVhEarcvTWMjzLLKhW/sl32C/NhIilSZr936iilrahklHG8v4oljciUGqRbu9EiJaWAjDoV7L36xlPVdedaz0ElO8bMm5ieBM9uGOkvXPd9cKxUPwmifPMzoDdxvFRZbKsqu52yUQ4S+57g+o4qu/vsbWafeHRscmqYQUT2AQ0XKT0rZdrdBwACSnUS3WevFxwlApefgEjMzGqLle0CjI+1n+orXDLpmLQlVX4ZBVwupKwxU+YTNRkE8b3D7jZIP2iUcPdlWVPGpvgB27CoCursFY0WKSlkPLVxwqQ1S1Vx3enXai4/gTi6zzTJ9fuscTexSKlYoEbjHtgMcv7Wwy5IZW4KVd165nri5kehGhDaEkp3H7iFJu47zIs8dfVLiuDwMPuNYYFF7r56P1kVqpgaw4nD/IjdfFVvMlUFyjgiw+VXPouBSNRHeo9A+TFRw4JCZzeMTp50o5tZ9dORrsluc64HinjsZSxqeaSiA5F065nffdH332uXX6MtqazAiXBvxvbM2tCZSIWWlCN4o0iokttoSfWTqu69oREqfUOqIB3dDMLUllR0t6Ji9ZH0DcHljk1uL661jLsvLlvO3ddLGi1SECL+Ck13H+KvVlV095mli45N2D6OjkpXRJ9et1+FEFH8h154P+gxJd19QyNQEdktI2v8TFWBYp/UoElGnNptMM/dpz1Jdbr7Bvn03GyRivqihP7rIvHH7PGPSouh9i5GpwvFRrWNS0oUjsulrtEoT6HqLaa7r3giOUL6hBVI5WqP9gNx8Br3YQH5mSlc7r7w1MkH624+R000W6SETFlTSoek+z7MW3yUB9b4q5tz4yS+TuVctayseJvZFqI6FaJpRIQwzm/mGjSFyxAwWlK9xxQkcyI5U6yaTB39BAmXEq2pviKEzk2a7YJ2ufuC9fy6q7j7Bm1FAU0XKfvOngojV8a+9ObUPgjjC3a7D3MvJQqFce2PtwvL6tJL4PJTsYGYURWpCeMptdzYqCIz2FWu28go8zzm01BOpWVPmFcsiuqiQA2CIOI3PXFh+tkp2blhlEx1LwAFAhW5+xzVDZDGi5QKfWuBBgj4pkhJEWwXhjAoBD/iVJ+kJXYq+btMmMPmdqGghA9AwIcPX48UF36wAFA6xbmIO6EFAOkFBl+kTRKQQkAqFRmC+hDeJOol6BAWCPImBim0hJCQsgUpvfAGobcHZcwnCu33L7a4yn9xSs/yHN144mO1CwdKQKj4CcblLTZfzT1BbSr6HfhhxartQwdk+GFPub/chr+soNoICmrHBNtgn/GhlB5bKYzwcoHgS5HQE6o6bmpO8sLTAQRtrNIMEr2l0SKlIOPgBgH4EPBDv5qACMZQ6S6rcDUSi+xfc0KYElFUytgeHmeKlFKBTAVl/HB7eIQ0D0YsVOH9RmqfoAyyosMPPZa6ina4kJoQlkh5kUh5nme4AaXRL5D0y8ZCZltOphvGh1JV7+7mo6x2uYUNQQlI/QClAKHScX5Cu8Ctth01PaMN+j6g2j6UHnAevvpLPtRhBUVx6jtBF4IxdhLtqM/J7HuKRxCZNzOBMiOLnNkojLnxgjtoWGbAX36jRcrsEEoN4I07qizHXXBMwtWa1dmUIVCJ80frsb1sJnCM1g13n33e6GPo9mcvpfyNpDtM957VSeh8QhXWMQHm4O34htLJF5i+MSTauMquWRhlcquNwssdSUx1VJ/vqIP0hYF0i5pP6ENyz2m2SEEifmoI/qrRzR6WEGm0deWIbjEOjfUMIhSZuKAZcIHIktJPHIafLsuZr/RxscvG+awyJI2EVMOVcqjTero5LqsvIitHX5y6KxYqMhiqJwOoTt1ZS3pFo0VKu2hS7pS87zF6OBZOoXLVkWf2Bq4/vd0HlG/uSVhYyetSkRsyVatpQZFGUtePuxuhy0t1lJW3L8oz6bPxDYzI69abLChNESdNo0UqgX7ayC8Ur2Z9CS5xUmUEMC6j248yN4vidSGSymRG9gxPkxkvur0ZFIlM0VgW14BNV7kq1+S6vtTiisogfUOP4YveZwiW7V7OWneRGMzraIe9Fqqy9Y+ESFW5kRR9MVlls8qnQ0TzI2d0VFi8bpv06WPNbnRSH0WDd+t0r1Rpd1nH6+wPea67cnWkF21BaVffMD1Jjw1WF6g5jk9nP7fHSelXLUr2ehlc5VzBQHVTts7hiTMcQorS4tijwstlLMj3NeuxEeY5Uv34pGuCCLhqItSpYKVG91f8wWeJUicC5VqPBco3hIsPRQPBECggeV/RA831dk3Wg4fel0eZtjToB5ZmW1IVn3TNJ4wyx1V7+k1bXWnzO9qb6NxOdnTrAJA4q3E0xovUS8ZXaj9sVLW4O8UVzODe33lgRr67D8ZrRx+B9IEsd5+Na18V0bLPUzvj5O4Din37vSgb7M8u7z42Nonc7r54lLnpSqRQ9Y6stDNVKfv0WraejL0Jl1+RsJmYllK73Ybv+2i3g8X32/B9wG8HQwmRzCpG+ozK+J6LUiKVrr/EMZ1Y7HVfAzAiIlUsJOX9s9XKAmbkg+sJJ2kx6fKxGOlyLgtPixSfanuPK4KqE+oSqGzXS34IeV69pluv3W6H64FY+X6Q5cT3w/bGNjdYlPu7ThQp6RFyHVemjNnHNUgaL1JFT75VO6y7cfHZ15Tt7ouFxyWKLnFid1R/qHMcShnqvgHk3dCy+qKUCjJLRG4+itTAUYnw3/SDSD/a6aDFSdNokcrLWJ33B67Sx9Bp3bqjEwjcLPGxuacOj01HDZLRourDU6f1ZwVItNvt0OWn0F5W8NtIitNw3J/GFuUr+AIA/Oj+YQdTeJ43uAvsI40WqaLw4TKhlVl1FglR0VNGljVFSK/JiwTUS+Di8+G3A4FS7IMaGlQYWemHA6p9X0V937abt9+W/yAYWZEqIyJljyuKcon7porObZ6Td4RhoZsfelYElcuS6cU1ZHVu26Kkt8X9T3pMFOjeG0KUil3+rvaUFbVXdjBv1QCxQdJokdKD3PL88JpejYcRArSUGogroq/qD9UOjrH3ZR1Tpf6ijnOXSCXHO6nIXRRZT75Cu63QDl18bLZDgn5YEIASQYClEMH3Z95n9Kt292W1g8zTOMoopaLuiWFjOK+qJGVuLq59ZW5G5cbIJPcN89MISWMO5u1FNJ/rRlClnrKRVXkWlNkPZYqXtqJ8ZjkfPsIHB98IZnFZxs5DCwSrzEPPMET0mTTakjLJG7TW6Q2orJjpYsUuHV0OgJHsKPk0H88jYwZPDE+TGSGMcWr6tShQphNrq44ffFxH7JvLuyFlCVcgVkEVnCuqOdhuZZcrF+jc3afp+QDeEtdgU9mSuv322/Ga17wG69evhxACN9xwQ7RvaWkJF110EU488UQcccQRWL9+Pf78z/8cjz76aKKOxx9/HNu2bcOqVauwevVqnHvuuXjyySerXkpEluumaKlaX1GZeq/PEioaabWi/85mhGhW31K3P9asOsq6GLOCH6ou0cDddhuqreIZd0kj6bY9ZC221d3LpQyVReqpp57CC17wAlx55ZWpfU8//TTuvvtuXHLJJbj77rvxjW98A/fffz9e+9rXJspt27YNP/rRj3DzzTfjxhtvxO23347zzjuv6qXkikmdApW3P5lBosjtGCxJESo3gJQaVT8CxX/7sj+kqmKWZbFl1RPvi90/rmvMsq6SNwdjPBRpBGUt+Ozv3B1Ik2eBF9XR9VKyAQrVxWOiEALXX389zjzzzMwyd955J0499VQ8/PDD2LhxI37yk5/ghBNOwJ133olTTjkFAHDTTTfhVa96FX72s59h/fr1heddWFjA9PQ0Tnjhs+F5bp01nwRcmF9UEXllpQwWfU7dSe1uFIhelUIi47ROU+P7PpaWlsLrB9rtoGx7CVheBG8sNTG1chJTKyYhPYkVK1ZgamoKUkp4nhd1SNsPPea2vL6srMAFm7wbjz7OrCN4baPdPpzab7azdP9TsL683Ea77UP5gL+MeOozMlwIQHjBq5RAqyUhZdLyl1Ki1WqlPAJZGdNNsu5n3QQRdYLv+3jkoTnMz89j1apVmeV6HjgxPz8PIQRWr14NANi9ezdWr14dCRQAbN26FVJK7Nmzp6NzdOPiq1K+DldibEmVvT4O7u019k2+2veZrqeT85Z3hWhLKM/aUimBSllRZPgQiCcbT/zu3feKrHZT5Koz6aZMHUsZeho4cejQIVx00UV405veFCnl3Nwcjj766ORFtFpYs2YN5ubmnPUsLi5icXExer+wsJAqY95IlHJnOzf/KFX6AsrdpJI5+FzY1YhwfJV9XbFrkHeTfmNbTOa2MnThmKhAvofALWBJcWLLGm4EkuMvyz40mfc81/2rqH3a98xhoGeW1NLSEt74xjdCKYWrrrqqq7p27tyJ6enpaNmwYUO0L8s6yqLqF9DpF+a62VU7niHtTSXp4u1cDsrcULLO436yZgLZoUZYCwZzH6ir/dZFT0RKC9TDDz+Mm2++OeFvnJ2dxWOPPZYov7y8jMcffxyzs7PO+nbs2IH5+flo2b9/P4BsgSobHFGWzoWmM7dRv3zCpHdUdf11Wk+ReyYewBsM4lU++6KGEkOcbBd/lsuvU8q42+pqv3VQu7tPC9SDDz6IW2+9FWvXrk3s37JlCw4ePIi9e/fi5JNPBgDccsst8H0fmzdvdtY5NTWFqamp1PZOxEZT5guwzWZ7vW4dsSP9gsZpXSe9gN2T871l9fV0clOo+iO325lbgNzH5d1Uoj4s04JiGxppXG4/e79JXoBFkXuxE6r8niqL1JNPPomf/vSn0ft9+/bhnnvuwZo1a7Bu3Tr88R//Me6++27ceOONaLfbUT/TmjVrMDk5ieOPPx5nnHEG3va2t+Hqq6/G0tISzj//fJx99tmlIvvysG/yWSmTir7AsgSHBvPwCJGO6rJN5vSptEtQhmLkRxFmwTUG1+n7Cr6nAKnicS28yXRG9LQqIKyIKY3+7uztrj6qrB97p+3LbDNm9KdScXSfnUUCSEb0BZMYxuuqHQT7MonskKPgfAg17yVBiiR3m3WtO09Tok32um+qyu+icgj6t7/9bfz+7/9+avs555yDv/mbv8GmTZucx9166614+ctfDiAYzHv++efjm9/8JqSUOOuss3DFFVfgyCOPLHUNOgT9xJN/KzMEHYAz9NcWjzK+17zygZC4xyW4Q4/1zUuEC6KbT3wjslPZAIuLbRx6Opg9lYMwO8RwqaxcuQIrVgZh51NTU5icnIx+/OYAX73ueV7qR2t+17qs3m5+h2V/YsnjQoEJBSpoHwpKteH7yymRWl5ejiYybLdV1G5UG1GGCQrUkBNG9UEAnhe7+jwvFiTdxjzPw8TEROqhyA74KRKaokAhs13bdNp3ZT7A7d93oDAEvbIl9fKXv7zUDT2PNWvW4Nprr6166hRBBEx5M9Xc7lrvBH2ofRll69TH2ZF+Sd9z+FTDbqraKWo/9hOlbSEVfc9mqHBZVBTZkAwbt9ft600+JBkRfRSoZqAtqRDd7JQy193tMaoi4x7XZEYmd18nmDchW6zK3Hx0qwragNvtk1ePEGYDjMXIdXMkNaC/ih79Wd1iUU2cTNdw2oIvP/4kys9HgWoOIn7RTbRTy6jU6boMvugXYy1SQNq0Bar4d+P0Rlnliyy1QKiEIVSd92eQkvTgxm0/wZbprHbtT1tD6f7NLDGMjtERfBSo5pEZ5Zef6aRO8cpjEPeksRWprC+tyA2YPi6/jPtLFYawxRaUS+CC7RkfglTHcqlU/dFliU/WA44tWGU7rU13n75w93Z9TLRW+rOQIaMg7DwqlnHvKiNEdYhVuu31ts2NrUh1gvsLLrackscFj0lBEf3YVORaDK01cyFdoZBtnURlHPsTdWS49FzuvrLuv2Q5U5TifUHUXzIwIwiuiPuiYC5kuMmxnKIilsuvjvFSdt1Z27uNQegWilQJ8l12sciUsaBMl55Swtiefe6gRNgg7XFTpDPCG7hLTEyXnb2eqCJH3LJcdLmXlLK6rItFWqDMaMBokjy6+ppDwcNnVp9UrwQqy53ool+uv5EUqTxR6TSar9iKSufgywt+cF1C7hgbo1OV954uUfol20JyRfFltZv877m8WKUtMDtYQpdzCWHRhybDThCt7Hb1lXH3dXXunDrzvAid1FeVRotU3tNEmT9gnpDlke/Oq3Iedx3ZnaOx24/3pO5wBR3o7WYZc90e92a62+xxUnkuv06vK3br+cZruN1nRF/TESKYmiNoSyI15YZLtPIekOokqx+q7L2wGxotUnl0O/5J1wEURezpLyWdcb3sNZT9YqN+KdIdKvmaJ1L6O/R9PzWo0RxMax7r6n8qK1Cu99lCFQ/a9ZWim6/hlAmaMIXLbJ+9FKqyAtUrGi1SWZZUGQupSpmsAXPxMUCeu891LrNKpcpZUsFZAEWhqoUsd1+ijMP1l1fGtS/LOssq67agYreeSohr8EkoTKOF87dfob/IxnUv6ocFVgeNFql+UByqDuiQ8iyLK7+PzHxNN8K4DuMpC7wndYvL2nGVca276sgSFttNWEWk3Ivp7gsCJoAwWIK5+UgOWQ/dww5FqgZioXE/qZQXqBL+Xbr86sOwSHKLqXLRfVki1albxOVmUUr3hZkDftkPRUYXilQOxa5Ec3+VO4Q9bkqFllj2dZhWlHEY6RFlHjZcrjwzZN1Vpuicxe6+OPKP4jS6FD0UuQInsoIY3N0N2flLbQ9Qlhch77rLdKeUhSLVAcnxClUDI0xTyFzP/1IjoZJGAIUWKt6kukb/qFwBEnq7/QMzAyfM76mqSJllzYg9O5N6vD0QJz9Mf0ShajD6uxOu/sZyfeJdnd4QOVv4OvUCZIlip1CkOiT4o4etq8QdIvjSAVugAgvKVJwyJ69WnORj3xDKBku4LKmyQRKuY1x9WWnXoRYzy83HttB8IpFKR3W6BARIW0SJ6nL6oLLaeFG7HwSNFCn9B2y33ZMqRT/ynD90t08JSsV3hsCaSt9Y3OdxWFEqOWmdH92kjL4Hqw+CaW+6IykGKmEp2aG/UbYPJH/EwXdlWFLKqt9oE0XXodeTllTwZUffP2yxAtvBqGC5bp0PO/qZ2LiF6EjfaAylhWt73Bfudg/2crCwPcbLfM2ikSL1xBNPAAB+ePdDA74S0lSWFpextLgcvntqoNdCCAD4ywDCJrk8RrOaPvHEE5iens7cX3lm3mHA9308+uijUEph48aN2L9/f+7Mjk1mYWEBGzZsGOnPCPBzjhrj8DnH4TMCvfucSik88cQTWL9+febsv0BDLSkpJY455hgsLCwAAFatWjXSjQQYj88I8HOOGuPwOcfhMwK9+Zx5FpQmW74IIYSQAUORIoQQMrQ0WqSmpqbwoQ99CFNTU4O+lJ4xDp8R4OccNcbhc47DZwQG/zkbGThBCCFkPGi0JUUIIWS0oUgRQggZWihShBBChhaKFCGEkKGlsSJ15ZVX4jnPeQ5WrFiBzZs344477hj0JXXFzp078eIXvxhHHXUUjj76aJx55pm4//77E2UOHTqE7du3Y+3atTjyyCNx1lln4cCBAwO64u65/PLLIYTABRdcEG0blc/485//HH/6p3+KtWvXYuXKlTjxxBNx1113RfuVUrj00kuxbt06rFy5Elu3bsWDDz44wCuuTrvdxiWXXIJNmzZh5cqVeO5zn4uPfOQjqbxzTfuct99+O17zmtdg/fr1EELghhtuSOwv85kef/xxbNu2DatWrcLq1atx7rnn4sknn+zjp8gn7zMuLS3hoosuwoknnogjjjgC69evx5//+Z/j0UcfTdTRt8+oGsh1112nJicn1T//8z+rH/3oR+ptb3ubWr16tTpw4MCgL61jTj/9dHXNNdeoe++9V91zzz3qVa96ldq4caN68sknozJvf/vb1YYNG9SuXbvUXXfdpV7ykpeol770pQO86s6544471HOe8xz1/Oc/X73nPe+Jto/CZ3z88cfVs5/9bPXmN79Z7dmzRz300EPqW9/6lvrpT38albn88svV9PS0uuGGG9QPfvAD9drXvlZt2rRJ/frXvx7glVfjsssuU2vXrlU33nij2rdvn/r617+ujjzySPUP//APUZkmfs7/+I//UB/4wAfUN77xDQVAXX/99Yn9ZT7TGWecoV7wgheo733ve+q///u/1W//9m+rN73pTX3+JNnkfcaDBw+qrVu3qn/9139V9913n9q9e7c69dRT1cknn5yoo1+fsZEideqpp6rt27dH79vttlq/fr3auXPnAK+qXh577DEFQN12221KqaDhTExMqK9//etRmZ/85CcKgNq9e/egLrMjnnjiCXXssceqm2++Wf3e7/1eJFKj8hkvuugi9bKXvSxzv+/7anZ2Vv3t3/5ttO3gwYNqampKffWrX+3HJdbCq1/9avXWt741se31r3+92rZtm1JqND6nfQMv85l+/OMfKwDqzjvvjMr853/+pxJCqJ///Od9u/ayuITY5o477lAA1MMPP6yU6u9nbJy77/Dhw9i7dy+2bt0abZNSYuvWrdi9e/cAr6xe5ufnAQBr1qwBAOzduxdLS0uJz33cccdh48aNjfvc27dvx6tf/erEZwFG5zP++7//O0455RS84Q1vwNFHH42TTjoJX/jCF6L9+/btw9zcXOJzTk9PY/PmzY36nC996Uuxa9cuPPDAAwCAH/zgB/jOd76DV77ylQBG53OalPlMu3fvxurVq3HKKadEZbZu3QopJfbs2dP3a66D+fl5CCGwevVqAP39jI1LMPurX/0K7XYbMzMzie0zMzO47777BnRV9eL7Pi644AKcdtppeN7zngcAmJubw+TkZNRINDMzM5ibmxvAVXbGddddh7vvvht33nlnat+ofMaHHnoIV111FS688EL89V//Ne688068+93vxuTkJM4555zos7jacJM+58UXX4yFhQUcd9xx8DwP7XYbl112GbZt2wYAI/M5Tcp8prm5ORx99NGJ/a1WC2vWrGnk5z506BAuuugivOlNb4oSzPbzMzZOpMaB7du3495778V3vvOdQV9Krezfvx/vec97cPPNN2PFihWDvpye4fs+TjnlFHzsYx8DAJx00km49957cfXVV+Occ84Z8NXVx9e+9jV85StfwbXXXovf+Z3fwT333IMLLrgA69evH6nPOc4sLS3hjW98I5RSuOqqqwZyDY1z9z3rWc+C53mpiK8DBw5gdnZ2QFdVH+effz5uvPFG3HrrrTjmmGOi7bOzszh8+DAOHjyYKN+kz71371489thjeNGLXoRWq4VWq4XbbrsNV1xxBVqtFmZmZhr/GQFg3bp1OOGEExLbjj/+eDzyyCMAEH2Wprfh973vfbj44otx9tln48QTT8Sf/dmf4b3vfS927twJYHQ+p0mZzzQ7O4vHHnsssX95eRmPP/54oz63FqiHH34YN998c2Kajn5+xsaJ1OTkJE4++WTs2rUr2ub7Pnbt2oUtW7YM8Mq6QymF888/H9dffz1uueUWbNq0KbH/5JNPxsTEROJz33///XjkkUca87lf8YpX4Ic//CHuueeeaDnllFOwbdu2aL3pnxEATjvttNTwgQceeADPfvazAQCbNm3C7Oxs4nMuLCxgz549jfqcTz/9dGqyOs/z4PvBrLKj8jlNynymLVu24ODBg9i7d29U5pZbboHv+9i8eXPfr7kTtEA9+OCD+K//+i+sXbs2sb+vn7HWMIw+cd1116mpqSn1xS9+Uf34xz9W5513nlq9erWam5sb9KV1zDve8Q41PT2tvv3tb6tf/OIX0fL0009HZd7+9rerjRs3qltuuUXdddddasuWLWrLli0DvOruMaP7lBqNz3jHHXeoVqulLrvsMvXggw+qr3zlK+oZz3iG+pd/+ZeozOWXX65Wr16t/u3f/k39z//8j3rd61439KHZNuecc476zd/8zSgE/Rvf+IZ61rOepd7//vdHZZr4OZ944gn1/e9/X33/+99XANQnP/lJ9f3vfz+KbCvzmc444wx10kknqT179qjvfOc76thjjx2qEPS8z3j48GH12te+Vh1zzDHqnnvuSdyPFhcXozr69RkbKVJKKfWZz3xGbdy4UU1OTqpTTz1Vfe973xv0JXUFAOdyzTXXRGV+/etfq3e+853qmc98pnrGM56h/uiP/kj94he/GNxF14AtUqPyGb/5zW+q5z3veWpqakodd9xx6vOf/3xiv+/76pJLLlEzMzNqampKveIVr1D333//gK62MxYWFtR73vMetXHjRrVixQr1W7/1W+oDH/hA4kbWxM956623On+L55xzjlKq3Gf63//9X/WmN71JHXnkkWrVqlXqLW95i3riiScG8Gnc5H3Gffv2Zd6Pbr311qiOfn1GTtVBCCFkaGlcnxQhhJDxgSJFCCFkaKFIEUIIGVooUoQQQoYWihQhhJChhSJFCCFkaKFIEUIIGVooUoQQQoYWihQhhJChhSJFCCFkaKFIEUIIGVooUoQQQoaW/w9d5s1ckNK6TAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(revert_normalisation(test_img))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "2poy64vsRkZI",
        "outputId": "d38db0f5-7a32-49a7-ebbf-00bafc166908"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1d2afd6e410>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(tensor2im(test_img.unsqueeze(0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "V9acyER1Rp9Q",
        "outputId": "d3a293f1-9741-4b4a-9924-d903459931a2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1d2b1f502b0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "A5A005Y9zTLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "epochs = 200\n",
        "lambda_weight = 10\n",
        "lambda_idt = 0.5\n",
        "blocks = 6\n",
        "\n",
        "checkpoint_instance_dir = \"1679338115.1399138\"\n",
        "checkpoint_epoch_dir = \"100\""
      ],
      "metadata": {
        "id": "aWkX11dPzVQy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=1\n",
        ")"
      ],
      "metadata": {
        "id": "hJAOKwNWSd0q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if checkpoint_instance_dir is not None and checkpoint_epoch_dir is not None:\n",
        "    cyclegan = CycleGAN.load(f\"./runs/{checkpoint_instance_dir}\", f\"{checkpoint_epoch_dir}\", device, blocks)\n",
        "else:\n",
        "    cyclegan = CycleGAN(device, blocks)"
      ],
      "metadata": {
        "id": "iMHhDLJEzWiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c104b5a3-07ab-46cf-9ce6-a4c50ad0d1af"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models and buffers loaded from ./runs/1679338115.1399138/100/checkpoint.tar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{cyclegan.save_folder}/info_{checkpoint_epoch_dir}.json\", \"w+\") as fp:\n",
        "    json.dump({\n",
        "        \"block_count\": cyclegan.block_count,\n",
        "        \"data_folders\": {\n",
        "            \"train_X\": train_X_loc,\n",
        "            \"test_X\": test_X_loc,\n",
        "            \"train_Y\": train_Y_loc,\n",
        "            \"test_Y\": test_Y_loc\n",
        "        },\n",
        "        \"batch_size\": batch_size,\n",
        "        \"max_epochs\": epochs,\n",
        "        \"start_epoch\": cyclegan.start_epoch,\n",
        "        \"lambda_weight\": lambda_weight,\n",
        "        \"lambda_idt\": lambda_idt,\n",
        "        \"checkpoint\": {\n",
        "            \"instance\": checkpoint_instance_dir,\n",
        "            \"epoch\": checkpoint_epoch_dir\n",
        "        }\n",
        "    }, fp, indent=2)"
      ],
      "metadata": {
        "id": "101i3qKVUxd5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U29wuJyF4sIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if cyclegan.start_epoch != 0:\n",
        "    print(f\"Resuming training from epoch {cyclegan.start_epoch + 1}\")\n",
        "else:\n",
        "    print(\"Starting training from scratch\")\n",
        "\n",
        "for epoch in range(cyclegan.start_epoch + 1, epochs + 1):\n",
        "    cyclegan.G.train()\n",
        "    cyclegan.D_X.train()\n",
        "    cyclegan.F.train()\n",
        "    cyclegan.D_Y.train()\n",
        "\n",
        "    batch_start_time = time.time()\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    cum_D_X_loss = 0\n",
        "    cum_G_fool_loss = 0\n",
        "    cum_G_cycle_loss = 0\n",
        "\n",
        "    cum_D_Y_loss = 0\n",
        "    cum_F_fool_loss = 0\n",
        "    cum_F_cycle_loss = 0\n",
        "\n",
        "    ep_D_X_loss = 0\n",
        "    ep_G_fool_loss = 0\n",
        "    ep_G_cycle_loss = 0\n",
        "\n",
        "    ep_D_Y_loss = 0\n",
        "    ep_F_fool_loss = 0\n",
        "    ep_F_cycle_loss = 0\n",
        "\n",
        "    for batch_no, (org_x, org_y) in enumerate(dataloader):\n",
        "        # model.set_inputs\n",
        "        org_x = org_x.to(device)\n",
        "        org_y = org_y.to(device)\n",
        "\n",
        "        # model.forward\n",
        "        fake_y = cyclegan.G(org_x)\n",
        "        cycled_x = cyclegan.F(fake_y)\n",
        "\n",
        "        fake_x = cyclegan.F(org_y)\n",
        "        cycled_y = cyclegan.G(fake_x)\n",
        "\n",
        "        # set_requires_grad netD_A, netD_B, False\n",
        "        for param in cyclegan.D_X.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in cyclegan.D_Y.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # optG zero grad\n",
        "        cyclegan.G_opt.zero_grad()\n",
        "        cyclegan.F_opt.zero_grad()\n",
        "\n",
        "        # model.backward_G\n",
        "        # COPIED \n",
        "        idt_X = cyclegan.G(org_y)\n",
        "        loss_idt_X = cyclegan.identity_loss(idt_X, org_y)\n",
        "\n",
        "        idt_Y = cyclegan.F(org_x)\n",
        "        loss_idt_Y = cyclegan.identity_loss(idt_Y, org_x)\n",
        "        # END COPIED\n",
        "\n",
        "        G_fool = cyclegan.D_X(fake_y) # should this be fake_x? \n",
        "        G_fool_loss = cyclegan.gan_X_loss(G_fool, torch.ones_like(G_fool))\n",
        "        F_fool = cyclegan.D_Y(fake_x)\n",
        "        F_fool_loss = cyclegan.gan_Y_loss(F_fool, torch.ones_like(F_fool))\n",
        "        \n",
        "        # Do the X -> Y -> X images look like they belong in X (and Y etc)\n",
        "        G_cycle_loss = cyclegan.cycle_X_loss(cycled_x, org_x)\n",
        "        F_cycle_loss = cyclegan.cycle_Y_loss(cycled_y, org_y)\n",
        "\n",
        "        G_loss = G_fool_loss + lambda_weight * G_cycle_loss + lambda_weight * lambda_idt * loss_idt_X\n",
        "        F_loss = F_fool_loss + lambda_weight * F_cycle_loss + lambda_weight * lambda_idt * loss_idt_Y\n",
        "\n",
        "        G_loss.backward()\n",
        "        F_loss.backward()\n",
        "\n",
        "        # optG step\n",
        "        cyclegan.G_opt.step()\n",
        "        cyclegan.F_opt.step()\n",
        "\n",
        "        # set_requires_grad D_X, netD_B, True\n",
        "        for param in cyclegan.D_X.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        for param in cyclegan.D_Y.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # optD zero grad\n",
        "        cyclegan.D_X_opt.zero_grad()\n",
        "        cyclegan.D_Y_opt.zero_grad()\n",
        "\n",
        "        real_pred_Y = cyclegan.D_X(org_y)\n",
        "        D_Y_real_loss = cyclegan.gan_Y_loss(real_pred_Y, torch.ones_like(real_pred_Y))\n",
        "        # Use the history of fakes\n",
        "        rand_fake_Y = cyclegan.fake_Y_buffer.randomise_existing_batch(fake_y) # .to(device)\n",
        "        fake_pred_Y = cyclegan.D_X(rand_fake_Y.detach())\n",
        "        D_Y_fake_loss = cyclegan.gan_Y_loss(fake_pred_Y, torch.zeros_like(fake_pred_Y))\n",
        "        D_Y_loss = (D_Y_real_loss + D_Y_fake_loss) * 0.5\n",
        "\n",
        "        D_Y_loss.backward()\n",
        "\n",
        "        real_pred_X = cyclegan.D_Y(org_x)\n",
        "        D_X_real_loss = cyclegan.gan_X_loss(real_pred_X, torch.ones_like(real_pred_X))\n",
        "        # Use the history of fakes\n",
        "        rand_fake_X = cyclegan.fake_X_buffer.randomise_existing_batch(fake_x) # .to(device)\n",
        "        fake_pred_X = cyclegan.D_Y(rand_fake_X.detach())\n",
        "        D_X_fake_loss = cyclegan.gan_X_loss(fake_pred_X, torch.zeros_like(fake_pred_X))\n",
        "        D_X_loss = (D_X_real_loss + D_X_fake_loss) * 0.5\n",
        "\n",
        "        D_X_loss.backward()\n",
        "\n",
        "        # optD step\n",
        "        cyclegan.D_X_opt.step()\n",
        "        cyclegan.D_Y_opt.step()\n",
        "\n",
        "        # Update buffers\n",
        "        cyclegan.fake_X_buffer.add(fake_x)\n",
        "        cyclegan.fake_Y_buffer.add(fake_y)\n",
        "\n",
        "        # Accumulate loss\n",
        "        cum_D_X_loss += D_X_loss.detach()\n",
        "        ep_D_X_loss += D_X_loss.detach()\n",
        "        cum_G_fool_loss += G_fool_loss.detach()\n",
        "        ep_G_fool_loss += G_fool_loss.detach()\n",
        "        cum_G_cycle_loss += G_cycle_loss.detach()\n",
        "        ep_G_cycle_loss += G_cycle_loss.detach()\n",
        "\n",
        "        cum_D_Y_loss += D_Y_loss.detach()\n",
        "        ep_D_Y_loss += D_Y_loss.detach()\n",
        "        cum_F_fool_loss += F_fool_loss.detach()\n",
        "        ep_F_fool_loss += F_fool_loss.detach()\n",
        "        cum_F_cycle_loss += F_cycle_loss.detach()\n",
        "        ep_F_cycle_loss += F_cycle_loss.detach()\n",
        "\n",
        "        if epoch == 0 and batch_no < 55:\n",
        "            print(f\"[{epoch}:{batch_no}] fake_X_buffer: {len(cyclegan.fake_X_buffer)}, fake_Y_buffer: {len(cyclegan.fake_Y_buffer)}\")\n",
        "        \n",
        "        if batch_no % 100 == 0 and batch_no != 0: \n",
        "            duration = time.time() - batch_start_time\n",
        "\n",
        "            print(f\"[{epoch}:{batch_no}] Took {duration:.2f}s\")\n",
        "            print(f\"[{epoch}:{batch_no}] cum_D_X_loss: {cum_D_X_loss / 100:.3f}, cum_G_fool_loss: {cum_G_fool_loss / 100:.3f}, cum_G_cycle_loss: {cum_G_cycle_loss / 100:.3f}\")\n",
        "            print(f\"[{epoch}:{batch_no}] cum_D_Y_loss: {cum_D_Y_loss / 100:.3f}, cum_F_fool_loss: {cum_F_fool_loss / 100:.3f}, cum_F_cycle_loss: {cum_F_cycle_loss / 100:.3f}\")\n",
        "            print(f\"[{epoch}:{batch_no}] fake_X_buffer: {len(cyclegan.fake_X_buffer)}, fake_Y_buffer: {len(cyclegan.fake_Y_buffer)}\")\n",
        "\n",
        "            cum_D_X_loss = 0\n",
        "            cum_G_fool_loss = 0\n",
        "            cum_G_cycle_loss = 0\n",
        "\n",
        "            cum_D_Y_loss = 0\n",
        "            cum_F_fool_loss = 0\n",
        "            cum_F_cycle_loss = 0\n",
        "\n",
        "            batch_start_time = time.time()\n",
        "    \n",
        "    print(f\"[{epoch}:END] Completed epoch in {time.time() - epoch_start_time}s\")\n",
        "    print(f\"[{epoch}:{batch_no}] ep_D_X_loss: {ep_D_X_loss / len(dataloader):.3f}, ep_G_fool_loss: {ep_G_fool_loss / len(dataloader):.3f}, ep_G_cycle_loss: {ep_G_cycle_loss / len(dataloader):.3f}\")\n",
        "    print(f\"[{epoch}:{batch_no}] ep_D_Y_loss: {ep_D_Y_loss / len(dataloader):.3f}, ep_F_fool_loss: {ep_F_fool_loss / len(dataloader):.3f}, ep_F_cycle_loss: {ep_F_cycle_loss / len(dataloader):.3f}\")\n",
        "\n",
        "    cyclegan.G.eval()\n",
        "    cyclegan.F.eval()\n",
        "\n",
        "    if vis is not None:\n",
        "        eval_start_time = time.time()\n",
        "\n",
        "        G_eval_forward = cyclegan.apply(test_xs, x_to_y=True)\n",
        "        F_eval_forward = cyclegan.apply(test_ys, x_to_y=False)\n",
        "\n",
        "        G_rev = cyclegan.apply(G_eval_forward, x_to_y=False)\n",
        "        F_rev = cyclegan.apply(F_eval_forward, x_to_y=True)\n",
        "\n",
        "        G_eval_forward = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in G_eval_forward])\n",
        "        F_eval_forward = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in F_eval_forward])\n",
        "        G_rev = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in G_rev])\n",
        "        F_rev = torch.stack([revert_normalisation(x).permute(2, 0, 1) for x in F_rev])\n",
        "\n",
        "        G_eval_grid = torchvision.utils.make_grid(G_eval_forward, nrow=4)\n",
        "        F_eval_grid = torchvision.utils.make_grid(F_eval_forward, nrow=4)\n",
        "        G_rev_grid = torchvision.utils.make_grid(G_rev, nrow=4)\n",
        "        F_rev_grid = torchvision.utils.make_grid(F_rev, nrow=4)\n",
        "\n",
        "        vis.image(G_eval_grid, win=\"G_eval\", opts={\n",
        "            \"caption\": f\"X -> Y evaluation, epoch {epoch}\",\n",
        "            \"store_history\": True\n",
        "        })\n",
        "\n",
        "        vis.image(F_eval_grid, win=\"F_eval\", opts={\n",
        "            \"caption\": f\"Y -> X evaluation, epoch {epoch}\",\n",
        "            \"store_history\": True\n",
        "        })\n",
        "\n",
        "        vis.image(G_rev_grid, win=\"G_rev\", opts={\n",
        "            \"caption\": f\"X -> Y -> X evaluation, epoch {epoch}\",\n",
        "            \"store_history\": True\n",
        "        })\n",
        "\n",
        "        vis.image(F_rev_grid, win=\"F_rev\", opts={\n",
        "            \"caption\": f\"Y -> X -> Y evaluation, epoch {epoch}\",\n",
        "            \"store_history\": True\n",
        "        })\n",
        "\n",
        "        print(f\"[{epoch}:END] Completed eval in {time.time() - eval_start_time}s\")\n",
        "\n",
        "    cyclegan.G.train()\n",
        "    cyclegan.F.train()\n",
        "\n",
        "    cyclegan.step_learning_rates()\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"[{epoch}:END] Saving models and training information\")\n",
        "        cyclegan.save(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8dbwRZ-25NL",
        "outputId": "8e8cc0f0-8abc-4113-ffbe-d53f9c7ece80"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[105:400] Took 11.63s\n",
            "[105:400] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.721, cum_G_cycle_loss: 0.089\n",
            "[105:400] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.706, cum_F_cycle_loss: 0.075\n",
            "[105:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[105:500] Took 11.64s\n",
            "[105:500] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.704, cum_G_cycle_loss: 0.089\n",
            "[105:500] cum_D_Y_loss: 0.102, cum_F_fool_loss: 0.628, cum_F_cycle_loss: 0.077\n",
            "[105:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[105:600] Took 11.64s\n",
            "[105:600] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.699, cum_G_cycle_loss: 0.089\n",
            "[105:600] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.711, cum_F_cycle_loss: 0.072\n",
            "[105:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[105:700] Took 11.64s\n",
            "[105:700] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.726, cum_G_cycle_loss: 0.092\n",
            "[105:700] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.074\n",
            "[105:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[105:800] Took 11.63s\n",
            "[105:800] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.693, cum_G_cycle_loss: 0.091\n",
            "[105:800] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.671, cum_F_cycle_loss: 0.072\n",
            "[105:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[105:900] Took 11.64s\n",
            "[105:900] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.635, cum_G_cycle_loss: 0.087\n",
            "[105:900] cum_D_Y_loss: 0.113, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.076\n",
            "[105:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[105:1000] Took 11.63s\n",
            "[105:1000] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.684, cum_G_cycle_loss: 0.092\n",
            "[105:1000] cum_D_Y_loss: 0.102, cum_F_fool_loss: 0.680, cum_F_cycle_loss: 0.070\n",
            "[105:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[105:1100] Took 11.64s\n",
            "[105:1100] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.688, cum_G_cycle_loss: 0.104\n",
            "[105:1100] cum_D_Y_loss: 0.104, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.076\n",
            "[105:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[105:END] Completed epoch in 141.0167953968048s\n",
            "[105:1187] ep_D_X_loss: 0.091, ep_G_fool_loss: 0.695, ep_G_cycle_loss: 0.093\n",
            "[105:1187] ep_D_Y_loss: 0.097, ep_F_fool_loss: 0.700, ep_F_cycle_loss: 0.076\n",
            "[105:END] Completed eval in 0.7529737949371338s\n",
            "Updated G_opt learning rate from 0.0001900990099009901 to 0.00018811881188118812\n",
            "Updated F_opt learning rate from 0.0001900990099009901 to 0.00018811881188118812\n",
            "Updated D_X_opt learning rate from 0.0001900990099009901 to 0.00018811881188118812\n",
            "Updated D_Y_opt learning rate from 0.0001900990099009901 to 0.00018811881188118812\n",
            "[105:END] Saving models and training information\n",
            "[106:100] Took 13.84s\n",
            "[106:100] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.663, cum_G_cycle_loss: 0.093\n",
            "[106:100] cum_D_Y_loss: 0.103, cum_F_fool_loss: 0.670, cum_F_cycle_loss: 0.075\n",
            "[106:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:200] Took 11.63s\n",
            "[106:200] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.651, cum_G_cycle_loss: 0.093\n",
            "[106:200] cum_D_Y_loss: 0.097, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.080\n",
            "[106:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:300] Took 11.64s\n",
            "[106:300] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.703, cum_G_cycle_loss: 0.085\n",
            "[106:300] cum_D_Y_loss: 0.093, cum_F_fool_loss: 0.615, cum_F_cycle_loss: 0.074\n",
            "[106:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:400] Took 11.63s\n",
            "[106:400] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.086\n",
            "[106:400] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.688, cum_F_cycle_loss: 0.071\n",
            "[106:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:500] Took 11.63s\n",
            "[106:500] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.704, cum_G_cycle_loss: 0.094\n",
            "[106:500] cum_D_Y_loss: 0.094, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.073\n",
            "[106:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:600] Took 11.63s\n",
            "[106:600] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.087\n",
            "[106:600] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.068\n",
            "[106:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:700] Took 11.64s\n",
            "[106:700] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.696, cum_G_cycle_loss: 0.099\n",
            "[106:700] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.729, cum_F_cycle_loss: 0.079\n",
            "[106:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:800] Took 11.63s\n",
            "[106:800] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.666, cum_G_cycle_loss: 0.094\n",
            "[106:800] cum_D_Y_loss: 0.091, cum_F_fool_loss: 0.660, cum_F_cycle_loss: 0.074\n",
            "[106:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:900] Took 11.64s\n",
            "[106:900] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.702, cum_G_cycle_loss: 0.089\n",
            "[106:900] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.077\n",
            "[106:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:1000] Took 11.63s\n",
            "[106:1000] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.098\n",
            "[106:1000] cum_D_Y_loss: 0.097, cum_F_fool_loss: 0.668, cum_F_cycle_loss: 0.071\n",
            "[106:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:1100] Took 11.64s\n",
            "[106:1100] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.706, cum_G_cycle_loss: 0.102\n",
            "[106:1100] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.078\n",
            "[106:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[106:END] Completed epoch in 141.05945658683777s\n",
            "[106:1187] ep_D_X_loss: 0.090, ep_G_fool_loss: 0.697, ep_G_cycle_loss: 0.092\n",
            "[106:1187] ep_D_Y_loss: 0.089, ep_F_fool_loss: 0.682, ep_F_cycle_loss: 0.075\n",
            "[106:END] Completed eval in 0.8028805255889893s\n",
            "Updated G_opt learning rate from 0.00018811881188118812 to 0.00018613861386138615\n",
            "Updated F_opt learning rate from 0.00018811881188118812 to 0.00018613861386138615\n",
            "Updated D_X_opt learning rate from 0.00018811881188118812 to 0.00018613861386138615\n",
            "Updated D_Y_opt learning rate from 0.00018811881188118812 to 0.00018613861386138615\n",
            "[107:100] Took 13.73s\n",
            "[107:100] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.094\n",
            "[107:100] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.075\n",
            "[107:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:200] Took 11.64s\n",
            "[107:200] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.707, cum_G_cycle_loss: 0.093\n",
            "[107:200] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.075\n",
            "[107:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:300] Took 11.63s\n",
            "[107:300] cum_D_X_loss: 0.104, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.090\n",
            "[107:300] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.644, cum_F_cycle_loss: 0.080\n",
            "[107:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:400] Took 11.63s\n",
            "[107:400] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.678, cum_G_cycle_loss: 0.090\n",
            "[107:400] cum_D_Y_loss: 0.106, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.077\n",
            "[107:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:500] Took 11.63s\n",
            "[107:500] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.654, cum_G_cycle_loss: 0.094\n",
            "[107:500] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.672, cum_F_cycle_loss: 0.072\n",
            "[107:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:600] Took 11.64s\n",
            "[107:600] cum_D_X_loss: 0.104, cum_G_fool_loss: 0.686, cum_G_cycle_loss: 0.098\n",
            "[107:600] cum_D_Y_loss: 0.099, cum_F_fool_loss: 0.613, cum_F_cycle_loss: 0.073\n",
            "[107:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:700] Took 11.63s\n",
            "[107:700] cum_D_X_loss: 0.104, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.093\n",
            "[107:700] cum_D_Y_loss: 0.102, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.073\n",
            "[107:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:800] Took 11.63s\n",
            "[107:800] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.711, cum_G_cycle_loss: 0.093\n",
            "[107:800] cum_D_Y_loss: 0.094, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.073\n",
            "[107:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:900] Took 11.63s\n",
            "[107:900] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.684, cum_G_cycle_loss: 0.098\n",
            "[107:900] cum_D_Y_loss: 0.102, cum_F_fool_loss: 0.669, cum_F_cycle_loss: 0.070\n",
            "[107:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:1000] Took 11.63s\n",
            "[107:1000] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.640, cum_G_cycle_loss: 0.089\n",
            "[107:1000] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.072\n",
            "[107:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:1100] Took 11.64s\n",
            "[107:1100] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.639, cum_G_cycle_loss: 0.091\n",
            "[107:1100] cum_D_Y_loss: 0.100, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.078\n",
            "[107:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[107:END] Completed epoch in 140.95837807655334s\n",
            "[107:1187] ep_D_X_loss: 0.094, ep_G_fool_loss: 0.698, ep_G_cycle_loss: 0.093\n",
            "[107:1187] ep_D_Y_loss: 0.094, ep_F_fool_loss: 0.679, ep_F_cycle_loss: 0.075\n",
            "[107:END] Completed eval in 0.8048462867736816s\n",
            "Updated G_opt learning rate from 0.00018613861386138615 to 0.00018415841584158417\n",
            "Updated F_opt learning rate from 0.00018613861386138615 to 0.00018415841584158417\n",
            "Updated D_X_opt learning rate from 0.00018613861386138615 to 0.00018415841584158417\n",
            "Updated D_Y_opt learning rate from 0.00018613861386138615 to 0.00018415841584158417\n",
            "[108:100] Took 13.85s\n",
            "[108:100] cum_D_X_loss: 0.103, cum_G_fool_loss: 0.666, cum_G_cycle_loss: 0.091\n",
            "[108:100] cum_D_Y_loss: 0.100, cum_F_fool_loss: 0.652, cum_F_cycle_loss: 0.077\n",
            "[108:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:200] Took 11.63s\n",
            "[108:200] cum_D_X_loss: 0.120, cum_G_fool_loss: 0.649, cum_G_cycle_loss: 0.096\n",
            "[108:200] cum_D_Y_loss: 0.104, cum_F_fool_loss: 0.655, cum_F_cycle_loss: 0.077\n",
            "[108:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:300] Took 11.64s\n",
            "[108:300] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.720, cum_G_cycle_loss: 0.085\n",
            "[108:300] cum_D_Y_loss: 0.101, cum_F_fool_loss: 0.706, cum_F_cycle_loss: 0.076\n",
            "[108:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:400] Took 11.63s\n",
            "[108:400] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.698, cum_G_cycle_loss: 0.094\n",
            "[108:400] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.682, cum_F_cycle_loss: 0.070\n",
            "[108:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:500] Took 11.63s\n",
            "[108:500] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.093\n",
            "[108:500] cum_D_Y_loss: 0.099, cum_F_fool_loss: 0.655, cum_F_cycle_loss: 0.079\n",
            "[108:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:600] Took 11.63s\n",
            "[108:600] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.084\n",
            "[108:600] cum_D_Y_loss: 0.097, cum_F_fool_loss: 0.666, cum_F_cycle_loss: 0.067\n",
            "[108:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:700] Took 11.63s\n",
            "[108:700] cum_D_X_loss: 0.109, cum_G_fool_loss: 0.715, cum_G_cycle_loss: 0.094\n",
            "[108:700] cum_D_Y_loss: 0.095, cum_F_fool_loss: 0.634, cum_F_cycle_loss: 0.078\n",
            "[108:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:800] Took 11.64s\n",
            "[108:800] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.708, cum_G_cycle_loss: 0.086\n",
            "[108:800] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.728, cum_F_cycle_loss: 0.075\n",
            "[108:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:900] Took 11.63s\n",
            "[108:900] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.095\n",
            "[108:900] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.731, cum_F_cycle_loss: 0.077\n",
            "[108:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:1000] Took 11.63s\n",
            "[108:1000] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.693, cum_G_cycle_loss: 0.091\n",
            "[108:1000] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.079\n",
            "[108:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:1100] Took 11.64s\n",
            "[108:1100] cum_D_X_loss: 0.117, cum_G_fool_loss: 0.649, cum_G_cycle_loss: 0.090\n",
            "[108:1100] cum_D_Y_loss: 0.107, cum_F_fool_loss: 0.668, cum_F_cycle_loss: 0.076\n",
            "[108:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[108:END] Completed epoch in 141.06271123886108s\n",
            "[108:1187] ep_D_X_loss: 0.096, ep_G_fool_loss: 0.697, ep_G_cycle_loss: 0.092\n",
            "[108:1187] ep_D_Y_loss: 0.093, ep_F_fool_loss: 0.679, ep_F_cycle_loss: 0.075\n",
            "[108:END] Completed eval in 0.8128530979156494s\n",
            "Updated G_opt learning rate from 0.00018415841584158417 to 0.0001821782178217822\n",
            "Updated F_opt learning rate from 0.00018415841584158417 to 0.0001821782178217822\n",
            "Updated D_X_opt learning rate from 0.00018415841584158417 to 0.0001821782178217822\n",
            "Updated D_Y_opt learning rate from 0.00018415841584158417 to 0.0001821782178217822\n",
            "[109:100] Took 13.80s\n",
            "[109:100] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.750, cum_G_cycle_loss: 0.097\n",
            "[109:100] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.079\n",
            "[109:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:200] Took 11.64s\n",
            "[109:200] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.089\n",
            "[109:200] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.666, cum_F_cycle_loss: 0.077\n",
            "[109:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:300] Took 11.63s\n",
            "[109:300] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.714, cum_G_cycle_loss: 0.092\n",
            "[109:300] cum_D_Y_loss: 0.096, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.078\n",
            "[109:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:400] Took 11.65s\n",
            "[109:400] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.695, cum_G_cycle_loss: 0.086\n",
            "[109:400] cum_D_Y_loss: 0.109, cum_F_fool_loss: 0.681, cum_F_cycle_loss: 0.075\n",
            "[109:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:500] Took 11.64s\n",
            "[109:500] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.689, cum_G_cycle_loss: 0.100\n",
            "[109:500] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.070\n",
            "[109:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:600] Took 11.63s\n",
            "[109:600] cum_D_X_loss: 0.104, cum_G_fool_loss: 0.695, cum_G_cycle_loss: 0.092\n",
            "[109:600] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.071\n",
            "[109:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:700] Took 11.63s\n",
            "[109:700] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.683, cum_G_cycle_loss: 0.097\n",
            "[109:700] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.077\n",
            "[109:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:800] Took 11.64s\n",
            "[109:800] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.716, cum_G_cycle_loss: 0.092\n",
            "[109:800] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.073\n",
            "[109:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:900] Took 11.64s\n",
            "[109:900] cum_D_X_loss: 0.112, cum_G_fool_loss: 0.691, cum_G_cycle_loss: 0.096\n",
            "[109:900] cum_D_Y_loss: 0.108, cum_F_fool_loss: 0.632, cum_F_cycle_loss: 0.083\n",
            "[109:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:1000] Took 11.63s\n",
            "[109:1000] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.715, cum_G_cycle_loss: 0.092\n",
            "[109:1000] cum_D_Y_loss: 0.094, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.075\n",
            "[109:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:1100] Took 11.64s\n",
            "[109:1100] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.661, cum_G_cycle_loss: 0.087\n",
            "[109:1100] cum_D_Y_loss: 0.107, cum_F_fool_loss: 0.659, cum_F_cycle_loss: 0.074\n",
            "[109:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[109:END] Completed epoch in 141.0491349697113s\n",
            "[109:1187] ep_D_X_loss: 0.098, ep_G_fool_loss: 0.701, ep_G_cycle_loss: 0.092\n",
            "[109:1187] ep_D_Y_loss: 0.092, ep_F_fool_loss: 0.686, ep_F_cycle_loss: 0.075\n",
            "[109:END] Completed eval in 0.808863639831543s\n",
            "Updated G_opt learning rate from 0.0001821782178217822 to 0.00018019801980198022\n",
            "Updated F_opt learning rate from 0.0001821782178217822 to 0.00018019801980198022\n",
            "Updated D_X_opt learning rate from 0.0001821782178217822 to 0.00018019801980198022\n",
            "Updated D_Y_opt learning rate from 0.0001821782178217822 to 0.00018019801980198022\n",
            "[110:100] Took 13.84s\n",
            "[110:100] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.675, cum_G_cycle_loss: 0.091\n",
            "[110:100] cum_D_Y_loss: 0.097, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.078\n",
            "[110:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:200] Took 11.63s\n",
            "[110:200] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.730, cum_G_cycle_loss: 0.100\n",
            "[110:200] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.071\n",
            "[110:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:300] Took 11.63s\n",
            "[110:300] cum_D_X_loss: 0.105, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.094\n",
            "[110:300] cum_D_Y_loss: 0.091, cum_F_fool_loss: 0.612, cum_F_cycle_loss: 0.080\n",
            "[110:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:400] Took 11.63s\n",
            "[110:400] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.751, cum_G_cycle_loss: 0.085\n",
            "[110:400] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.073\n",
            "[110:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:500] Took 11.63s\n",
            "[110:500] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.093\n",
            "[110:500] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.083\n",
            "[110:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:600] Took 11.63s\n",
            "[110:600] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.734, cum_G_cycle_loss: 0.082\n",
            "[110:600] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.072\n",
            "[110:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:700] Took 11.64s\n",
            "[110:700] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.694, cum_G_cycle_loss: 0.093\n",
            "[110:700] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.665, cum_F_cycle_loss: 0.077\n",
            "[110:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:800] Took 11.63s\n",
            "[110:800] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.091\n",
            "[110:800] cum_D_Y_loss: 0.097, cum_F_fool_loss: 0.663, cum_F_cycle_loss: 0.073\n",
            "[110:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:900] Took 11.63s\n",
            "[110:900] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.766, cum_G_cycle_loss: 0.092\n",
            "[110:900] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.077\n",
            "[110:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:1000] Took 11.63s\n",
            "[110:1000] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.682, cum_G_cycle_loss: 0.098\n",
            "[110:1000] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.076\n",
            "[110:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:1100] Took 11.63s\n",
            "[110:1100] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.730, cum_G_cycle_loss: 0.092\n",
            "[110:1100] cum_D_Y_loss: 0.099, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.082\n",
            "[110:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[110:END] Completed epoch in 141.04192399978638s\n",
            "[110:1187] ep_D_X_loss: 0.093, ep_G_fool_loss: 0.724, ep_G_cycle_loss: 0.092\n",
            "[110:1187] ep_D_Y_loss: 0.088, ep_F_fool_loss: 0.683, ep_F_cycle_loss: 0.076\n",
            "[110:END] Completed eval in 0.7978928089141846s\n",
            "Updated G_opt learning rate from 0.00018019801980198022 to 0.00017821782178217824\n",
            "Updated F_opt learning rate from 0.00018019801980198022 to 0.00017821782178217824\n",
            "Updated D_X_opt learning rate from 0.00018019801980198022 to 0.00017821782178217824\n",
            "Updated D_Y_opt learning rate from 0.00018019801980198022 to 0.00017821782178217824\n",
            "[110:END] Saving models and training information\n",
            "[111:100] Took 13.82s\n",
            "[111:100] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.103\n",
            "[111:100] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.082\n",
            "[111:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:200] Took 11.63s\n",
            "[111:200] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.740, cum_G_cycle_loss: 0.101\n",
            "[111:200] cum_D_Y_loss: 0.103, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.077\n",
            "[111:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:300] Took 11.63s\n",
            "[111:300] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.670, cum_G_cycle_loss: 0.088\n",
            "[111:300] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.072\n",
            "[111:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:400] Took 11.63s\n",
            "[111:400] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.694, cum_G_cycle_loss: 0.093\n",
            "[111:400] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.669, cum_F_cycle_loss: 0.077\n",
            "[111:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:500] Took 11.63s\n",
            "[111:500] cum_D_X_loss: 0.104, cum_G_fool_loss: 0.707, cum_G_cycle_loss: 0.084\n",
            "[111:500] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.664, cum_F_cycle_loss: 0.072\n",
            "[111:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:600] Took 11.63s\n",
            "[111:600] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.694, cum_G_cycle_loss: 0.082\n",
            "[111:600] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.073\n",
            "[111:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:700] Took 11.63s\n",
            "[111:700] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.717, cum_G_cycle_loss: 0.094\n",
            "[111:700] cum_D_Y_loss: 0.095, cum_F_fool_loss: 0.656, cum_F_cycle_loss: 0.074\n",
            "[111:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:800] Took 11.63s\n",
            "[111:800] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.684, cum_G_cycle_loss: 0.098\n",
            "[111:800] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.084\n",
            "[111:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:900] Took 11.64s\n",
            "[111:900] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.731, cum_G_cycle_loss: 0.093\n",
            "[111:900] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.080\n",
            "[111:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:1000] Took 11.64s\n",
            "[111:1000] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.691, cum_G_cycle_loss: 0.093\n",
            "[111:1000] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.653, cum_F_cycle_loss: 0.071\n",
            "[111:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:1100] Took 11.63s\n",
            "[111:1100] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.096\n",
            "[111:1100] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.072\n",
            "[111:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[111:END] Completed epoch in 141.0286967754364s\n",
            "[111:1187] ep_D_X_loss: 0.092, ep_G_fool_loss: 0.711, ep_G_cycle_loss: 0.093\n",
            "[111:1187] ep_D_Y_loss: 0.088, ep_F_fool_loss: 0.681, ep_F_cycle_loss: 0.076\n",
            "[111:END] Completed eval in 0.8058464527130127s\n",
            "Updated G_opt learning rate from 0.00017821782178217824 to 0.00017623762376237624\n",
            "Updated F_opt learning rate from 0.00017821782178217824 to 0.00017623762376237624\n",
            "Updated D_X_opt learning rate from 0.00017821782178217824 to 0.00017623762376237624\n",
            "Updated D_Y_opt learning rate from 0.00017821782178217824 to 0.00017623762376237624\n",
            "[112:100] Took 13.78s\n",
            "[112:100] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.761, cum_G_cycle_loss: 0.085\n",
            "[112:100] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.073\n",
            "[112:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:200] Took 11.64s\n",
            "[112:200] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.713, cum_G_cycle_loss: 0.089\n",
            "[112:200] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.660, cum_F_cycle_loss: 0.074\n",
            "[112:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:300] Took 11.63s\n",
            "[112:300] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.084\n",
            "[112:300] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.065\n",
            "[112:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:400] Took 11.64s\n",
            "[112:400] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.733, cum_G_cycle_loss: 0.088\n",
            "[112:400] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.683, cum_F_cycle_loss: 0.069\n",
            "[112:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:500] Took 11.64s\n",
            "[112:500] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.093\n",
            "[112:500] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.706, cum_F_cycle_loss: 0.071\n",
            "[112:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:600] Took 11.64s\n",
            "[112:600] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.091\n",
            "[112:600] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.595, cum_F_cycle_loss: 0.065\n",
            "[112:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:700] Took 11.64s\n",
            "[112:700] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.082\n",
            "[112:700] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.665, cum_F_cycle_loss: 0.079\n",
            "[112:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:800] Took 11.64s\n",
            "[112:800] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.731, cum_G_cycle_loss: 0.093\n",
            "[112:800] cum_D_Y_loss: 0.101, cum_F_fool_loss: 0.651, cum_F_cycle_loss: 0.070\n",
            "[112:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:900] Took 11.64s\n",
            "[112:900] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.713, cum_G_cycle_loss: 0.091\n",
            "[112:900] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.680, cum_F_cycle_loss: 0.072\n",
            "[112:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:1000] Took 11.64s\n",
            "[112:1000] cum_D_X_loss: 0.106, cum_G_fool_loss: 0.685, cum_G_cycle_loss: 0.085\n",
            "[112:1000] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.625, cum_F_cycle_loss: 0.070\n",
            "[112:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:1100] Took 11.63s\n",
            "[112:1100] cum_D_X_loss: 0.108, cum_G_fool_loss: 0.681, cum_G_cycle_loss: 0.089\n",
            "[112:1100] cum_D_Y_loss: 0.102, cum_F_fool_loss: 0.646, cum_F_cycle_loss: 0.071\n",
            "[112:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[112:END] Completed epoch in 141.00517463684082s\n",
            "[112:1187] ep_D_X_loss: 0.095, ep_G_fool_loss: 0.720, ep_G_cycle_loss: 0.088\n",
            "[112:1187] ep_D_Y_loss: 0.086, ep_F_fool_loss: 0.667, ep_F_cycle_loss: 0.071\n",
            "[112:END] Completed eval in 0.8497276306152344s\n",
            "Updated G_opt learning rate from 0.00017623762376237624 to 0.00017425742574257426\n",
            "Updated F_opt learning rate from 0.00017623762376237624 to 0.00017425742574257426\n",
            "Updated D_X_opt learning rate from 0.00017623762376237624 to 0.00017425742574257426\n",
            "Updated D_Y_opt learning rate from 0.00017623762376237624 to 0.00017425742574257426\n",
            "[113:100] Took 13.84s\n",
            "[113:100] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.673, cum_G_cycle_loss: 0.084\n",
            "[113:100] cum_D_Y_loss: 0.100, cum_F_fool_loss: 0.659, cum_F_cycle_loss: 0.076\n",
            "[113:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:200] Took 11.65s\n",
            "[113:200] cum_D_X_loss: 0.111, cum_G_fool_loss: 0.700, cum_G_cycle_loss: 0.089\n",
            "[113:200] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.607, cum_F_cycle_loss: 0.076\n",
            "[113:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:300] Took 11.64s\n",
            "[113:300] cum_D_X_loss: 0.105, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.087\n",
            "[113:300] cum_D_Y_loss: 0.102, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.073\n",
            "[113:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:400] Took 11.63s\n",
            "[113:400] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.704, cum_G_cycle_loss: 0.089\n",
            "[113:400] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.625, cum_F_cycle_loss: 0.071\n",
            "[113:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:500] Took 11.64s\n",
            "[113:500] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.653, cum_G_cycle_loss: 0.094\n",
            "[113:500] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.655, cum_F_cycle_loss: 0.076\n",
            "[113:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:600] Took 11.64s\n",
            "[113:600] cum_D_X_loss: 0.108, cum_G_fool_loss: 0.713, cum_G_cycle_loss: 0.093\n",
            "[113:600] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.590, cum_F_cycle_loss: 0.071\n",
            "[113:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:700] Took 11.64s\n",
            "[113:700] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.094\n",
            "[113:700] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.681, cum_F_cycle_loss: 0.084\n",
            "[113:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:800] Took 11.63s\n",
            "[113:800] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.087\n",
            "[113:800] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.654, cum_F_cycle_loss: 0.067\n",
            "[113:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:900] Took 11.64s\n",
            "[113:900] cum_D_X_loss: 0.103, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.088\n",
            "[113:900] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.071\n",
            "[113:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:1000] Took 11.64s\n",
            "[113:1000] cum_D_X_loss: 0.107, cum_G_fool_loss: 0.703, cum_G_cycle_loss: 0.089\n",
            "[113:1000] cum_D_Y_loss: 0.096, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.077\n",
            "[113:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:1100] Took 11.64s\n",
            "[113:1100] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.685, cum_G_cycle_loss: 0.085\n",
            "[113:1100] cum_D_Y_loss: 0.096, cum_F_fool_loss: 0.721, cum_F_cycle_loss: 0.077\n",
            "[113:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[113:END] Completed epoch in 141.08748841285706s\n",
            "[113:1187] ep_D_X_loss: 0.099, ep_G_fool_loss: 0.705, ep_G_cycle_loss: 0.089\n",
            "[113:1187] ep_D_Y_loss: 0.088, ep_F_fool_loss: 0.660, ep_F_cycle_loss: 0.074\n",
            "[113:END] Completed eval in 0.9115273952484131s\n",
            "Updated G_opt learning rate from 0.00017425742574257426 to 0.00017227722772277228\n",
            "Updated F_opt learning rate from 0.00017425742574257426 to 0.00017227722772277228\n",
            "Updated D_X_opt learning rate from 0.00017425742574257426 to 0.00017227722772277228\n",
            "Updated D_Y_opt learning rate from 0.00017425742574257426 to 0.00017227722772277228\n",
            "[114:100] Took 13.83s\n",
            "[114:100] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.090\n",
            "[114:100] cum_D_Y_loss: 0.102, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.079\n",
            "[114:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:200] Took 11.64s\n",
            "[114:200] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.090\n",
            "[114:200] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.078\n",
            "[114:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:300] Took 11.64s\n",
            "[114:300] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.668, cum_G_cycle_loss: 0.084\n",
            "[114:300] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.658, cum_F_cycle_loss: 0.073\n",
            "[114:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:400] Took 11.63s\n",
            "[114:400] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.702, cum_G_cycle_loss: 0.085\n",
            "[114:400] cum_D_Y_loss: 0.103, cum_F_fool_loss: 0.680, cum_F_cycle_loss: 0.076\n",
            "[114:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:500] Took 11.64s\n",
            "[114:500] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.663, cum_G_cycle_loss: 0.096\n",
            "[114:500] cum_D_Y_loss: 0.101, cum_F_fool_loss: 0.670, cum_F_cycle_loss: 0.073\n",
            "[114:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:600] Took 11.64s\n",
            "[114:600] cum_D_X_loss: 0.106, cum_G_fool_loss: 0.729, cum_G_cycle_loss: 0.090\n",
            "[114:600] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.680, cum_F_cycle_loss: 0.073\n",
            "[114:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:700] Took 11.64s\n",
            "[114:700] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.692, cum_G_cycle_loss: 0.088\n",
            "[114:700] cum_D_Y_loss: 0.093, cum_F_fool_loss: 0.738, cum_F_cycle_loss: 0.078\n",
            "[114:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:800] Took 11.63s\n",
            "[114:800] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.682, cum_G_cycle_loss: 0.086\n",
            "[114:800] cum_D_Y_loss: 0.100, cum_F_fool_loss: 0.663, cum_F_cycle_loss: 0.075\n",
            "[114:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:900] Took 11.64s\n",
            "[114:900] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.087\n",
            "[114:900] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.599, cum_F_cycle_loss: 0.068\n",
            "[114:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:1000] Took 11.64s\n",
            "[114:1000] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.780, cum_G_cycle_loss: 0.095\n",
            "[114:1000] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.672, cum_F_cycle_loss: 0.069\n",
            "[114:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:1100] Took 11.64s\n",
            "[114:1100] cum_D_X_loss: 0.102, cum_G_fool_loss: 0.672, cum_G_cycle_loss: 0.094\n",
            "[114:1100] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.073\n",
            "[114:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[114:END] Completed epoch in 141.0769009590149s\n",
            "[114:1187] ep_D_X_loss: 0.093, ep_G_fool_loss: 0.709, ep_G_cycle_loss: 0.090\n",
            "[114:1187] ep_D_Y_loss: 0.089, ep_F_fool_loss: 0.677, ep_F_cycle_loss: 0.074\n",
            "[114:END] Completed eval in 0.919539213180542s\n",
            "Updated G_opt learning rate from 0.00017227722772277228 to 0.0001702970297029703\n",
            "Updated F_opt learning rate from 0.00017227722772277228 to 0.0001702970297029703\n",
            "Updated D_X_opt learning rate from 0.00017227722772277228 to 0.0001702970297029703\n",
            "Updated D_Y_opt learning rate from 0.00017227722772277228 to 0.0001702970297029703\n",
            "[115:100] Took 13.83s\n",
            "[115:100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.711, cum_G_cycle_loss: 0.089\n",
            "[115:100] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.079\n",
            "[115:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:200] Took 11.63s\n",
            "[115:200] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.767, cum_G_cycle_loss: 0.091\n",
            "[115:200] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.075\n",
            "[115:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:300] Took 11.63s\n",
            "[115:300] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.091\n",
            "[115:300] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.726, cum_F_cycle_loss: 0.071\n",
            "[115:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:400] Took 11.63s\n",
            "[115:400] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.090\n",
            "[115:400] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.671, cum_F_cycle_loss: 0.076\n",
            "[115:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:500] Took 11.63s\n",
            "[115:500] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.684, cum_G_cycle_loss: 0.084\n",
            "[115:500] cum_D_Y_loss: 0.098, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.077\n",
            "[115:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:600] Took 11.63s\n",
            "[115:600] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.689, cum_G_cycle_loss: 0.086\n",
            "[115:600] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.072\n",
            "[115:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:700] Took 11.64s\n",
            "[115:700] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.097\n",
            "[115:700] cum_D_Y_loss: 0.098, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.073\n",
            "[115:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:800] Took 11.63s\n",
            "[115:800] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.729, cum_G_cycle_loss: 0.091\n",
            "[115:800] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.075\n",
            "[115:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:900] Took 11.64s\n",
            "[115:900] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.099\n",
            "[115:900] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.676, cum_F_cycle_loss: 0.073\n",
            "[115:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:1000] Took 11.63s\n",
            "[115:1000] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.785, cum_G_cycle_loss: 0.087\n",
            "[115:1000] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.731, cum_F_cycle_loss: 0.077\n",
            "[115:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:1100] Took 11.63s\n",
            "[115:1100] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.716, cum_G_cycle_loss: 0.096\n",
            "[115:1100] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.632, cum_F_cycle_loss: 0.081\n",
            "[115:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[115:END] Completed epoch in 141.06161665916443s\n",
            "[115:1187] ep_D_X_loss: 0.092, ep_G_fool_loss: 0.731, ep_G_cycle_loss: 0.091\n",
            "[115:1187] ep_D_Y_loss: 0.087, ep_F_fool_loss: 0.693, ep_F_cycle_loss: 0.075\n",
            "[115:END] Completed eval in 0.9524521827697754s\n",
            "Updated G_opt learning rate from 0.0001702970297029703 to 0.00016831683168316833\n",
            "Updated F_opt learning rate from 0.0001702970297029703 to 0.00016831683168316833\n",
            "Updated D_X_opt learning rate from 0.0001702970297029703 to 0.00016831683168316833\n",
            "Updated D_Y_opt learning rate from 0.0001702970297029703 to 0.00016831683168316833\n",
            "[115:END] Saving models and training information\n",
            "[116:100] Took 13.80s\n",
            "[116:100] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.689, cum_G_cycle_loss: 0.089\n",
            "[116:100] cum_D_Y_loss: 0.099, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.087\n",
            "[116:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:200] Took 11.64s\n",
            "[116:200] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.729, cum_G_cycle_loss: 0.084\n",
            "[116:200] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.075\n",
            "[116:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:300] Took 11.64s\n",
            "[116:300] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.641, cum_G_cycle_loss: 0.097\n",
            "[116:300] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.644, cum_F_cycle_loss: 0.073\n",
            "[116:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:400] Took 11.64s\n",
            "[116:400] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.089\n",
            "[116:400] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.650, cum_F_cycle_loss: 0.071\n",
            "[116:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:500] Took 11.63s\n",
            "[116:500] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.668, cum_G_cycle_loss: 0.094\n",
            "[116:500] cum_D_Y_loss: 0.101, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.068\n",
            "[116:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:600] Took 11.64s\n",
            "[116:600] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.731, cum_G_cycle_loss: 0.093\n",
            "[116:600] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.665, cum_F_cycle_loss: 0.072\n",
            "[116:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:700] Took 11.64s\n",
            "[116:700] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.087\n",
            "[116:700] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.075\n",
            "[116:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:800] Took 11.64s\n",
            "[116:800] cum_D_X_loss: 0.111, cum_G_fool_loss: 0.726, cum_G_cycle_loss: 0.092\n",
            "[116:800] cum_D_Y_loss: 0.093, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.074\n",
            "[116:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:900] Took 11.64s\n",
            "[116:900] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.083\n",
            "[116:900] cum_D_Y_loss: 0.093, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.069\n",
            "[116:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:1000] Took 11.64s\n",
            "[116:1000] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.087\n",
            "[116:1000] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.074\n",
            "[116:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:1100] Took 11.64s\n",
            "[116:1100] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.750, cum_G_cycle_loss: 0.092\n",
            "[116:1100] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.726, cum_F_cycle_loss: 0.070\n",
            "[116:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[116:END] Completed epoch in 141.04764533042908s\n",
            "[116:1187] ep_D_X_loss: 0.096, ep_G_fool_loss: 0.708, ep_G_cycle_loss: 0.090\n",
            "[116:1187] ep_D_Y_loss: 0.088, ep_F_fool_loss: 0.680, ep_F_cycle_loss: 0.074\n",
            "[116:END] Completed eval in 0.8965985774993896s\n",
            "Updated G_opt learning rate from 0.00016831683168316833 to 0.00016633663366336633\n",
            "Updated F_opt learning rate from 0.00016831683168316833 to 0.00016633663366336633\n",
            "Updated D_X_opt learning rate from 0.00016831683168316833 to 0.00016633663366336633\n",
            "Updated D_Y_opt learning rate from 0.00016831683168316833 to 0.00016633663366336633\n",
            "[117:100] Took 13.79s\n",
            "[117:100] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.704, cum_G_cycle_loss: 0.091\n",
            "[117:100] cum_D_Y_loss: 0.104, cum_F_fool_loss: 0.654, cum_F_cycle_loss: 0.072\n",
            "[117:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:200] Took 11.63s\n",
            "[117:200] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.710, cum_G_cycle_loss: 0.083\n",
            "[117:200] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.074\n",
            "[117:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:300] Took 11.62s\n",
            "[117:300] cum_D_X_loss: 0.108, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.095\n",
            "[117:300] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.079\n",
            "[117:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:400] Took 11.64s\n",
            "[117:400] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.715, cum_G_cycle_loss: 0.083\n",
            "[117:400] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.079\n",
            "[117:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:500] Took 11.63s\n",
            "[117:500] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.086\n",
            "[117:500] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.644, cum_F_cycle_loss: 0.068\n",
            "[117:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:600] Took 11.63s\n",
            "[117:600] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.706, cum_G_cycle_loss: 0.083\n",
            "[117:600] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.071\n",
            "[117:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:700] Took 11.63s\n",
            "[117:700] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.093\n",
            "[117:700] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.718, cum_F_cycle_loss: 0.077\n",
            "[117:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:800] Took 11.63s\n",
            "[117:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.084\n",
            "[117:800] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.641, cum_F_cycle_loss: 0.077\n",
            "[117:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:900] Took 11.64s\n",
            "[117:900] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.094\n",
            "[117:900] cum_D_Y_loss: 0.094, cum_F_fool_loss: 0.656, cum_F_cycle_loss: 0.075\n",
            "[117:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:1000] Took 11.63s\n",
            "[117:1000] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.746, cum_G_cycle_loss: 0.093\n",
            "[117:1000] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.661, cum_F_cycle_loss: 0.074\n",
            "[117:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:1100] Took 11.63s\n",
            "[117:1100] cum_D_X_loss: 0.102, cum_G_fool_loss: 0.685, cum_G_cycle_loss: 0.089\n",
            "[117:1100] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.673, cum_F_cycle_loss: 0.078\n",
            "[117:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[117:END] Completed epoch in 140.9858148097992s\n",
            "[117:1187] ep_D_X_loss: 0.091, ep_G_fool_loss: 0.717, ep_G_cycle_loss: 0.089\n",
            "[117:1187] ep_D_Y_loss: 0.088, ep_F_fool_loss: 0.675, ep_F_cycle_loss: 0.074\n",
            "[117:END] Completed eval in 0.8656847476959229s\n",
            "Updated G_opt learning rate from 0.00016633663366336633 to 0.00016435643564356438\n",
            "Updated F_opt learning rate from 0.00016633663366336633 to 0.00016435643564356438\n",
            "Updated D_X_opt learning rate from 0.00016633663366336633 to 0.00016435643564356438\n",
            "Updated D_Y_opt learning rate from 0.00016633663366336633 to 0.00016435643564356438\n",
            "[118:100] Took 13.86s\n",
            "[118:100] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.077\n",
            "[118:100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.069\n",
            "[118:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:200] Took 11.63s\n",
            "[118:200] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.687, cum_G_cycle_loss: 0.086\n",
            "[118:200] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.069\n",
            "[118:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:300] Took 11.63s\n",
            "[118:300] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.084\n",
            "[118:300] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.078\n",
            "[118:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:400] Took 11.63s\n",
            "[118:400] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.693, cum_G_cycle_loss: 0.085\n",
            "[118:400] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.072\n",
            "[118:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:500] Took 11.63s\n",
            "[118:500] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.095\n",
            "[118:500] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.086\n",
            "[118:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:600] Took 11.63s\n",
            "[118:600] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.658, cum_G_cycle_loss: 0.089\n",
            "[118:600] cum_D_Y_loss: 0.100, cum_F_fool_loss: 0.709, cum_F_cycle_loss: 0.072\n",
            "[118:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:700] Took 11.63s\n",
            "[118:700] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.716, cum_G_cycle_loss: 0.084\n",
            "[118:700] cum_D_Y_loss: 0.109, cum_F_fool_loss: 0.670, cum_F_cycle_loss: 0.073\n",
            "[118:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:800] Took 11.63s\n",
            "[118:800] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.658, cum_G_cycle_loss: 0.090\n",
            "[118:800] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.660, cum_F_cycle_loss: 0.079\n",
            "[118:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:900] Took 11.63s\n",
            "[118:900] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.080\n",
            "[118:900] cum_D_Y_loss: 0.096, cum_F_fool_loss: 0.645, cum_F_cycle_loss: 0.066\n",
            "[118:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:1000] Took 11.63s\n",
            "[118:1000] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.090\n",
            "[118:1000] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.641, cum_F_cycle_loss: 0.073\n",
            "[118:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:1100] Took 11.63s\n",
            "[118:1100] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.687, cum_G_cycle_loss: 0.096\n",
            "[118:1100] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.634, cum_F_cycle_loss: 0.070\n",
            "[118:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[118:END] Completed epoch in 141.06176567077637s\n",
            "[118:1187] ep_D_X_loss: 0.094, ep_G_fool_loss: 0.709, ep_G_cycle_loss: 0.087\n",
            "[118:1187] ep_D_Y_loss: 0.086, ep_F_fool_loss: 0.679, ep_F_cycle_loss: 0.073\n",
            "[118:END] Completed eval in 0.9454708099365234s\n",
            "Updated G_opt learning rate from 0.00016435643564356438 to 0.0001623762376237624\n",
            "Updated F_opt learning rate from 0.00016435643564356438 to 0.0001623762376237624\n",
            "Updated D_X_opt learning rate from 0.00016435643564356438 to 0.0001623762376237624\n",
            "Updated D_Y_opt learning rate from 0.00016435643564356438 to 0.0001623762376237624\n",
            "[119:100] Took 14.05s\n",
            "[119:100] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.732, cum_G_cycle_loss: 0.092\n",
            "[119:100] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.636, cum_F_cycle_loss: 0.070\n",
            "[119:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:200] Took 11.63s\n",
            "[119:200] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.711, cum_G_cycle_loss: 0.082\n",
            "[119:200] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.665, cum_F_cycle_loss: 0.077\n",
            "[119:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:300] Took 11.63s\n",
            "[119:300] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.703, cum_G_cycle_loss: 0.087\n",
            "[119:300] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.075\n",
            "[119:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:400] Took 11.63s\n",
            "[119:400] cum_D_X_loss: 0.107, cum_G_fool_loss: 0.766, cum_G_cycle_loss: 0.081\n",
            "[119:400] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.073\n",
            "[119:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:500] Took 11.63s\n",
            "[119:500] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.767, cum_G_cycle_loss: 0.087\n",
            "[119:500] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.676, cum_F_cycle_loss: 0.076\n",
            "[119:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:600] Took 11.63s\n",
            "[119:600] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.088\n",
            "[119:600] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.635, cum_F_cycle_loss: 0.068\n",
            "[119:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:700] Took 11.62s\n",
            "[119:700] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.083\n",
            "[119:700] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.065\n",
            "[119:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:800] Took 11.63s\n",
            "[119:800] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.718, cum_G_cycle_loss: 0.091\n",
            "[119:800] cum_D_Y_loss: 0.112, cum_F_fool_loss: 0.612, cum_F_cycle_loss: 0.077\n",
            "[119:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:900] Took 11.63s\n",
            "[119:900] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.671, cum_G_cycle_loss: 0.091\n",
            "[119:900] cum_D_Y_loss: 0.094, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.074\n",
            "[119:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:1000] Took 11.63s\n",
            "[119:1000] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.701, cum_G_cycle_loss: 0.085\n",
            "[119:1000] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.667, cum_F_cycle_loss: 0.073\n",
            "[119:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:1100] Took 11.63s\n",
            "[119:1100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.094\n",
            "[119:1100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.711, cum_F_cycle_loss: 0.067\n",
            "[119:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[119:END] Completed epoch in 141.2334430217743s\n",
            "[119:1187] ep_D_X_loss: 0.096, ep_G_fool_loss: 0.726, ep_G_cycle_loss: 0.088\n",
            "[119:1187] ep_D_Y_loss: 0.084, ep_F_fool_loss: 0.669, ep_F_cycle_loss: 0.073\n",
            "[119:END] Completed eval in 0.9165146350860596s\n",
            "Updated G_opt learning rate from 0.0001623762376237624 to 0.00016039603960396042\n",
            "Updated F_opt learning rate from 0.0001623762376237624 to 0.00016039603960396042\n",
            "Updated D_X_opt learning rate from 0.0001623762376237624 to 0.00016039603960396042\n",
            "Updated D_Y_opt learning rate from 0.0001623762376237624 to 0.00016039603960396042\n",
            "[120:100] Took 14.00s\n",
            "[120:100] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.089\n",
            "[120:100] cum_D_Y_loss: 0.094, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.080\n",
            "[120:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:200] Took 11.64s\n",
            "[120:200] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.718, cum_G_cycle_loss: 0.088\n",
            "[120:200] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.662, cum_F_cycle_loss: 0.072\n",
            "[120:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:300] Took 11.63s\n",
            "[120:300] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.750, cum_G_cycle_loss: 0.093\n",
            "[120:300] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.083\n",
            "[120:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:400] Took 11.63s\n",
            "[120:400] cum_D_X_loss: 0.120, cum_G_fool_loss: 0.718, cum_G_cycle_loss: 0.092\n",
            "[120:400] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.642, cum_F_cycle_loss: 0.077\n",
            "[120:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:500] Took 11.63s\n",
            "[120:500] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.696, cum_G_cycle_loss: 0.090\n",
            "[120:500] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.079\n",
            "[120:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:600] Took 11.63s\n",
            "[120:600] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.706, cum_G_cycle_loss: 0.085\n",
            "[120:600] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.637, cum_F_cycle_loss: 0.078\n",
            "[120:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:700] Took 11.64s\n",
            "[120:700] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.092\n",
            "[120:700] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.634, cum_F_cycle_loss: 0.075\n",
            "[120:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:800] Took 11.64s\n",
            "[120:800] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.674, cum_G_cycle_loss: 0.095\n",
            "[120:800] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.070\n",
            "[120:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:900] Took 11.64s\n",
            "[120:900] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.698, cum_G_cycle_loss: 0.084\n",
            "[120:900] cum_D_Y_loss: 0.097, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.073\n",
            "[120:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:1000] Took 11.63s\n",
            "[120:1000] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.714, cum_G_cycle_loss: 0.083\n",
            "[120:1000] cum_D_Y_loss: 0.102, cum_F_fool_loss: 0.632, cum_F_cycle_loss: 0.068\n",
            "[120:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:1100] Took 11.63s\n",
            "[120:1100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.673, cum_G_cycle_loss: 0.087\n",
            "[120:1100] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.728, cum_F_cycle_loss: 0.072\n",
            "[120:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[120:END] Completed epoch in 141.21913743019104s\n",
            "[120:1187] ep_D_X_loss: 0.095, ep_G_fool_loss: 0.710, ep_G_cycle_loss: 0.089\n",
            "[120:1187] ep_D_Y_loss: 0.086, ep_F_fool_loss: 0.674, ep_F_cycle_loss: 0.075\n",
            "[120:END] Completed eval in 0.9604315757751465s\n",
            "Updated G_opt learning rate from 0.00016039603960396042 to 0.00015841584158415842\n",
            "Updated F_opt learning rate from 0.00016039603960396042 to 0.00015841584158415842\n",
            "Updated D_X_opt learning rate from 0.00016039603960396042 to 0.00015841584158415842\n",
            "Updated D_Y_opt learning rate from 0.00016039603960396042 to 0.00015841584158415842\n",
            "[120:END] Saving models and training information\n",
            "[121:100] Took 13.79s\n",
            "[121:100] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.712, cum_G_cycle_loss: 0.085\n",
            "[121:100] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.735, cum_F_cycle_loss: 0.075\n",
            "[121:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:200] Took 11.63s\n",
            "[121:200] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.769, cum_G_cycle_loss: 0.096\n",
            "[121:200] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.682, cum_F_cycle_loss: 0.069\n",
            "[121:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:300] Took 11.63s\n",
            "[121:300] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.756, cum_G_cycle_loss: 0.082\n",
            "[121:300] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.066\n",
            "[121:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:400] Took 11.63s\n",
            "[121:400] cum_D_X_loss: 0.102, cum_G_fool_loss: 0.707, cum_G_cycle_loss: 0.087\n",
            "[121:400] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.665, cum_F_cycle_loss: 0.082\n",
            "[121:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:500] Took 11.64s\n",
            "[121:500] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.746, cum_G_cycle_loss: 0.084\n",
            "[121:500] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.074\n",
            "[121:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:600] Took 11.63s\n",
            "[121:600] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.710, cum_G_cycle_loss: 0.079\n",
            "[121:600] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.734, cum_F_cycle_loss: 0.073\n",
            "[121:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:700] Took 11.63s\n",
            "[121:700] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.697, cum_G_cycle_loss: 0.087\n",
            "[121:700] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.065\n",
            "[121:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:800] Took 11.63s\n",
            "[121:800] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.764, cum_G_cycle_loss: 0.081\n",
            "[121:800] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.659, cum_F_cycle_loss: 0.070\n",
            "[121:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:900] Took 11.63s\n",
            "[121:900] cum_D_X_loss: 0.108, cum_G_fool_loss: 0.693, cum_G_cycle_loss: 0.087\n",
            "[121:900] cum_D_Y_loss: 0.099, cum_F_fool_loss: 0.654, cum_F_cycle_loss: 0.073\n",
            "[121:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:1000] Took 11.63s\n",
            "[121:1000] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.087\n",
            "[121:1000] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.672, cum_F_cycle_loss: 0.070\n",
            "[121:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:1100] Took 11.63s\n",
            "[121:1100] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.707, cum_G_cycle_loss: 0.100\n",
            "[121:1100] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.074\n",
            "[121:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[121:END] Completed epoch in 140.9899344444275s\n",
            "[121:1187] ep_D_X_loss: 0.091, ep_G_fool_loss: 0.727, ep_G_cycle_loss: 0.087\n",
            "[121:1187] ep_D_Y_loss: 0.082, ep_F_fool_loss: 0.688, ep_F_cycle_loss: 0.072\n",
            "[121:END] Completed eval in 1.0262739658355713s\n",
            "Updated G_opt learning rate from 0.00015841584158415842 to 0.00015643564356435644\n",
            "Updated F_opt learning rate from 0.00015841584158415842 to 0.00015643564356435644\n",
            "Updated D_X_opt learning rate from 0.00015841584158415842 to 0.00015643564356435644\n",
            "Updated D_Y_opt learning rate from 0.00015841584158415842 to 0.00015643564356435644\n",
            "[122:100] Took 13.78s\n",
            "[122:100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.700, cum_G_cycle_loss: 0.083\n",
            "[122:100] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.708, cum_F_cycle_loss: 0.070\n",
            "[122:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:200] Took 11.63s\n",
            "[122:200] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.087\n",
            "[122:200] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.074\n",
            "[122:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:300] Took 11.64s\n",
            "[122:300] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.089\n",
            "[122:300] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.071\n",
            "[122:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:400] Took 11.63s\n",
            "[122:400] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.696, cum_G_cycle_loss: 0.092\n",
            "[122:400] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.657, cum_F_cycle_loss: 0.071\n",
            "[122:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:500] Took 11.63s\n",
            "[122:500] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.089\n",
            "[122:500] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.661, cum_F_cycle_loss: 0.074\n",
            "[122:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:600] Took 11.63s\n",
            "[122:600] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.729, cum_G_cycle_loss: 0.087\n",
            "[122:600] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.740, cum_F_cycle_loss: 0.069\n",
            "[122:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:700] Took 11.63s\n",
            "[122:700] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.085\n",
            "[122:700] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.765, cum_F_cycle_loss: 0.068\n",
            "[122:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:800] Took 11.63s\n",
            "[122:800] cum_D_X_loss: 0.103, cum_G_fool_loss: 0.704, cum_G_cycle_loss: 0.085\n",
            "[122:800] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.071\n",
            "[122:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:900] Took 11.63s\n",
            "[122:900] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.763, cum_G_cycle_loss: 0.089\n",
            "[122:900] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.608, cum_F_cycle_loss: 0.073\n",
            "[122:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:1000] Took 11.63s\n",
            "[122:1000] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.677, cum_G_cycle_loss: 0.095\n",
            "[122:1000] cum_D_Y_loss: 0.091, cum_F_fool_loss: 0.738, cum_F_cycle_loss: 0.074\n",
            "[122:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:1100] Took 11.63s\n",
            "[122:1100] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.710, cum_G_cycle_loss: 0.085\n",
            "[122:1100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.071\n",
            "[122:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[122:END] Completed epoch in 140.96445679664612s\n",
            "[122:1187] ep_D_X_loss: 0.088, ep_G_fool_loss: 0.725, ep_G_cycle_loss: 0.087\n",
            "[122:1187] ep_D_Y_loss: 0.083, ep_F_fool_loss: 0.698, ep_F_cycle_loss: 0.071\n",
            "[122:END] Completed eval in 0.9215342998504639s\n",
            "Updated G_opt learning rate from 0.00015643564356435644 to 0.00015445544554455447\n",
            "Updated F_opt learning rate from 0.00015643564356435644 to 0.00015445544554455447\n",
            "Updated D_X_opt learning rate from 0.00015643564356435644 to 0.00015445544554455447\n",
            "Updated D_Y_opt learning rate from 0.00015643564356435644 to 0.00015445544554455447\n",
            "[123:100] Took 13.84s\n",
            "[123:100] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.085\n",
            "[123:100] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.655, cum_F_cycle_loss: 0.068\n",
            "[123:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:200] Took 11.63s\n",
            "[123:200] cum_D_X_loss: 0.104, cum_G_fool_loss: 0.776, cum_G_cycle_loss: 0.085\n",
            "[123:200] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.750, cum_F_cycle_loss: 0.068\n",
            "[123:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:300] Took 11.64s\n",
            "[123:300] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.079\n",
            "[123:300] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.660, cum_F_cycle_loss: 0.067\n",
            "[123:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:400] Took 11.63s\n",
            "[123:400] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.080\n",
            "[123:400] cum_D_Y_loss: 0.094, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.067\n",
            "[123:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:500] Took 11.63s\n",
            "[123:500] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.087\n",
            "[123:500] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.065\n",
            "[123:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:600] Took 11.63s\n",
            "[123:600] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.696, cum_G_cycle_loss: 0.089\n",
            "[123:600] cum_D_Y_loss: 0.102, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.079\n",
            "[123:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:700] Took 11.64s\n",
            "[123:700] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.712, cum_G_cycle_loss: 0.090\n",
            "[123:700] cum_D_Y_loss: 0.102, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.075\n",
            "[123:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:800] Took 11.63s\n",
            "[123:800] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.086\n",
            "[123:800] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.646, cum_F_cycle_loss: 0.071\n",
            "[123:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:900] Took 11.64s\n",
            "[123:900] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.687, cum_G_cycle_loss: 0.090\n",
            "[123:900] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.641, cum_F_cycle_loss: 0.075\n",
            "[123:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:1000] Took 11.63s\n",
            "[123:1000] cum_D_X_loss: 0.109, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.090\n",
            "[123:1000] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.658, cum_F_cycle_loss: 0.072\n",
            "[123:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:1100] Took 11.63s\n",
            "[123:1100] cum_D_X_loss: 0.113, cum_G_fool_loss: 0.702, cum_G_cycle_loss: 0.087\n",
            "[123:1100] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.610, cum_F_cycle_loss: 0.068\n",
            "[123:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[123:END] Completed epoch in 141.04622292518616s\n",
            "[123:1187] ep_D_X_loss: 0.094, ep_G_fool_loss: 0.725, ep_G_cycle_loss: 0.086\n",
            "[123:1187] ep_D_Y_loss: 0.084, ep_F_fool_loss: 0.677, ep_F_cycle_loss: 0.071\n",
            "[123:END] Completed eval in 0.954474687576294s\n",
            "Updated G_opt learning rate from 0.00015445544554455447 to 0.0001524752475247525\n",
            "Updated F_opt learning rate from 0.00015445544554455447 to 0.0001524752475247525\n",
            "Updated D_X_opt learning rate from 0.00015445544554455447 to 0.0001524752475247525\n",
            "Updated D_Y_opt learning rate from 0.00015445544554455447 to 0.0001524752475247525\n",
            "[124:100] Took 14.03s\n",
            "[124:100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.734, cum_G_cycle_loss: 0.092\n",
            "[124:100] cum_D_Y_loss: 0.091, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.078\n",
            "[124:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:200] Took 11.64s\n",
            "[124:200] cum_D_X_loss: 0.107, cum_G_fool_loss: 0.663, cum_G_cycle_loss: 0.085\n",
            "[124:200] cum_D_Y_loss: 0.099, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.082\n",
            "[124:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:300] Took 11.63s\n",
            "[124:300] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.704, cum_G_cycle_loss: 0.091\n",
            "[124:300] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.072\n",
            "[124:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:400] Took 11.64s\n",
            "[124:400] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.082\n",
            "[124:400] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.073\n",
            "[124:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:500] Took 11.63s\n",
            "[124:500] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.093\n",
            "[124:500] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.076\n",
            "[124:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:600] Took 11.64s\n",
            "[124:600] cum_D_X_loss: 0.103, cum_G_fool_loss: 0.732, cum_G_cycle_loss: 0.082\n",
            "[124:600] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.667, cum_F_cycle_loss: 0.064\n",
            "[124:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:700] Took 11.64s\n",
            "[124:700] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.097\n",
            "[124:700] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.664, cum_F_cycle_loss: 0.071\n",
            "[124:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:800] Took 11.64s\n",
            "[124:800] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.082\n",
            "[124:800] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.069\n",
            "[124:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:900] Took 11.63s\n",
            "[124:900] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.746, cum_G_cycle_loss: 0.081\n",
            "[124:900] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.658, cum_F_cycle_loss: 0.069\n",
            "[124:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:1000] Took 11.64s\n",
            "[124:1000] cum_D_X_loss: 0.106, cum_G_fool_loss: 0.713, cum_G_cycle_loss: 0.084\n",
            "[124:1000] cum_D_Y_loss: 0.093, cum_F_fool_loss: 0.620, cum_F_cycle_loss: 0.066\n",
            "[124:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:1100] Took 11.64s\n",
            "[124:1100] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.657, cum_G_cycle_loss: 0.079\n",
            "[124:1100] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.070\n",
            "[124:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[124:END] Completed epoch in 141.26745080947876s\n",
            "[124:1187] ep_D_X_loss: 0.094, ep_G_fool_loss: 0.716, ep_G_cycle_loss: 0.086\n",
            "[124:1187] ep_D_Y_loss: 0.084, ep_F_fool_loss: 0.684, ep_F_cycle_loss: 0.071\n",
            "[124:END] Completed eval in 0.9933552742004395s\n",
            "Updated G_opt learning rate from 0.0001524752475247525 to 0.00015049504950495051\n",
            "Updated F_opt learning rate from 0.0001524752475247525 to 0.00015049504950495051\n",
            "Updated D_X_opt learning rate from 0.0001524752475247525 to 0.00015049504950495051\n",
            "Updated D_Y_opt learning rate from 0.0001524752475247525 to 0.00015049504950495051\n",
            "[125:100] Took 13.94s\n",
            "[125:100] cum_D_X_loss: 0.104, cum_G_fool_loss: 0.686, cum_G_cycle_loss: 0.088\n",
            "[125:100] cum_D_Y_loss: 0.094, cum_F_fool_loss: 0.623, cum_F_cycle_loss: 0.069\n",
            "[125:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:200] Took 11.64s\n",
            "[125:200] cum_D_X_loss: 0.112, cum_G_fool_loss: 0.687, cum_G_cycle_loss: 0.088\n",
            "[125:200] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.660, cum_F_cycle_loss: 0.066\n",
            "[125:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:300] Took 11.79s\n",
            "[125:300] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.751, cum_G_cycle_loss: 0.085\n",
            "[125:300] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.070\n",
            "[125:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:400] Took 12.10s\n",
            "[125:400] cum_D_X_loss: 0.104, cum_G_fool_loss: 0.716, cum_G_cycle_loss: 0.088\n",
            "[125:400] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.069\n",
            "[125:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:500] Took 13.44s\n",
            "[125:500] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.090\n",
            "[125:500] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.623, cum_F_cycle_loss: 0.071\n",
            "[125:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:600] Took 12.42s\n",
            "[125:600] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.078\n",
            "[125:600] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.670, cum_F_cycle_loss: 0.074\n",
            "[125:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:700] Took 11.81s\n",
            "[125:700] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.749, cum_G_cycle_loss: 0.089\n",
            "[125:700] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.719, cum_F_cycle_loss: 0.071\n",
            "[125:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:800] Took 11.79s\n",
            "[125:800] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.086\n",
            "[125:800] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.667, cum_F_cycle_loss: 0.072\n",
            "[125:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:900] Took 11.97s\n",
            "[125:900] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.685, cum_G_cycle_loss: 0.092\n",
            "[125:900] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.680, cum_F_cycle_loss: 0.076\n",
            "[125:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:1000] Took 11.72s\n",
            "[125:1000] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.716, cum_G_cycle_loss: 0.074\n",
            "[125:1000] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.069\n",
            "[125:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:1100] Took 11.68s\n",
            "[125:1100] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.081\n",
            "[125:1100] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.067\n",
            "[125:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[125:END] Completed epoch in 145.2547013759613s\n",
            "[125:1187] ep_D_X_loss: 0.093, ep_G_fool_loss: 0.729, ep_G_cycle_loss: 0.084\n",
            "[125:1187] ep_D_Y_loss: 0.081, ep_F_fool_loss: 0.678, ep_F_cycle_loss: 0.071\n",
            "[125:END] Completed eval in 0.9704039096832275s\n",
            "Updated G_opt learning rate from 0.00015049504950495051 to 0.0001485148514851485\n",
            "Updated F_opt learning rate from 0.00015049504950495051 to 0.0001485148514851485\n",
            "Updated D_X_opt learning rate from 0.00015049504950495051 to 0.0001485148514851485\n",
            "Updated D_Y_opt learning rate from 0.00015049504950495051 to 0.0001485148514851485\n",
            "[125:END] Saving models and training information\n",
            "[126:100] Took 14.05s\n",
            "[126:100] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.781, cum_G_cycle_loss: 0.087\n",
            "[126:100] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.665, cum_F_cycle_loss: 0.073\n",
            "[126:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:200] Took 11.63s\n",
            "[126:200] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.086\n",
            "[126:200] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.669, cum_F_cycle_loss: 0.069\n",
            "[126:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:300] Took 11.75s\n",
            "[126:300] cum_D_X_loss: 0.103, cum_G_fool_loss: 0.688, cum_G_cycle_loss: 0.084\n",
            "[126:300] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.072\n",
            "[126:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:400] Took 12.14s\n",
            "[126:400] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.092\n",
            "[126:400] cum_D_Y_loss: 0.091, cum_F_fool_loss: 0.656, cum_F_cycle_loss: 0.075\n",
            "[126:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:500] Took 11.80s\n",
            "[126:500] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.701, cum_G_cycle_loss: 0.083\n",
            "[126:500] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.714, cum_F_cycle_loss: 0.068\n",
            "[126:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:600] Took 12.78s\n",
            "[126:600] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.749, cum_G_cycle_loss: 0.086\n",
            "[126:600] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.071\n",
            "[126:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:700] Took 12.24s\n",
            "[126:700] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.084\n",
            "[126:700] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.070\n",
            "[126:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:800] Took 11.90s\n",
            "[126:800] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.082\n",
            "[126:800] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.071\n",
            "[126:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:900] Took 11.82s\n",
            "[126:900] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.679, cum_G_cycle_loss: 0.078\n",
            "[126:900] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.711, cum_F_cycle_loss: 0.064\n",
            "[126:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:1000] Took 11.70s\n",
            "[126:1000] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.082\n",
            "[126:1000] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.682, cum_F_cycle_loss: 0.074\n",
            "[126:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:1100] Took 11.63s\n",
            "[126:1100] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.085\n",
            "[126:1100] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.666, cum_F_cycle_loss: 0.073\n",
            "[126:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[126:END] Completed epoch in 144.29532408714294s\n",
            "[126:1187] ep_D_X_loss: 0.093, ep_G_fool_loss: 0.734, ep_G_cycle_loss: 0.084\n",
            "[126:1187] ep_D_Y_loss: 0.081, ep_F_fool_loss: 0.686, ep_F_cycle_loss: 0.071\n",
            "[126:END] Completed eval in 0.9723982810974121s\n",
            "Updated G_opt learning rate from 0.0001485148514851485 to 0.00014653465346534653\n",
            "Updated F_opt learning rate from 0.0001485148514851485 to 0.00014653465346534653\n",
            "Updated D_X_opt learning rate from 0.0001485148514851485 to 0.00014653465346534653\n",
            "Updated D_Y_opt learning rate from 0.0001485148514851485 to 0.00014653465346534653\n",
            "[127:100] Took 13.84s\n",
            "[127:100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.713, cum_G_cycle_loss: 0.085\n",
            "[127:100] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.677, cum_F_cycle_loss: 0.067\n",
            "[127:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:200] Took 11.63s\n",
            "[127:200] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.089\n",
            "[127:200] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.763, cum_F_cycle_loss: 0.076\n",
            "[127:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:300] Took 11.63s\n",
            "[127:300] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.670, cum_G_cycle_loss: 0.079\n",
            "[127:300] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.652, cum_F_cycle_loss: 0.070\n",
            "[127:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:400] Took 11.63s\n",
            "[127:400] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.084\n",
            "[127:400] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.069\n",
            "[127:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:500] Took 12.24s\n",
            "[127:500] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.083\n",
            "[127:500] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.711, cum_F_cycle_loss: 0.069\n",
            "[127:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:600] Took 11.95s\n",
            "[127:600] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.079\n",
            "[127:600] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.068\n",
            "[127:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:700] Took 12.28s\n",
            "[127:700] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.081\n",
            "[127:700] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.070\n",
            "[127:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:800] Took 11.81s\n",
            "[127:800] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.088\n",
            "[127:800] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.673, cum_F_cycle_loss: 0.066\n",
            "[127:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:900] Took 11.73s\n",
            "[127:900] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.806, cum_G_cycle_loss: 0.084\n",
            "[127:900] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.067\n",
            "[127:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:1000] Took 11.74s\n",
            "[127:1000] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.082\n",
            "[127:1000] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.065\n",
            "[127:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:1100] Took 11.74s\n",
            "[127:1100] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.081\n",
            "[127:1100] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.069\n",
            "[127:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[127:END] Completed epoch in 143.48604536056519s\n",
            "[127:1187] ep_D_X_loss: 0.088, ep_G_fool_loss: 0.741, ep_G_cycle_loss: 0.084\n",
            "[127:1187] ep_D_Y_loss: 0.079, ep_F_fool_loss: 0.699, ep_F_cycle_loss: 0.069\n",
            "[127:END] Completed eval in 1.0850746631622314s\n",
            "Updated G_opt learning rate from 0.00014653465346534653 to 0.00014455445544554456\n",
            "Updated F_opt learning rate from 0.00014653465346534653 to 0.00014455445544554456\n",
            "Updated D_X_opt learning rate from 0.00014653465346534653 to 0.00014455445544554456\n",
            "Updated D_Y_opt learning rate from 0.00014653465346534653 to 0.00014455445544554456\n",
            "[128:100] Took 14.16s\n",
            "[128:100] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.751, cum_G_cycle_loss: 0.081\n",
            "[128:100] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.071\n",
            "[128:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:200] Took 12.17s\n",
            "[128:200] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.678, cum_G_cycle_loss: 0.089\n",
            "[128:200] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.070\n",
            "[128:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:300] Took 12.53s\n",
            "[128:300] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.710, cum_G_cycle_loss: 0.083\n",
            "[128:300] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.069\n",
            "[128:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:400] Took 12.93s\n",
            "[128:400] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.081\n",
            "[128:400] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.747, cum_F_cycle_loss: 0.069\n",
            "[128:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:500] Took 13.23s\n",
            "[128:500] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.712, cum_G_cycle_loss: 0.088\n",
            "[128:500] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.071\n",
            "[128:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:600] Took 13.28s\n",
            "[128:600] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.704, cum_G_cycle_loss: 0.079\n",
            "[128:600] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.686, cum_F_cycle_loss: 0.072\n",
            "[128:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:700] Took 12.46s\n",
            "[128:700] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.698, cum_G_cycle_loss: 0.086\n",
            "[128:700] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.706, cum_F_cycle_loss: 0.075\n",
            "[128:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:800] Took 12.65s\n",
            "[128:800] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.720, cum_G_cycle_loss: 0.087\n",
            "[128:800] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.668, cum_F_cycle_loss: 0.073\n",
            "[128:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:900] Took 12.56s\n",
            "[128:900] cum_D_X_loss: 0.103, cum_G_fool_loss: 0.792, cum_G_cycle_loss: 0.087\n",
            "[128:900] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.624, cum_F_cycle_loss: 0.070\n",
            "[128:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:1000] Took 12.36s\n",
            "[128:1000] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.078\n",
            "[128:1000] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.068\n",
            "[128:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:1100] Took 12.45s\n",
            "[128:1100] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.731, cum_G_cycle_loss: 0.088\n",
            "[128:1100] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.676, cum_F_cycle_loss: 0.064\n",
            "[128:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[128:END] Completed epoch in 152.5150306224823s\n",
            "[128:1187] ep_D_X_loss: 0.089, ep_G_fool_loss: 0.725, ep_G_cycle_loss: 0.084\n",
            "[128:1187] ep_D_Y_loss: 0.083, ep_F_fool_loss: 0.690, ep_F_cycle_loss: 0.070\n",
            "[128:END] Completed eval in 1.1087090969085693s\n",
            "Updated G_opt learning rate from 0.00014455445544554456 to 0.00014257425742574258\n",
            "Updated F_opt learning rate from 0.00014455445544554456 to 0.00014257425742574258\n",
            "Updated D_X_opt learning rate from 0.00014455445544554456 to 0.00014257425742574258\n",
            "Updated D_Y_opt learning rate from 0.00014455445544554456 to 0.00014257425742574258\n",
            "[129:100] Took 15.24s\n",
            "[129:100] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.085\n",
            "[129:100] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.072\n",
            "[129:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:200] Took 12.01s\n",
            "[129:200] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.084\n",
            "[129:200] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.671, cum_F_cycle_loss: 0.074\n",
            "[129:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:300] Took 11.83s\n",
            "[129:300] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.090\n",
            "[129:300] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.066\n",
            "[129:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:400] Took 12.15s\n",
            "[129:400] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.794, cum_G_cycle_loss: 0.090\n",
            "[129:400] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.681, cum_F_cycle_loss: 0.067\n",
            "[129:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:500] Took 12.13s\n",
            "[129:500] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.710, cum_G_cycle_loss: 0.084\n",
            "[129:500] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.673, cum_F_cycle_loss: 0.069\n",
            "[129:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:600] Took 12.06s\n",
            "[129:600] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.732, cum_G_cycle_loss: 0.086\n",
            "[129:600] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.069\n",
            "[129:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:700] Took 12.02s\n",
            "[129:700] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.078\n",
            "[129:700] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.074\n",
            "[129:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:800] Took 11.91s\n",
            "[129:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.676, cum_G_cycle_loss: 0.085\n",
            "[129:800] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.719, cum_F_cycle_loss: 0.071\n",
            "[129:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:900] Took 11.69s\n",
            "[129:900] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.704, cum_G_cycle_loss: 0.083\n",
            "[129:900] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.634, cum_F_cycle_loss: 0.072\n",
            "[129:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:1000] Took 11.68s\n",
            "[129:1000] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.718, cum_G_cycle_loss: 0.080\n",
            "[129:1000] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.718, cum_F_cycle_loss: 0.068\n",
            "[129:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:1100] Took 12.26s\n",
            "[129:1100] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.089\n",
            "[129:1100] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.066\n",
            "[129:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[129:END] Completed epoch in 146.5022430419922s\n",
            "[129:1187] ep_D_X_loss: 0.089, ep_G_fool_loss: 0.731, ep_G_cycle_loss: 0.085\n",
            "[129:1187] ep_D_Y_loss: 0.079, ep_F_fool_loss: 0.685, ep_F_cycle_loss: 0.070\n",
            "[129:END] Completed eval in 1.1040465831756592s\n",
            "Updated G_opt learning rate from 0.00014257425742574258 to 0.0001405940594059406\n",
            "Updated F_opt learning rate from 0.00014257425742574258 to 0.0001405940594059406\n",
            "Updated D_X_opt learning rate from 0.00014257425742574258 to 0.0001405940594059406\n",
            "Updated D_Y_opt learning rate from 0.00014257425742574258 to 0.0001405940594059406\n",
            "[130:100] Took 14.70s\n",
            "[130:100] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.078\n",
            "[130:100] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.670, cum_F_cycle_loss: 0.077\n",
            "[130:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:200] Took 12.39s\n",
            "[130:200] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.691, cum_G_cycle_loss: 0.083\n",
            "[130:200] cum_D_Y_loss: 0.098, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.078\n",
            "[130:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:300] Took 12.43s\n",
            "[130:300] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.082\n",
            "[130:300] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.068\n",
            "[130:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:400] Took 12.28s\n",
            "[130:400] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.706, cum_G_cycle_loss: 0.080\n",
            "[130:400] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.657, cum_F_cycle_loss: 0.069\n",
            "[130:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:500] Took 12.22s\n",
            "[130:500] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.684, cum_G_cycle_loss: 0.079\n",
            "[130:500] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.688, cum_F_cycle_loss: 0.069\n",
            "[130:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:600] Took 12.32s\n",
            "[130:600] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.665, cum_G_cycle_loss: 0.088\n",
            "[130:600] cum_D_Y_loss: 0.093, cum_F_fool_loss: 0.646, cum_F_cycle_loss: 0.071\n",
            "[130:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:700] Took 12.33s\n",
            "[130:700] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.726, cum_G_cycle_loss: 0.084\n",
            "[130:700] cum_D_Y_loss: 0.096, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.065\n",
            "[130:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:800] Took 12.54s\n",
            "[130:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.081\n",
            "[130:800] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.067\n",
            "[130:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:900] Took 12.42s\n",
            "[130:900] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.717, cum_G_cycle_loss: 0.075\n",
            "[130:900] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.065\n",
            "[130:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:1000] Took 12.30s\n",
            "[130:1000] cum_D_X_loss: 0.104, cum_G_fool_loss: 0.699, cum_G_cycle_loss: 0.088\n",
            "[130:1000] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.654, cum_F_cycle_loss: 0.072\n",
            "[130:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:1100] Took 12.15s\n",
            "[130:1100] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.080\n",
            "[130:1100] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.067\n",
            "[130:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[130:END] Completed epoch in 149.7060067653656s\n",
            "[130:1187] ep_D_X_loss: 0.088, ep_G_fool_loss: 0.713, ep_G_cycle_loss: 0.082\n",
            "[130:1187] ep_D_Y_loss: 0.087, ep_F_fool_loss: 0.685, ep_F_cycle_loss: 0.070\n",
            "[130:END] Completed eval in 1.0906362533569336s\n",
            "Updated G_opt learning rate from 0.0001405940594059406 to 0.0001386138613861386\n",
            "Updated F_opt learning rate from 0.0001405940594059406 to 0.0001386138613861386\n",
            "Updated D_X_opt learning rate from 0.0001405940594059406 to 0.0001386138613861386\n",
            "Updated D_Y_opt learning rate from 0.0001405940594059406 to 0.0001386138613861386\n",
            "[130:END] Saving models and training information\n",
            "[131:100] Took 14.60s\n",
            "[131:100] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.080\n",
            "[131:100] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.067\n",
            "[131:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:200] Took 12.02s\n",
            "[131:200] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.075\n",
            "[131:200] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.683, cum_F_cycle_loss: 0.067\n",
            "[131:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:300] Took 11.93s\n",
            "[131:300] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.763, cum_G_cycle_loss: 0.076\n",
            "[131:300] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.704, cum_F_cycle_loss: 0.072\n",
            "[131:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:400] Took 12.38s\n",
            "[131:400] cum_D_X_loss: 0.105, cum_G_fool_loss: 0.711, cum_G_cycle_loss: 0.086\n",
            "[131:400] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.066\n",
            "[131:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:500] Took 12.15s\n",
            "[131:500] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.691, cum_G_cycle_loss: 0.084\n",
            "[131:500] cum_D_Y_loss: 0.093, cum_F_fool_loss: 0.665, cum_F_cycle_loss: 0.072\n",
            "[131:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:600] Took 12.02s\n",
            "[131:600] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.721, cum_G_cycle_loss: 0.078\n",
            "[131:600] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.688, cum_F_cycle_loss: 0.071\n",
            "[131:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:700] Took 12.11s\n",
            "[131:700] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.720, cum_G_cycle_loss: 0.082\n",
            "[131:700] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.066\n",
            "[131:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:800] Took 12.51s\n",
            "[131:800] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.085\n",
            "[131:800] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.072\n",
            "[131:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:900] Took 12.25s\n",
            "[131:900] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.729, cum_G_cycle_loss: 0.092\n",
            "[131:900] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.075\n",
            "[131:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:1000] Took 12.13s\n",
            "[131:1000] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.688, cum_G_cycle_loss: 0.086\n",
            "[131:1000] cum_D_Y_loss: 0.094, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.076\n",
            "[131:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:1100] Took 12.38s\n",
            "[131:1100] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.084\n",
            "[131:1100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.074\n",
            "[131:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[131:END] Completed epoch in 147.84754514694214s\n",
            "[131:1187] ep_D_X_loss: 0.088, ep_G_fool_loss: 0.722, ep_G_cycle_loss: 0.083\n",
            "[131:1187] ep_D_Y_loss: 0.080, ep_F_fool_loss: 0.693, ep_F_cycle_loss: 0.071\n",
            "[131:END] Completed eval in 1.0681488513946533s\n",
            "Updated G_opt learning rate from 0.0001386138613861386 to 0.00013663366336633662\n",
            "Updated F_opt learning rate from 0.0001386138613861386 to 0.00013663366336633662\n",
            "Updated D_X_opt learning rate from 0.0001386138613861386 to 0.00013663366336633662\n",
            "Updated D_Y_opt learning rate from 0.0001386138613861386 to 0.00013663366336633662\n",
            "[132:100] Took 14.62s\n",
            "[132:100] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.784, cum_G_cycle_loss: 0.081\n",
            "[132:100] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.729, cum_F_cycle_loss: 0.066\n",
            "[132:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:200] Took 12.47s\n",
            "[132:200] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.082\n",
            "[132:200] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.637, cum_F_cycle_loss: 0.066\n",
            "[132:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:300] Took 12.22s\n",
            "[132:300] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.693, cum_G_cycle_loss: 0.082\n",
            "[132:300] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.070\n",
            "[132:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:400] Took 12.49s\n",
            "[132:400] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.713, cum_G_cycle_loss: 0.086\n",
            "[132:400] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.730, cum_F_cycle_loss: 0.068\n",
            "[132:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:500] Took 12.38s\n",
            "[132:500] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.721, cum_G_cycle_loss: 0.082\n",
            "[132:500] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.670, cum_F_cycle_loss: 0.066\n",
            "[132:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:600] Took 12.01s\n",
            "[132:600] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.086\n",
            "[132:600] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.071\n",
            "[132:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:700] Took 12.46s\n",
            "[132:700] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.718, cum_G_cycle_loss: 0.083\n",
            "[132:700] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.076\n",
            "[132:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:800] Took 12.61s\n",
            "[132:800] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.093\n",
            "[132:800] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.072\n",
            "[132:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:900] Took 12.45s\n",
            "[132:900] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.086\n",
            "[132:900] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.070\n",
            "[132:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:1000] Took 12.43s\n",
            "[132:1000] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.700, cum_G_cycle_loss: 0.087\n",
            "[132:1000] cum_D_Y_loss: 0.098, cum_F_fool_loss: 0.673, cum_F_cycle_loss: 0.078\n",
            "[132:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:1100] Took 12.30s\n",
            "[132:1100] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.083\n",
            "[132:1100] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.069\n",
            "[132:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[132:END] Completed epoch in 149.84433317184448s\n",
            "[132:1187] ep_D_X_loss: 0.087, ep_G_fool_loss: 0.725, ep_G_cycle_loss: 0.084\n",
            "[132:1187] ep_D_Y_loss: 0.081, ep_F_fool_loss: 0.701, ep_F_cycle_loss: 0.070\n",
            "[132:END] Completed eval in 1.0632200241088867s\n",
            "Updated G_opt learning rate from 0.00013663366336633662 to 0.00013465346534653465\n",
            "Updated F_opt learning rate from 0.00013663366336633662 to 0.00013465346534653465\n",
            "Updated D_X_opt learning rate from 0.00013663366336633662 to 0.00013465346534653465\n",
            "Updated D_Y_opt learning rate from 0.00013663366336633662 to 0.00013465346534653465\n",
            "[133:100] Took 14.60s\n",
            "[133:100] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.685, cum_G_cycle_loss: 0.083\n",
            "[133:100] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.068\n",
            "[133:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:200] Took 12.29s\n",
            "[133:200] cum_D_X_loss: 0.109, cum_G_fool_loss: 0.689, cum_G_cycle_loss: 0.084\n",
            "[133:200] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.617, cum_F_cycle_loss: 0.070\n",
            "[133:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:300] Took 12.52s\n",
            "[133:300] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.717, cum_G_cycle_loss: 0.080\n",
            "[133:300] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.660, cum_F_cycle_loss: 0.067\n",
            "[133:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:400] Took 12.45s\n",
            "[133:400] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.078\n",
            "[133:400] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.706, cum_F_cycle_loss: 0.067\n",
            "[133:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:500] Took 12.36s\n",
            "[133:500] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.701, cum_G_cycle_loss: 0.086\n",
            "[133:500] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.069\n",
            "[133:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:600] Took 12.27s\n",
            "[133:600] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.717, cum_G_cycle_loss: 0.078\n",
            "[133:600] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.071\n",
            "[133:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:700] Took 12.39s\n",
            "[133:700] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.683, cum_G_cycle_loss: 0.082\n",
            "[133:700] cum_D_Y_loss: 0.091, cum_F_fool_loss: 0.668, cum_F_cycle_loss: 0.067\n",
            "[133:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:800] Took 12.41s\n",
            "[133:800] cum_D_X_loss: 0.105, cum_G_fool_loss: 0.684, cum_G_cycle_loss: 0.081\n",
            "[133:800] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.071\n",
            "[133:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:900] Took 12.41s\n",
            "[133:900] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.776, cum_G_cycle_loss: 0.083\n",
            "[133:900] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.071\n",
            "[133:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:1000] Took 12.12s\n",
            "[133:1000] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.091\n",
            "[133:1000] cum_D_Y_loss: 0.099, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.075\n",
            "[133:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:1100] Took 12.03s\n",
            "[133:1100] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.083\n",
            "[133:1100] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.069\n",
            "[133:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[133:END] Completed epoch in 149.48192858695984s\n",
            "[133:1187] ep_D_X_loss: 0.095, ep_G_fool_loss: 0.717, ep_G_cycle_loss: 0.082\n",
            "[133:1187] ep_D_Y_loss: 0.081, ep_F_fool_loss: 0.685, ep_F_cycle_loss: 0.070\n",
            "[133:END] Completed eval in 1.1793828010559082s\n",
            "Updated G_opt learning rate from 0.00013465346534653465 to 0.00013267326732673267\n",
            "Updated F_opt learning rate from 0.00013465346534653465 to 0.00013267326732673267\n",
            "Updated D_X_opt learning rate from 0.00013465346534653465 to 0.00013267326732673267\n",
            "Updated D_Y_opt learning rate from 0.00013465346534653465 to 0.00013267326732673267\n",
            "[134:100] Took 14.89s\n",
            "[134:100] cum_D_X_loss: 0.110, cum_G_fool_loss: 0.659, cum_G_cycle_loss: 0.082\n",
            "[134:100] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.649, cum_F_cycle_loss: 0.071\n",
            "[134:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:200] Took 12.59s\n",
            "[134:200] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.731, cum_G_cycle_loss: 0.082\n",
            "[134:200] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.072\n",
            "[134:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:300] Took 12.66s\n",
            "[134:300] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.746, cum_G_cycle_loss: 0.083\n",
            "[134:300] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.069\n",
            "[134:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:400] Took 12.37s\n",
            "[134:400] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.734, cum_G_cycle_loss: 0.083\n",
            "[134:400] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.071\n",
            "[134:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:500] Took 12.22s\n",
            "[134:500] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.088\n",
            "[134:500] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.672, cum_F_cycle_loss: 0.066\n",
            "[134:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:600] Took 12.58s\n",
            "[134:600] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.079\n",
            "[134:600] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.669, cum_F_cycle_loss: 0.068\n",
            "[134:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:700] Took 12.11s\n",
            "[134:700] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.763, cum_G_cycle_loss: 0.082\n",
            "[134:700] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.680, cum_F_cycle_loss: 0.068\n",
            "[134:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:800] Took 12.06s\n",
            "[134:800] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.776, cum_G_cycle_loss: 0.087\n",
            "[134:800] cum_D_Y_loss: 0.056, cum_F_fool_loss: 0.682, cum_F_cycle_loss: 0.070\n",
            "[134:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:900] Took 12.20s\n",
            "[134:900] cum_D_X_loss: 0.109, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.077\n",
            "[134:900] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.676, cum_F_cycle_loss: 0.073\n",
            "[134:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:1000] Took 12.25s\n",
            "[134:1000] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.084\n",
            "[134:1000] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.668, cum_F_cycle_loss: 0.065\n",
            "[134:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:1100] Took 12.31s\n",
            "[134:1100] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.083\n",
            "[134:1100] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.737, cum_F_cycle_loss: 0.075\n",
            "[134:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[134:END] Completed epoch in 149.9886395931244s\n",
            "[134:1187] ep_D_X_loss: 0.091, ep_G_fool_loss: 0.739, ep_G_cycle_loss: 0.082\n",
            "[134:1187] ep_D_Y_loss: 0.077, ep_F_fool_loss: 0.683, ep_F_cycle_loss: 0.069\n",
            "[134:END] Completed eval in 1.123992919921875s\n",
            "Updated G_opt learning rate from 0.00013267326732673267 to 0.0001306930693069307\n",
            "Updated F_opt learning rate from 0.00013267326732673267 to 0.0001306930693069307\n",
            "Updated D_X_opt learning rate from 0.00013267326732673267 to 0.0001306930693069307\n",
            "Updated D_Y_opt learning rate from 0.00013267326732673267 to 0.0001306930693069307\n",
            "[135:100] Took 14.74s\n",
            "[135:100] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.077\n",
            "[135:100] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.737, cum_F_cycle_loss: 0.070\n",
            "[135:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:200] Took 12.60s\n",
            "[135:200] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.078\n",
            "[135:200] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.064\n",
            "[135:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:300] Took 12.46s\n",
            "[135:300] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.701, cum_G_cycle_loss: 0.075\n",
            "[135:300] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.704, cum_F_cycle_loss: 0.067\n",
            "[135:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:400] Took 12.32s\n",
            "[135:400] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.726, cum_G_cycle_loss: 0.077\n",
            "[135:400] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.068\n",
            "[135:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:500] Took 12.36s\n",
            "[135:500] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.716, cum_G_cycle_loss: 0.075\n",
            "[135:500] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.063\n",
            "[135:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:600] Took 12.57s\n",
            "[135:600] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.082\n",
            "[135:600] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.071\n",
            "[135:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:700] Took 12.35s\n",
            "[135:700] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.686, cum_G_cycle_loss: 0.079\n",
            "[135:700] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.076\n",
            "[135:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:800] Took 12.71s\n",
            "[135:800] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.081\n",
            "[135:800] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.726, cum_F_cycle_loss: 0.069\n",
            "[135:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:900] Took 12.71s\n",
            "[135:900] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.733, cum_G_cycle_loss: 0.081\n",
            "[135:900] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.067\n",
            "[135:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:1000] Took 12.65s\n",
            "[135:1000] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.083\n",
            "[135:1000] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.788, cum_F_cycle_loss: 0.075\n",
            "[135:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:1100] Took 12.64s\n",
            "[135:1100] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.766, cum_G_cycle_loss: 0.081\n",
            "[135:1100] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.068\n",
            "[135:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[135:END] Completed epoch in 151.73332715034485s\n",
            "[135:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.725, ep_G_cycle_loss: 0.079\n",
            "[135:1187] ep_D_Y_loss: 0.081, ep_F_fool_loss: 0.711, ep_F_cycle_loss: 0.069\n",
            "[135:END] Completed eval in 1.1698534488677979s\n",
            "Updated G_opt learning rate from 0.0001306930693069307 to 0.00012871287128712872\n",
            "Updated F_opt learning rate from 0.0001306930693069307 to 0.00012871287128712872\n",
            "Updated D_X_opt learning rate from 0.0001306930693069307 to 0.00012871287128712872\n",
            "Updated D_Y_opt learning rate from 0.0001306930693069307 to 0.00012871287128712872\n",
            "[135:END] Saving models and training information\n",
            "[136:100] Took 14.83s\n",
            "[136:100] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.764, cum_G_cycle_loss: 0.080\n",
            "[136:100] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.083\n",
            "[136:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:200] Took 12.59s\n",
            "[136:200] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.695, cum_G_cycle_loss: 0.084\n",
            "[136:200] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.068\n",
            "[136:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:300] Took 12.40s\n",
            "[136:300] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.751, cum_G_cycle_loss: 0.088\n",
            "[136:300] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.072\n",
            "[136:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:400] Took 12.41s\n",
            "[136:400] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.082\n",
            "[136:400] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.715, cum_F_cycle_loss: 0.074\n",
            "[136:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:500] Took 12.48s\n",
            "[136:500] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.737, cum_G_cycle_loss: 0.081\n",
            "[136:500] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.069\n",
            "[136:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:600] Took 12.28s\n",
            "[136:600] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.659, cum_G_cycle_loss: 0.079\n",
            "[136:600] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.066\n",
            "[136:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:700] Took 12.28s\n",
            "[136:700] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.689, cum_G_cycle_loss: 0.082\n",
            "[136:700] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.636, cum_F_cycle_loss: 0.074\n",
            "[136:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:800] Took 12.60s\n",
            "[136:800] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.080\n",
            "[136:800] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.074\n",
            "[136:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:900] Took 12.56s\n",
            "[136:900] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.078\n",
            "[136:900] cum_D_Y_loss: 0.056, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.079\n",
            "[136:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:1000] Took 12.46s\n",
            "[136:1000] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.746, cum_G_cycle_loss: 0.075\n",
            "[136:1000] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.075\n",
            "[136:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:1100] Took 12.47s\n",
            "[136:1100] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.697, cum_G_cycle_loss: 0.078\n",
            "[136:1100] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.706, cum_F_cycle_loss: 0.074\n",
            "[136:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[136:END] Completed epoch in 151.26981854438782s\n",
            "[136:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.727, ep_G_cycle_loss: 0.081\n",
            "[136:1187] ep_D_Y_loss: 0.077, ep_F_fool_loss: 0.691, ep_F_cycle_loss: 0.073\n",
            "[136:END] Completed eval in 1.1741414070129395s\n",
            "Updated G_opt learning rate from 0.00012871287128712872 to 0.00012673267326732674\n",
            "Updated F_opt learning rate from 0.00012871287128712872 to 0.00012673267326732674\n",
            "Updated D_X_opt learning rate from 0.00012871287128712872 to 0.00012673267326732674\n",
            "Updated D_Y_opt learning rate from 0.00012871287128712872 to 0.00012673267326732674\n",
            "[137:100] Took 23.09s\n",
            "[137:100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.787, cum_G_cycle_loss: 0.081\n",
            "[137:100] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.731, cum_F_cycle_loss: 0.068\n",
            "[137:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:200] Took 12.77s\n",
            "[137:200] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.712, cum_G_cycle_loss: 0.074\n",
            "[137:200] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.070\n",
            "[137:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:300] Took 12.42s\n",
            "[137:300] cum_D_X_loss: 0.109, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.077\n",
            "[137:300] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.068\n",
            "[137:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:400] Took 12.55s\n",
            "[137:400] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.695, cum_G_cycle_loss: 0.079\n",
            "[137:400] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.070\n",
            "[137:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:500] Took 12.47s\n",
            "[137:500] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.698, cum_G_cycle_loss: 0.079\n",
            "[137:500] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.061\n",
            "[137:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:600] Took 12.15s\n",
            "[137:600] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.714, cum_G_cycle_loss: 0.084\n",
            "[137:600] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.649, cum_F_cycle_loss: 0.067\n",
            "[137:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:700] Took 12.30s\n",
            "[137:700] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.079\n",
            "[137:700] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.673, cum_F_cycle_loss: 0.069\n",
            "[137:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:800] Took 12.15s\n",
            "[137:800] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.084\n",
            "[137:800] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.625, cum_F_cycle_loss: 0.064\n",
            "[137:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:900] Took 12.73s\n",
            "[137:900] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.086\n",
            "[137:900] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.069\n",
            "[137:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:1000] Took 12.61s\n",
            "[137:1000] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.786, cum_G_cycle_loss: 0.082\n",
            "[137:1000] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.067\n",
            "[137:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:1100] Took 12.35s\n",
            "[137:1100] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.078\n",
            "[137:1100] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.067\n",
            "[137:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[137:END] Completed epoch in 159.14913749694824s\n",
            "[137:1187] ep_D_X_loss: 0.091, ep_G_fool_loss: 0.730, ep_G_cycle_loss: 0.081\n",
            "[137:1187] ep_D_Y_loss: 0.076, ep_F_fool_loss: 0.689, ep_F_cycle_loss: 0.068\n",
            "[137:END] Completed eval in 1.1364398002624512s\n",
            "Updated G_opt learning rate from 0.00012673267326732674 to 0.00012475247524752477\n",
            "Updated F_opt learning rate from 0.00012673267326732674 to 0.00012475247524752477\n",
            "Updated D_X_opt learning rate from 0.00012673267326732674 to 0.00012475247524752477\n",
            "Updated D_Y_opt learning rate from 0.00012673267326732674 to 0.00012475247524752477\n",
            "[138:100] Took 14.69s\n",
            "[138:100] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.734, cum_G_cycle_loss: 0.081\n",
            "[138:100] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.076\n",
            "[138:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:200] Took 12.35s\n",
            "[138:200] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.685, cum_G_cycle_loss: 0.082\n",
            "[138:200] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.639, cum_F_cycle_loss: 0.069\n",
            "[138:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:300] Took 12.96s\n",
            "[138:300] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.794, cum_G_cycle_loss: 0.077\n",
            "[138:300] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.071\n",
            "[138:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:400] Took 12.75s\n",
            "[138:400] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.746, cum_G_cycle_loss: 0.086\n",
            "[138:400] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.671, cum_F_cycle_loss: 0.066\n",
            "[138:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:500] Took 12.41s\n",
            "[138:500] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.657, cum_G_cycle_loss: 0.082\n",
            "[138:500] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.072\n",
            "[138:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:600] Took 12.22s\n",
            "[138:600] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.675, cum_G_cycle_loss: 0.082\n",
            "[138:600] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.638, cum_F_cycle_loss: 0.072\n",
            "[138:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:700] Took 12.11s\n",
            "[138:700] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.082\n",
            "[138:700] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.631, cum_F_cycle_loss: 0.074\n",
            "[138:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:800] Took 12.35s\n",
            "[138:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.713, cum_G_cycle_loss: 0.079\n",
            "[138:800] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.067\n",
            "[138:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:900] Took 12.46s\n",
            "[138:900] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.076\n",
            "[138:900] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.655, cum_F_cycle_loss: 0.069\n",
            "[138:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:1000] Took 12.41s\n",
            "[138:1000] cum_D_X_loss: 0.102, cum_G_fool_loss: 0.713, cum_G_cycle_loss: 0.081\n",
            "[138:1000] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.643, cum_F_cycle_loss: 0.069\n",
            "[138:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:1100] Took 12.48s\n",
            "[138:1100] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.081\n",
            "[138:1100] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.073\n",
            "[138:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[138:END] Completed epoch in 150.84198141098022s\n",
            "[138:1187] ep_D_X_loss: 0.090, ep_G_fool_loss: 0.724, ep_G_cycle_loss: 0.081\n",
            "[138:1187] ep_D_Y_loss: 0.080, ep_F_fool_loss: 0.674, ep_F_cycle_loss: 0.071\n",
            "[138:END] Completed eval in 1.1588714122772217s\n",
            "Updated G_opt learning rate from 0.00012475247524752477 to 0.0001227722772277228\n",
            "Updated F_opt learning rate from 0.00012475247524752477 to 0.0001227722772277228\n",
            "Updated D_X_opt learning rate from 0.00012475247524752477 to 0.0001227722772277228\n",
            "Updated D_Y_opt learning rate from 0.00012475247524752477 to 0.0001227722772277228\n",
            "[139:100] Took 14.69s\n",
            "[139:100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.740, cum_G_cycle_loss: 0.080\n",
            "[139:100] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.729, cum_F_cycle_loss: 0.081\n",
            "[139:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:200] Took 11.79s\n",
            "[139:200] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.074\n",
            "[139:200] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.739, cum_F_cycle_loss: 0.071\n",
            "[139:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:300] Took 11.73s\n",
            "[139:300] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.718, cum_G_cycle_loss: 0.076\n",
            "[139:300] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.073\n",
            "[139:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:400] Took 11.72s\n",
            "[139:400] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.086\n",
            "[139:400] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.070\n",
            "[139:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:500] Took 11.72s\n",
            "[139:500] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.696, cum_G_cycle_loss: 0.079\n",
            "[139:500] cum_D_Y_loss: 0.091, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.070\n",
            "[139:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:600] Took 11.73s\n",
            "[139:600] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.714, cum_G_cycle_loss: 0.078\n",
            "[139:600] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.671, cum_F_cycle_loss: 0.071\n",
            "[139:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:700] Took 11.73s\n",
            "[139:700] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.082\n",
            "[139:700] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.068\n",
            "[139:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:800] Took 11.72s\n",
            "[139:800] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.072\n",
            "[139:800] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.636, cum_F_cycle_loss: 0.070\n",
            "[139:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:900] Took 11.73s\n",
            "[139:900] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.733, cum_G_cycle_loss: 0.084\n",
            "[139:900] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.715, cum_F_cycle_loss: 0.073\n",
            "[139:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:1000] Took 11.73s\n",
            "[139:1000] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.777, cum_G_cycle_loss: 0.084\n",
            "[139:1000] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.728, cum_F_cycle_loss: 0.064\n",
            "[139:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:1100] Took 11.73s\n",
            "[139:1100] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.083\n",
            "[139:1100] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.066\n",
            "[139:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[139:END] Completed epoch in 142.99729323387146s\n",
            "[139:1187] ep_D_X_loss: 0.086, ep_G_fool_loss: 0.739, ep_G_cycle_loss: 0.079\n",
            "[139:1187] ep_D_Y_loss: 0.078, ep_F_fool_loss: 0.709, ep_F_cycle_loss: 0.070\n",
            "[139:END] Completed eval in 1.2107605934143066s\n",
            "Updated G_opt learning rate from 0.0001227722772277228 to 0.0001207920792079208\n",
            "Updated F_opt learning rate from 0.0001227722772277228 to 0.0001207920792079208\n",
            "Updated D_X_opt learning rate from 0.0001227722772277228 to 0.0001207920792079208\n",
            "Updated D_Y_opt learning rate from 0.0001227722772277228 to 0.0001207920792079208\n",
            "[140:100] Took 13.89s\n",
            "[140:100] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.782, cum_G_cycle_loss: 0.079\n",
            "[140:100] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.616, cum_F_cycle_loss: 0.061\n",
            "[140:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:200] Took 11.73s\n",
            "[140:200] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.708, cum_G_cycle_loss: 0.080\n",
            "[140:200] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.067\n",
            "[140:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:300] Took 11.74s\n",
            "[140:300] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.730, cum_G_cycle_loss: 0.079\n",
            "[140:300] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.070\n",
            "[140:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:400] Took 11.73s\n",
            "[140:400] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.090\n",
            "[140:400] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.749, cum_F_cycle_loss: 0.071\n",
            "[140:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:500] Took 11.72s\n",
            "[140:500] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.075\n",
            "[140:500] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.655, cum_F_cycle_loss: 0.068\n",
            "[140:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:600] Took 11.73s\n",
            "[140:600] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.084\n",
            "[140:600] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.671, cum_F_cycle_loss: 0.070\n",
            "[140:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:700] Took 11.73s\n",
            "[140:700] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.692, cum_G_cycle_loss: 0.078\n",
            "[140:700] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.646, cum_F_cycle_loss: 0.069\n",
            "[140:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:800] Took 11.73s\n",
            "[140:800] cum_D_X_loss: 0.102, cum_G_fool_loss: 0.695, cum_G_cycle_loss: 0.076\n",
            "[140:800] cum_D_Y_loss: 0.092, cum_F_fool_loss: 0.664, cum_F_cycle_loss: 0.073\n",
            "[140:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:900] Took 11.73s\n",
            "[140:900] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.089\n",
            "[140:900] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.711, cum_F_cycle_loss: 0.072\n",
            "[140:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:1000] Took 11.73s\n",
            "[140:1000] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.077\n",
            "[140:1000] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.064\n",
            "[140:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:1100] Took 11.73s\n",
            "[140:1100] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.692, cum_G_cycle_loss: 0.082\n",
            "[140:1100] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.655, cum_F_cycle_loss: 0.069\n",
            "[140:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[140:END] Completed epoch in 142.1433892250061s\n",
            "[140:1187] ep_D_X_loss: 0.088, ep_G_fool_loss: 0.724, ep_G_cycle_loss: 0.080\n",
            "[140:1187] ep_D_Y_loss: 0.083, ep_F_fool_loss: 0.679, ep_F_cycle_loss: 0.068\n",
            "[140:END] Completed eval in 1.1389153003692627s\n",
            "Updated G_opt learning rate from 0.0001207920792079208 to 0.00011881188118811881\n",
            "Updated F_opt learning rate from 0.0001207920792079208 to 0.00011881188118811881\n",
            "Updated D_X_opt learning rate from 0.0001207920792079208 to 0.00011881188118811881\n",
            "Updated D_Y_opt learning rate from 0.0001207920792079208 to 0.00011881188118811881\n",
            "[140:END] Saving models and training information\n",
            "[141:100] Took 13.88s\n",
            "[141:100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.721, cum_G_cycle_loss: 0.078\n",
            "[141:100] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.682, cum_F_cycle_loss: 0.070\n",
            "[141:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:200] Took 11.73s\n",
            "[141:200] cum_D_X_loss: 0.068, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.078\n",
            "[141:200] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.757, cum_F_cycle_loss: 0.069\n",
            "[141:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:300] Took 11.73s\n",
            "[141:300] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.750, cum_G_cycle_loss: 0.076\n",
            "[141:300] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.070\n",
            "[141:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:400] Took 11.73s\n",
            "[141:400] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.078\n",
            "[141:400] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.666, cum_F_cycle_loss: 0.067\n",
            "[141:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:500] Took 11.72s\n",
            "[141:500] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.735, cum_G_cycle_loss: 0.082\n",
            "[141:500] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.072\n",
            "[141:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:600] Took 11.73s\n",
            "[141:600] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.082\n",
            "[141:600] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.071\n",
            "[141:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:700] Took 11.73s\n",
            "[141:700] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.077\n",
            "[141:700] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.071\n",
            "[141:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:800] Took 11.73s\n",
            "[141:800] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.761, cum_G_cycle_loss: 0.082\n",
            "[141:800] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.734, cum_F_cycle_loss: 0.070\n",
            "[141:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:900] Took 11.72s\n",
            "[141:900] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.718, cum_G_cycle_loss: 0.079\n",
            "[141:900] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.743, cum_F_cycle_loss: 0.067\n",
            "[141:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:1000] Took 11.73s\n",
            "[141:1000] cum_D_X_loss: 0.105, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.080\n",
            "[141:1000] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.064\n",
            "[141:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:1100] Took 11.73s\n",
            "[141:1100] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.085\n",
            "[141:1100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.647, cum_F_cycle_loss: 0.074\n",
            "[141:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[141:END] Completed epoch in 142.11862349510193s\n",
            "[141:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.736, ep_G_cycle_loss: 0.079\n",
            "[141:1187] ep_D_Y_loss: 0.078, ep_F_fool_loss: 0.705, ep_F_cycle_loss: 0.069\n",
            "[141:END] Completed eval in 1.1429154872894287s\n",
            "Updated G_opt learning rate from 0.00011881188118811881 to 0.00011683168316831685\n",
            "Updated F_opt learning rate from 0.00011881188118811881 to 0.00011683168316831685\n",
            "Updated D_X_opt learning rate from 0.00011881188118811881 to 0.00011683168316831685\n",
            "Updated D_Y_opt learning rate from 0.00011881188118811881 to 0.00011683168316831685\n",
            "[142:100] Took 13.87s\n",
            "[142:100] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.706, cum_G_cycle_loss: 0.075\n",
            "[142:100] cum_D_Y_loss: 0.098, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.069\n",
            "[142:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:200] Took 11.72s\n",
            "[142:200] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.800, cum_G_cycle_loss: 0.082\n",
            "[142:200] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.718, cum_F_cycle_loss: 0.066\n",
            "[142:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:300] Took 11.73s\n",
            "[142:300] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.079\n",
            "[142:300] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.671, cum_F_cycle_loss: 0.065\n",
            "[142:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:400] Took 11.72s\n",
            "[142:400] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.079\n",
            "[142:400] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.761, cum_F_cycle_loss: 0.075\n",
            "[142:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:500] Took 11.73s\n",
            "[142:500] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.734, cum_G_cycle_loss: 0.080\n",
            "[142:500] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.064\n",
            "[142:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:600] Took 11.72s\n",
            "[142:600] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.075\n",
            "[142:600] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.659, cum_F_cycle_loss: 0.071\n",
            "[142:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:700] Took 11.72s\n",
            "[142:700] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.079\n",
            "[142:700] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.068\n",
            "[142:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:800] Took 11.73s\n",
            "[142:800] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.781, cum_G_cycle_loss: 0.082\n",
            "[142:800] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.657, cum_F_cycle_loss: 0.071\n",
            "[142:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:900] Took 11.73s\n",
            "[142:900] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.078\n",
            "[142:900] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.711, cum_F_cycle_loss: 0.072\n",
            "[142:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:1000] Took 11.72s\n",
            "[142:1000] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.781, cum_G_cycle_loss: 0.080\n",
            "[142:1000] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.673, cum_F_cycle_loss: 0.074\n",
            "[142:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:1100] Took 11.73s\n",
            "[142:1100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.689, cum_G_cycle_loss: 0.082\n",
            "[142:1100] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.071\n",
            "[142:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[142:END] Completed epoch in 142.09990096092224s\n",
            "[142:1187] ep_D_X_loss: 0.089, ep_G_fool_loss: 0.737, ep_G_cycle_loss: 0.080\n",
            "[142:1187] ep_D_Y_loss: 0.080, ep_F_fool_loss: 0.689, ep_F_cycle_loss: 0.070\n",
            "[142:END] Completed eval in 1.173891544342041s\n",
            "Updated G_opt learning rate from 0.00011683168316831685 to 0.00011485148514851487\n",
            "Updated F_opt learning rate from 0.00011683168316831685 to 0.00011485148514851487\n",
            "Updated D_X_opt learning rate from 0.00011683168316831685 to 0.00011485148514851487\n",
            "Updated D_Y_opt learning rate from 0.00011683168316831685 to 0.00011485148514851487\n",
            "[143:100] Took 13.85s\n",
            "[143:100] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.084\n",
            "[143:100] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.073\n",
            "[143:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:200] Took 11.69s\n",
            "[143:200] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.081\n",
            "[143:200] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.066\n",
            "[143:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:300] Took 11.70s\n",
            "[143:300] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.680, cum_G_cycle_loss: 0.077\n",
            "[143:300] cum_D_Y_loss: 0.096, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.067\n",
            "[143:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:400] Took 11.70s\n",
            "[143:400] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.744, cum_G_cycle_loss: 0.074\n",
            "[143:400] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.070\n",
            "[143:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:500] Took 11.70s\n",
            "[143:500] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.082\n",
            "[143:500] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.652, cum_F_cycle_loss: 0.069\n",
            "[143:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:600] Took 11.69s\n",
            "[143:600] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.084\n",
            "[143:600] cum_D_Y_loss: 0.096, cum_F_fool_loss: 0.744, cum_F_cycle_loss: 0.069\n",
            "[143:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:700] Took 11.70s\n",
            "[143:700] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.789, cum_G_cycle_loss: 0.082\n",
            "[143:700] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.663, cum_F_cycle_loss: 0.065\n",
            "[143:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:800] Took 11.69s\n",
            "[143:800] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.076\n",
            "[143:800] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.070\n",
            "[143:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:900] Took 11.70s\n",
            "[143:900] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.721, cum_G_cycle_loss: 0.078\n",
            "[143:900] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.706, cum_F_cycle_loss: 0.064\n",
            "[143:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:1000] Took 11.69s\n",
            "[143:1000] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.079\n",
            "[143:1000] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.666, cum_F_cycle_loss: 0.067\n",
            "[143:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:1100] Took 11.70s\n",
            "[143:1100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.701, cum_G_cycle_loss: 0.077\n",
            "[143:1100] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.735, cum_F_cycle_loss: 0.071\n",
            "[143:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[143:END] Completed epoch in 141.75100588798523s\n",
            "[143:1187] ep_D_X_loss: 0.088, ep_G_fool_loss: 0.732, ep_G_cycle_loss: 0.080\n",
            "[143:1187] ep_D_Y_loss: 0.081, ep_F_fool_loss: 0.697, ep_F_cycle_loss: 0.068\n",
            "[143:END] Completed eval in 1.2436399459838867s\n",
            "Updated G_opt learning rate from 0.00011485148514851487 to 0.00011287128712871289\n",
            "Updated F_opt learning rate from 0.00011485148514851487 to 0.00011287128712871289\n",
            "Updated D_X_opt learning rate from 0.00011485148514851487 to 0.00011287128712871289\n",
            "Updated D_Y_opt learning rate from 0.00011485148514851487 to 0.00011287128712871289\n",
            "[144:100] Took 13.87s\n",
            "[144:100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.730, cum_G_cycle_loss: 0.078\n",
            "[144:100] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.071\n",
            "[144:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:200] Took 11.69s\n",
            "[144:200] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.749, cum_G_cycle_loss: 0.077\n",
            "[144:200] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.065\n",
            "[144:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:300] Took 11.70s\n",
            "[144:300] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.763, cum_G_cycle_loss: 0.081\n",
            "[144:300] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.070\n",
            "[144:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:400] Took 11.70s\n",
            "[144:400] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.072\n",
            "[144:400] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.734, cum_F_cycle_loss: 0.071\n",
            "[144:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:500] Took 11.70s\n",
            "[144:500] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.710, cum_G_cycle_loss: 0.076\n",
            "[144:500] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.066\n",
            "[144:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:600] Took 11.69s\n",
            "[144:600] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.796, cum_G_cycle_loss: 0.077\n",
            "[144:600] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.072\n",
            "[144:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:700] Took 11.70s\n",
            "[144:700] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.708, cum_G_cycle_loss: 0.077\n",
            "[144:700] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.652, cum_F_cycle_loss: 0.071\n",
            "[144:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:800] Took 11.70s\n",
            "[144:800] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.084\n",
            "[144:800] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.067\n",
            "[144:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:900] Took 11.70s\n",
            "[144:900] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.737, cum_G_cycle_loss: 0.078\n",
            "[144:900] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.070\n",
            "[144:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:1000] Took 11.69s\n",
            "[144:1000] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.734, cum_G_cycle_loss: 0.076\n",
            "[144:1000] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.676, cum_F_cycle_loss: 0.069\n",
            "[144:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:1100] Took 11.70s\n",
            "[144:1100] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.697, cum_G_cycle_loss: 0.079\n",
            "[144:1100] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.726, cum_F_cycle_loss: 0.065\n",
            "[144:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[144:END] Completed epoch in 141.761883020401s\n",
            "[144:1187] ep_D_X_loss: 0.089, ep_G_fool_loss: 0.739, ep_G_cycle_loss: 0.078\n",
            "[144:1187] ep_D_Y_loss: 0.073, ep_F_fool_loss: 0.692, ep_F_cycle_loss: 0.069\n",
            "[144:END] Completed eval in 1.2346677780151367s\n",
            "Updated G_opt learning rate from 0.00011287128712871289 to 0.0001108910891089109\n",
            "Updated F_opt learning rate from 0.00011287128712871289 to 0.0001108910891089109\n",
            "Updated D_X_opt learning rate from 0.00011287128712871289 to 0.0001108910891089109\n",
            "Updated D_Y_opt learning rate from 0.00011287128712871289 to 0.0001108910891089109\n",
            "[145:100] Took 13.84s\n",
            "[145:100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.078\n",
            "[145:100] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.719, cum_F_cycle_loss: 0.067\n",
            "[145:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:200] Took 11.70s\n",
            "[145:200] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.744, cum_G_cycle_loss: 0.075\n",
            "[145:200] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.067\n",
            "[145:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:300] Took 11.69s\n",
            "[145:300] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.761, cum_G_cycle_loss: 0.080\n",
            "[145:300] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.066\n",
            "[145:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:400] Took 11.69s\n",
            "[145:400] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.767, cum_G_cycle_loss: 0.074\n",
            "[145:400] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.069\n",
            "[145:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:500] Took 11.70s\n",
            "[145:500] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.083\n",
            "[145:500] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.072\n",
            "[145:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:600] Took 11.70s\n",
            "[145:600] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.079\n",
            "[145:600] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.069\n",
            "[145:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:700] Took 11.69s\n",
            "[145:700] cum_D_X_loss: 0.105, cum_G_fool_loss: 0.699, cum_G_cycle_loss: 0.079\n",
            "[145:700] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.654, cum_F_cycle_loss: 0.071\n",
            "[145:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:800] Took 11.70s\n",
            "[145:800] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.078\n",
            "[145:800] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.067\n",
            "[145:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:900] Took 11.70s\n",
            "[145:900] cum_D_X_loss: 0.070, cum_G_fool_loss: 0.772, cum_G_cycle_loss: 0.075\n",
            "[145:900] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.726, cum_F_cycle_loss: 0.068\n",
            "[145:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:1000] Took 11.70s\n",
            "[145:1000] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.711, cum_G_cycle_loss: 0.076\n",
            "[145:1000] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.066\n",
            "[145:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:1100] Took 11.69s\n",
            "[145:1100] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.730, cum_G_cycle_loss: 0.078\n",
            "[145:1100] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.683, cum_F_cycle_loss: 0.063\n",
            "[145:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[145:END] Completed epoch in 141.73693251609802s\n",
            "[145:1187] ep_D_X_loss: 0.089, ep_G_fool_loss: 0.741, ep_G_cycle_loss: 0.078\n",
            "[145:1187] ep_D_Y_loss: 0.075, ep_F_fool_loss: 0.695, ep_F_cycle_loss: 0.067\n",
            "[145:END] Completed eval in 1.1688728332519531s\n",
            "Updated G_opt learning rate from 0.0001108910891089109 to 0.00010891089108910893\n",
            "Updated F_opt learning rate from 0.0001108910891089109 to 0.00010891089108910893\n",
            "Updated D_X_opt learning rate from 0.0001108910891089109 to 0.00010891089108910893\n",
            "Updated D_Y_opt learning rate from 0.0001108910891089109 to 0.00010891089108910893\n",
            "[145:END] Saving models and training information\n",
            "[146:100] Took 13.84s\n",
            "[146:100] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.076\n",
            "[146:100] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.753, cum_F_cycle_loss: 0.071\n",
            "[146:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:200] Took 11.70s\n",
            "[146:200] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.712, cum_G_cycle_loss: 0.080\n",
            "[146:200] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.743, cum_F_cycle_loss: 0.070\n",
            "[146:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:300] Took 11.69s\n",
            "[146:300] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.077\n",
            "[146:300] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.072\n",
            "[146:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:400] Took 11.69s\n",
            "[146:400] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.759, cum_G_cycle_loss: 0.077\n",
            "[146:400] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.069\n",
            "[146:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:500] Took 11.69s\n",
            "[146:500] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.076\n",
            "[146:500] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.643, cum_F_cycle_loss: 0.067\n",
            "[146:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:600] Took 11.70s\n",
            "[146:600] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.701, cum_G_cycle_loss: 0.074\n",
            "[146:600] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.069\n",
            "[146:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:700] Took 11.70s\n",
            "[146:700] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.080\n",
            "[146:700] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.649, cum_F_cycle_loss: 0.069\n",
            "[146:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:800] Took 11.69s\n",
            "[146:800] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.777, cum_G_cycle_loss: 0.077\n",
            "[146:800] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.069\n",
            "[146:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:900] Took 11.69s\n",
            "[146:900] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.077\n",
            "[146:900] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.068\n",
            "[146:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:1000] Took 11.70s\n",
            "[146:1000] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.711, cum_G_cycle_loss: 0.077\n",
            "[146:1000] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.686, cum_F_cycle_loss: 0.070\n",
            "[146:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:1100] Took 11.68s\n",
            "[146:1100] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.077\n",
            "[146:1100] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.646, cum_F_cycle_loss: 0.070\n",
            "[146:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[146:END] Completed epoch in 141.71740889549255s\n",
            "[146:1187] ep_D_X_loss: 0.087, ep_G_fool_loss: 0.740, ep_G_cycle_loss: 0.077\n",
            "[146:1187] ep_D_Y_loss: 0.075, ep_F_fool_loss: 0.688, ep_F_cycle_loss: 0.069\n",
            "[146:END] Completed eval in 1.1788690090179443s\n",
            "Updated G_opt learning rate from 0.00010891089108910893 to 0.00010693069306930694\n",
            "Updated F_opt learning rate from 0.00010891089108910893 to 0.00010693069306930694\n",
            "Updated D_X_opt learning rate from 0.00010891089108910893 to 0.00010693069306930694\n",
            "Updated D_Y_opt learning rate from 0.00010891089108910893 to 0.00010693069306930694\n",
            "[147:100] Took 13.86s\n",
            "[147:100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.672, cum_G_cycle_loss: 0.079\n",
            "[147:100] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.747, cum_F_cycle_loss: 0.070\n",
            "[147:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:200] Took 11.69s\n",
            "[147:200] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.076\n",
            "[147:200] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.069\n",
            "[147:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:300] Took 11.69s\n",
            "[147:300] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.793, cum_G_cycle_loss: 0.071\n",
            "[147:300] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.654, cum_F_cycle_loss: 0.067\n",
            "[147:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:400] Took 11.68s\n",
            "[147:400] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.080\n",
            "[147:400] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.067\n",
            "[147:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:500] Took 11.69s\n",
            "[147:500] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.782, cum_G_cycle_loss: 0.076\n",
            "[147:500] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.068\n",
            "[147:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:600] Took 11.69s\n",
            "[147:600] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.782, cum_G_cycle_loss: 0.076\n",
            "[147:600] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.662, cum_F_cycle_loss: 0.061\n",
            "[147:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:700] Took 11.69s\n",
            "[147:700] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.076\n",
            "[147:700] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.063\n",
            "[147:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:800] Took 11.69s\n",
            "[147:800] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.781, cum_G_cycle_loss: 0.078\n",
            "[147:800] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.633, cum_F_cycle_loss: 0.063\n",
            "[147:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:900] Took 11.69s\n",
            "[147:900] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.750, cum_G_cycle_loss: 0.075\n",
            "[147:900] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.065\n",
            "[147:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:1000] Took 11.69s\n",
            "[147:1000] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.707, cum_G_cycle_loss: 0.085\n",
            "[147:1000] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.673, cum_F_cycle_loss: 0.064\n",
            "[147:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:1100] Took 11.70s\n",
            "[147:1100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.744, cum_G_cycle_loss: 0.072\n",
            "[147:1100] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.069\n",
            "[147:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[147:END] Completed epoch in 141.72181963920593s\n",
            "[147:1187] ep_D_X_loss: 0.084, ep_G_fool_loss: 0.744, ep_G_cycle_loss: 0.077\n",
            "[147:1187] ep_D_Y_loss: 0.076, ep_F_fool_loss: 0.692, ep_F_cycle_loss: 0.066\n",
            "[147:END] Completed eval in 1.1908142566680908s\n",
            "Updated G_opt learning rate from 0.00010693069306930694 to 0.00010495049504950496\n",
            "Updated F_opt learning rate from 0.00010693069306930694 to 0.00010495049504950496\n",
            "Updated D_X_opt learning rate from 0.00010693069306930694 to 0.00010495049504950496\n",
            "Updated D_Y_opt learning rate from 0.00010693069306930694 to 0.00010495049504950496\n",
            "[148:100] Took 13.95s\n",
            "[148:100] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.077\n",
            "[148:100] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.066\n",
            "[148:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:200] Took 11.70s\n",
            "[148:200] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.697, cum_G_cycle_loss: 0.077\n",
            "[148:200] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.659, cum_F_cycle_loss: 0.065\n",
            "[148:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:300] Took 11.69s\n",
            "[148:300] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.735, cum_G_cycle_loss: 0.077\n",
            "[148:300] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.738, cum_F_cycle_loss: 0.066\n",
            "[148:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:400] Took 11.69s\n",
            "[148:400] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.083\n",
            "[148:400] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.065\n",
            "[148:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:500] Took 11.69s\n",
            "[148:500] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.075\n",
            "[148:500] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.737, cum_F_cycle_loss: 0.068\n",
            "[148:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:600] Took 11.70s\n",
            "[148:600] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.070\n",
            "[148:600] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.686, cum_F_cycle_loss: 0.069\n",
            "[148:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:700] Took 11.70s\n",
            "[148:700] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.774, cum_G_cycle_loss: 0.079\n",
            "[148:700] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.756, cum_F_cycle_loss: 0.068\n",
            "[148:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:800] Took 11.69s\n",
            "[148:800] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.726, cum_G_cycle_loss: 0.081\n",
            "[148:800] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.659, cum_F_cycle_loss: 0.072\n",
            "[148:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:900] Took 11.70s\n",
            "[148:900] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.077\n",
            "[148:900] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.067\n",
            "[148:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:1000] Took 11.70s\n",
            "[148:1000] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.690, cum_G_cycle_loss: 0.075\n",
            "[148:1000] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.068\n",
            "[148:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:1100] Took 11.69s\n",
            "[148:1100] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.693, cum_G_cycle_loss: 0.075\n",
            "[148:1100] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.071\n",
            "[148:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[148:END] Completed epoch in 141.84877705574036s\n",
            "[148:1187] ep_D_X_loss: 0.089, ep_G_fool_loss: 0.725, ep_G_cycle_loss: 0.077\n",
            "[148:1187] ep_D_Y_loss: 0.079, ep_F_fool_loss: 0.694, ep_F_cycle_loss: 0.068\n",
            "[148:END] Completed eval in 1.1918435096740723s\n",
            "Updated G_opt learning rate from 0.00010495049504950496 to 0.00010297029702970298\n",
            "Updated F_opt learning rate from 0.00010495049504950496 to 0.00010297029702970298\n",
            "Updated D_X_opt learning rate from 0.00010495049504950496 to 0.00010297029702970298\n",
            "Updated D_Y_opt learning rate from 0.00010495049504950496 to 0.00010297029702970298\n",
            "[149:100] Took 13.96s\n",
            "[149:100] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.766, cum_G_cycle_loss: 0.077\n",
            "[149:100] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.731, cum_F_cycle_loss: 0.067\n",
            "[149:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:200] Took 11.83s\n",
            "[149:200] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.077\n",
            "[149:200] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.683, cum_F_cycle_loss: 0.070\n",
            "[149:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:300] Took 11.70s\n",
            "[149:300] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.075\n",
            "[149:300] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.668, cum_F_cycle_loss: 0.068\n",
            "[149:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:400] Took 11.69s\n",
            "[149:400] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.081\n",
            "[149:400] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.642, cum_F_cycle_loss: 0.065\n",
            "[149:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:500] Took 12.13s\n",
            "[149:500] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.749, cum_G_cycle_loss: 0.080\n",
            "[149:500] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.728, cum_F_cycle_loss: 0.067\n",
            "[149:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:600] Took 11.73s\n",
            "[149:600] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.718, cum_G_cycle_loss: 0.085\n",
            "[149:600] cum_D_Y_loss: 0.091, cum_F_fool_loss: 0.673, cum_F_cycle_loss: 0.068\n",
            "[149:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:700] Took 11.78s\n",
            "[149:700] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.072\n",
            "[149:700] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.069\n",
            "[149:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:800] Took 12.62s\n",
            "[149:800] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.783, cum_G_cycle_loss: 0.076\n",
            "[149:800] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.065\n",
            "[149:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:900] Took 11.93s\n",
            "[149:900] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.074\n",
            "[149:900] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.065\n",
            "[149:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:1000] Took 12.25s\n",
            "[149:1000] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.080\n",
            "[149:1000] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.065\n",
            "[149:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:1100] Took 12.14s\n",
            "[149:1100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.699, cum_G_cycle_loss: 0.072\n",
            "[149:1100] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.736, cum_F_cycle_loss: 0.068\n",
            "[149:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[149:END] Completed epoch in 144.74774312973022s\n",
            "[149:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.745, ep_G_cycle_loss: 0.078\n",
            "[149:1187] ep_D_Y_loss: 0.074, ep_F_fool_loss: 0.695, ep_F_cycle_loss: 0.067\n",
            "[149:END] Completed eval in 1.2257208824157715s\n",
            "Updated G_opt learning rate from 0.00010297029702970298 to 0.00010099009900990099\n",
            "Updated F_opt learning rate from 0.00010297029702970298 to 0.00010099009900990099\n",
            "Updated D_X_opt learning rate from 0.00010297029702970298 to 0.00010099009900990099\n",
            "Updated D_Y_opt learning rate from 0.00010297029702970298 to 0.00010099009900990099\n",
            "[150:100] Took 14.31s\n",
            "[150:100] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.713, cum_G_cycle_loss: 0.080\n",
            "[150:100] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.064\n",
            "[150:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:200] Took 11.82s\n",
            "[150:200] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.077\n",
            "[150:200] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.065\n",
            "[150:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:300] Took 11.69s\n",
            "[150:300] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.695, cum_G_cycle_loss: 0.076\n",
            "[150:300] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.071\n",
            "[150:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:400] Took 11.69s\n",
            "[150:400] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.079\n",
            "[150:400] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.062\n",
            "[150:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:500] Took 11.68s\n",
            "[150:500] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.698, cum_G_cycle_loss: 0.072\n",
            "[150:500] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.067\n",
            "[150:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:600] Took 11.68s\n",
            "[150:600] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.772, cum_G_cycle_loss: 0.072\n",
            "[150:600] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.069\n",
            "[150:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:700] Took 11.68s\n",
            "[150:700] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.077\n",
            "[150:700] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.070\n",
            "[150:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:800] Took 11.68s\n",
            "[150:800] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.774, cum_G_cycle_loss: 0.082\n",
            "[150:800] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.657, cum_F_cycle_loss: 0.067\n",
            "[150:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:900] Took 11.68s\n",
            "[150:900] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.761, cum_G_cycle_loss: 0.073\n",
            "[150:900] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.709, cum_F_cycle_loss: 0.070\n",
            "[150:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:1000] Took 11.68s\n",
            "[150:1000] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.080\n",
            "[150:1000] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.665, cum_F_cycle_loss: 0.069\n",
            "[150:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:1100] Took 11.68s\n",
            "[150:1100] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.075\n",
            "[150:1100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.746, cum_F_cycle_loss: 0.068\n",
            "[150:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[150:END] Completed epoch in 142.22160482406616s\n",
            "[150:1187] ep_D_X_loss: 0.087, ep_G_fool_loss: 0.734, ep_G_cycle_loss: 0.077\n",
            "[150:1187] ep_D_Y_loss: 0.077, ep_F_fool_loss: 0.697, ep_F_cycle_loss: 0.067\n",
            "[150:END] Completed eval in 1.2456657886505127s\n",
            "Updated G_opt learning rate from 0.00010099009900990099 to 9.900990099009902e-05\n",
            "Updated F_opt learning rate from 0.00010099009900990099 to 9.900990099009902e-05\n",
            "Updated D_X_opt learning rate from 0.00010099009900990099 to 9.900990099009902e-05\n",
            "Updated D_Y_opt learning rate from 0.00010099009900990099 to 9.900990099009902e-05\n",
            "[150:END] Saving models and training information\n",
            "[151:100] Took 13.85s\n",
            "[151:100] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.735, cum_G_cycle_loss: 0.074\n",
            "[151:100] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.683, cum_F_cycle_loss: 0.071\n",
            "[151:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:200] Took 11.68s\n",
            "[151:200] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.078\n",
            "[151:200] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.067\n",
            "[151:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:300] Took 11.69s\n",
            "[151:300] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.074\n",
            "[151:300] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.647, cum_F_cycle_loss: 0.068\n",
            "[151:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:400] Took 11.69s\n",
            "[151:400] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.073\n",
            "[151:400] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.721, cum_F_cycle_loss: 0.071\n",
            "[151:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:500] Took 11.68s\n",
            "[151:500] cum_D_X_loss: 0.071, cum_G_fool_loss: 0.704, cum_G_cycle_loss: 0.077\n",
            "[151:500] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.760, cum_F_cycle_loss: 0.069\n",
            "[151:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:600] Took 11.68s\n",
            "[151:600] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.780, cum_G_cycle_loss: 0.075\n",
            "[151:600] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.666, cum_F_cycle_loss: 0.067\n",
            "[151:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:700] Took 11.68s\n",
            "[151:700] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.733, cum_G_cycle_loss: 0.074\n",
            "[151:700] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.067\n",
            "[151:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:800] Took 11.69s\n",
            "[151:800] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.767, cum_G_cycle_loss: 0.076\n",
            "[151:800] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.754, cum_F_cycle_loss: 0.068\n",
            "[151:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:900] Took 11.68s\n",
            "[151:900] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.083\n",
            "[151:900] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.065\n",
            "[151:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:1000] Took 11.68s\n",
            "[151:1000] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.744, cum_G_cycle_loss: 0.076\n",
            "[151:1000] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.070\n",
            "[151:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:1100] Took 11.68s\n",
            "[151:1100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.729, cum_G_cycle_loss: 0.075\n",
            "[151:1100] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.704, cum_F_cycle_loss: 0.061\n",
            "[151:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[151:END] Completed epoch in 141.6160957813263s\n",
            "[151:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.744, ep_G_cycle_loss: 0.076\n",
            "[151:1187] ep_D_Y_loss: 0.076, ep_F_fool_loss: 0.706, ep_F_cycle_loss: 0.067\n",
            "[151:END] Completed eval in 1.2536451816558838s\n",
            "Updated G_opt learning rate from 9.900990099009902e-05 to 9.702970297029703e-05\n",
            "Updated F_opt learning rate from 9.900990099009902e-05 to 9.702970297029703e-05\n",
            "Updated D_X_opt learning rate from 9.900990099009902e-05 to 9.702970297029703e-05\n",
            "Updated D_Y_opt learning rate from 9.900990099009902e-05 to 9.702970297029703e-05\n",
            "[152:100] Took 13.89s\n",
            "[152:100] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.746, cum_G_cycle_loss: 0.074\n",
            "[152:100] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.676, cum_F_cycle_loss: 0.065\n",
            "[152:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:200] Took 11.68s\n",
            "[152:200] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.084\n",
            "[152:200] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.066\n",
            "[152:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:300] Took 11.68s\n",
            "[152:300] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.772, cum_G_cycle_loss: 0.076\n",
            "[152:300] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.657, cum_F_cycle_loss: 0.064\n",
            "[152:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:400] Took 11.68s\n",
            "[152:400] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.720, cum_G_cycle_loss: 0.073\n",
            "[152:400] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.067\n",
            "[152:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:500] Took 11.68s\n",
            "[152:500] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.072\n",
            "[152:500] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.065\n",
            "[152:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:600] Took 12.13s\n",
            "[152:600] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.076\n",
            "[152:600] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.065\n",
            "[152:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:700] Took 11.68s\n",
            "[152:700] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.073\n",
            "[152:700] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.067\n",
            "[152:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:800] Took 12.51s\n",
            "[152:800] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.732, cum_G_cycle_loss: 0.075\n",
            "[152:800] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.069\n",
            "[152:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:900] Took 11.68s\n",
            "[152:900] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.789, cum_G_cycle_loss: 0.077\n",
            "[152:900] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.067\n",
            "[152:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:1000] Took 11.70s\n",
            "[152:1000] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.769, cum_G_cycle_loss: 0.073\n",
            "[152:1000] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.064\n",
            "[152:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:1100] Took 11.69s\n",
            "[152:1100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.077\n",
            "[152:1100] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.686, cum_F_cycle_loss: 0.071\n",
            "[152:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[152:END] Completed epoch in 143.4491605758667s\n",
            "[152:1187] ep_D_X_loss: 0.086, ep_G_fool_loss: 0.749, ep_G_cycle_loss: 0.075\n",
            "[152:1187] ep_D_Y_loss: 0.075, ep_F_fool_loss: 0.690, ep_F_cycle_loss: 0.067\n",
            "[152:END] Completed eval in 1.2815721035003662s\n",
            "Updated G_opt learning rate from 9.702970297029703e-05 to 9.504950495049505e-05\n",
            "Updated F_opt learning rate from 9.702970297029703e-05 to 9.504950495049505e-05\n",
            "Updated D_X_opt learning rate from 9.702970297029703e-05 to 9.504950495049505e-05\n",
            "Updated D_Y_opt learning rate from 9.702970297029703e-05 to 9.504950495049505e-05\n",
            "[153:100] Took 14.08s\n",
            "[153:100] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.085\n",
            "[153:100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.068\n",
            "[153:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:200] Took 11.71s\n",
            "[153:200] cum_D_X_loss: 0.102, cum_G_fool_loss: 0.718, cum_G_cycle_loss: 0.073\n",
            "[153:200] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.664, cum_F_cycle_loss: 0.068\n",
            "[153:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:300] Took 11.70s\n",
            "[153:300] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.073\n",
            "[153:300] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.728, cum_F_cycle_loss: 0.069\n",
            "[153:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:400] Took 11.87s\n",
            "[153:400] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.074\n",
            "[153:400] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.668, cum_F_cycle_loss: 0.067\n",
            "[153:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:500] Took 12.16s\n",
            "[153:500] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.077\n",
            "[153:500] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.066\n",
            "[153:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:600] Took 11.72s\n",
            "[153:600] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.074\n",
            "[153:600] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.066\n",
            "[153:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:700] Took 11.76s\n",
            "[153:700] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.074\n",
            "[153:700] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.672, cum_F_cycle_loss: 0.071\n",
            "[153:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:800] Took 11.69s\n",
            "[153:800] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.771, cum_G_cycle_loss: 0.082\n",
            "[153:800] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.669, cum_F_cycle_loss: 0.066\n",
            "[153:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:900] Took 11.69s\n",
            "[153:900] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.698, cum_G_cycle_loss: 0.074\n",
            "[153:900] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.070\n",
            "[153:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:1000] Took 11.69s\n",
            "[153:1000] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.075\n",
            "[153:1000] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.062\n",
            "[153:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:1100] Took 11.69s\n",
            "[153:1100] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.771, cum_G_cycle_loss: 0.075\n",
            "[153:1100] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.068\n",
            "[153:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[153:END] Completed epoch in 142.66615962982178s\n",
            "[153:1187] ep_D_X_loss: 0.089, ep_G_fool_loss: 0.734, ep_G_cycle_loss: 0.076\n",
            "[153:1187] ep_D_Y_loss: 0.077, ep_F_fool_loss: 0.694, ep_F_cycle_loss: 0.068\n",
            "[153:END] Completed eval in 1.2656424045562744s\n",
            "Updated G_opt learning rate from 9.504950495049505e-05 to 9.306930693069307e-05\n",
            "Updated F_opt learning rate from 9.504950495049505e-05 to 9.306930693069307e-05\n",
            "Updated D_X_opt learning rate from 9.504950495049505e-05 to 9.306930693069307e-05\n",
            "Updated D_Y_opt learning rate from 9.504950495049505e-05 to 9.306930693069307e-05\n",
            "[154:100] Took 14.00s\n",
            "[154:100] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.751, cum_G_cycle_loss: 0.075\n",
            "[154:100] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.067\n",
            "[154:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:200] Took 11.68s\n",
            "[154:200] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.733, cum_G_cycle_loss: 0.079\n",
            "[154:200] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.067\n",
            "[154:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:300] Took 11.69s\n",
            "[154:300] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.777, cum_G_cycle_loss: 0.073\n",
            "[154:300] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.065\n",
            "[154:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:400] Took 11.69s\n",
            "[154:400] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.782, cum_G_cycle_loss: 0.073\n",
            "[154:400] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.066\n",
            "[154:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:500] Took 11.69s\n",
            "[154:500] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.699, cum_G_cycle_loss: 0.073\n",
            "[154:500] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.704, cum_F_cycle_loss: 0.069\n",
            "[154:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:600] Took 11.68s\n",
            "[154:600] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.075\n",
            "[154:600] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.066\n",
            "[154:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:700] Took 11.69s\n",
            "[154:700] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.073\n",
            "[154:700] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.067\n",
            "[154:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:800] Took 11.69s\n",
            "[154:800] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.720, cum_G_cycle_loss: 0.069\n",
            "[154:800] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.741, cum_F_cycle_loss: 0.067\n",
            "[154:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:900] Took 11.69s\n",
            "[154:900] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.798, cum_G_cycle_loss: 0.075\n",
            "[154:900] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.062\n",
            "[154:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:1000] Took 11.68s\n",
            "[154:1000] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.070\n",
            "[154:1000] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.067\n",
            "[154:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:1100] Took 11.69s\n",
            "[154:1100] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.735, cum_G_cycle_loss: 0.076\n",
            "[154:1100] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.686, cum_F_cycle_loss: 0.065\n",
            "[154:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[154:END] Completed epoch in 141.80324053764343s\n",
            "[154:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.749, ep_G_cycle_loss: 0.074\n",
            "[154:1187] ep_D_Y_loss: 0.074, ep_F_fool_loss: 0.697, ep_F_cycle_loss: 0.066\n",
            "[154:END] Completed eval in 1.2835659980773926s\n",
            "Updated G_opt learning rate from 9.306930693069307e-05 to 9.108910891089108e-05\n",
            "Updated F_opt learning rate from 9.306930693069307e-05 to 9.108910891089108e-05\n",
            "Updated D_X_opt learning rate from 9.306930693069307e-05 to 9.108910891089108e-05\n",
            "Updated D_Y_opt learning rate from 9.306930693069307e-05 to 9.108910891089108e-05\n",
            "[155:100] Took 13.90s\n",
            "[155:100] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.819, cum_G_cycle_loss: 0.074\n",
            "[155:100] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.656, cum_F_cycle_loss: 0.067\n",
            "[155:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:200] Took 11.68s\n",
            "[155:200] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.781, cum_G_cycle_loss: 0.072\n",
            "[155:200] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.067\n",
            "[155:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:300] Took 11.67s\n",
            "[155:300] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.072\n",
            "[155:300] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.069\n",
            "[155:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:400] Took 11.69s\n",
            "[155:400] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.073\n",
            "[155:400] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.682, cum_F_cycle_loss: 0.067\n",
            "[155:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:500] Took 11.69s\n",
            "[155:500] cum_D_X_loss: 0.071, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.081\n",
            "[155:500] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.068\n",
            "[155:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:600] Took 11.68s\n",
            "[155:600] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.713, cum_G_cycle_loss: 0.075\n",
            "[155:600] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.069\n",
            "[155:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:700] Took 11.68s\n",
            "[155:700] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.077\n",
            "[155:700] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.740, cum_F_cycle_loss: 0.068\n",
            "[155:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:800] Took 11.69s\n",
            "[155:800] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.081\n",
            "[155:800] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.704, cum_F_cycle_loss: 0.066\n",
            "[155:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:900] Took 11.69s\n",
            "[155:900] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.078\n",
            "[155:900] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.680, cum_F_cycle_loss: 0.062\n",
            "[155:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:1000] Took 11.68s\n",
            "[155:1000] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.075\n",
            "[155:1000] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.063\n",
            "[155:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:1100] Took 11.68s\n",
            "[155:1100] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.781, cum_G_cycle_loss: 0.074\n",
            "[155:1100] cum_D_Y_loss: 0.057, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.064\n",
            "[155:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[155:END] Completed epoch in 141.69029545783997s\n",
            "[155:1187] ep_D_X_loss: 0.087, ep_G_fool_loss: 0.758, ep_G_cycle_loss: 0.075\n",
            "[155:1187] ep_D_Y_loss: 0.071, ep_F_fool_loss: 0.689, ep_F_cycle_loss: 0.066\n",
            "[155:END] Completed eval in 1.2875831127166748s\n",
            "Updated G_opt learning rate from 9.108910891089108e-05 to 8.91089108910891e-05\n",
            "Updated F_opt learning rate from 9.108910891089108e-05 to 8.91089108910891e-05\n",
            "Updated D_X_opt learning rate from 9.108910891089108e-05 to 8.91089108910891e-05\n",
            "Updated D_Y_opt learning rate from 9.108910891089108e-05 to 8.91089108910891e-05\n",
            "[155:END] Saving models and training information\n",
            "[156:100] Took 14.18s\n",
            "[156:100] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.767, cum_G_cycle_loss: 0.081\n",
            "[156:100] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.734, cum_F_cycle_loss: 0.067\n",
            "[156:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:200] Took 12.09s\n",
            "[156:200] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.750, cum_G_cycle_loss: 0.082\n",
            "[156:200] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.756, cum_F_cycle_loss: 0.068\n",
            "[156:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:300] Took 11.70s\n",
            "[156:300] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.071\n",
            "[156:300] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.714, cum_F_cycle_loss: 0.063\n",
            "[156:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:400] Took 11.81s\n",
            "[156:400] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.677, cum_G_cycle_loss: 0.082\n",
            "[156:400] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.657, cum_F_cycle_loss: 0.067\n",
            "[156:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:500] Took 11.70s\n",
            "[156:500] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.077\n",
            "[156:500] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.715, cum_F_cycle_loss: 0.066\n",
            "[156:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:600] Took 12.22s\n",
            "[156:600] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.698, cum_G_cycle_loss: 0.071\n",
            "[156:600] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.757, cum_F_cycle_loss: 0.066\n",
            "[156:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:700] Took 11.95s\n",
            "[156:700] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.774, cum_G_cycle_loss: 0.071\n",
            "[156:700] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.071\n",
            "[156:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:800] Took 11.94s\n",
            "[156:800] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.794, cum_G_cycle_loss: 0.070\n",
            "[156:800] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.068\n",
            "[156:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:900] Took 11.79s\n",
            "[156:900] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.076\n",
            "[156:900] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.711, cum_F_cycle_loss: 0.068\n",
            "[156:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:1000] Took 11.80s\n",
            "[156:1000] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.073\n",
            "[156:1000] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.726, cum_F_cycle_loss: 0.065\n",
            "[156:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:1100] Took 11.84s\n",
            "[156:1100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.749, cum_G_cycle_loss: 0.074\n",
            "[156:1100] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.063\n",
            "[156:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[156:END] Completed epoch in 144.0328278541565s\n",
            "[156:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.738, ep_G_cycle_loss: 0.075\n",
            "[156:1187] ep_D_Y_loss: 0.075, ep_F_fool_loss: 0.708, ep_F_cycle_loss: 0.067\n",
            "[156:END] Completed eval in 1.3144829273223877s\n",
            "Updated G_opt learning rate from 8.91089108910891e-05 to 8.712871287128714e-05\n",
            "Updated F_opt learning rate from 8.91089108910891e-05 to 8.712871287128714e-05\n",
            "Updated D_X_opt learning rate from 8.91089108910891e-05 to 8.712871287128714e-05\n",
            "Updated D_Y_opt learning rate from 8.91089108910891e-05 to 8.712871287128714e-05\n",
            "[157:100] Took 13.67s\n",
            "[157:100] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.807, cum_G_cycle_loss: 0.073\n",
            "[157:100] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.708, cum_F_cycle_loss: 0.072\n",
            "[157:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:200] Took 12.01s\n",
            "[157:200] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.774, cum_G_cycle_loss: 0.073\n",
            "[157:200] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.064\n",
            "[157:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:300] Took 11.84s\n",
            "[157:300] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.765, cum_G_cycle_loss: 0.071\n",
            "[157:300] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.064\n",
            "[157:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:400] Took 11.72s\n",
            "[157:400] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.076\n",
            "[157:400] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.075\n",
            "[157:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:500] Took 11.78s\n",
            "[157:500] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.776, cum_G_cycle_loss: 0.080\n",
            "[157:500] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.065\n",
            "[157:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:600] Took 12.09s\n",
            "[157:600] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.759, cum_G_cycle_loss: 0.075\n",
            "[157:600] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.740, cum_F_cycle_loss: 0.060\n",
            "[157:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:700] Took 12.05s\n",
            "[157:700] cum_D_X_loss: 0.068, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.076\n",
            "[157:700] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.735, cum_F_cycle_loss: 0.064\n",
            "[157:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:800] Took 12.01s\n",
            "[157:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.799, cum_G_cycle_loss: 0.072\n",
            "[157:800] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.066\n",
            "[157:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:900] Took 11.90s\n",
            "[157:900] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.776, cum_G_cycle_loss: 0.071\n",
            "[157:900] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.657, cum_F_cycle_loss: 0.066\n",
            "[157:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:1000] Took 11.87s\n",
            "[157:1000] cum_D_X_loss: 0.102, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.075\n",
            "[157:1000] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.626, cum_F_cycle_loss: 0.067\n",
            "[157:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:1100] Took 11.88s\n",
            "[157:1100] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.759, cum_G_cycle_loss: 0.067\n",
            "[157:1100] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.711, cum_F_cycle_loss: 0.068\n",
            "[157:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[157:END] Completed epoch in 143.80728816986084s\n",
            "[157:1187] ep_D_X_loss: 0.084, ep_G_fool_loss: 0.770, ep_G_cycle_loss: 0.074\n",
            "[157:1187] ep_D_Y_loss: 0.072, ep_F_fool_loss: 0.692, ep_F_cycle_loss: 0.066\n",
            "[157:END] Completed eval in 1.3373925685882568s\n",
            "Updated G_opt learning rate from 8.712871287128714e-05 to 8.514851485148517e-05\n",
            "Updated F_opt learning rate from 8.712871287128714e-05 to 8.514851485148517e-05\n",
            "Updated D_X_opt learning rate from 8.712871287128714e-05 to 8.514851485148517e-05\n",
            "Updated D_Y_opt learning rate from 8.712871287128714e-05 to 8.514851485148517e-05\n",
            "[158:100] Took 14.18s\n",
            "[158:100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.696, cum_G_cycle_loss: 0.077\n",
            "[158:100] cum_D_Y_loss: 0.088, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.067\n",
            "[158:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:200] Took 11.80s\n",
            "[158:200] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.810, cum_G_cycle_loss: 0.079\n",
            "[158:200] cum_D_Y_loss: 0.057, cum_F_fool_loss: 0.759, cum_F_cycle_loss: 0.072\n",
            "[158:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:300] Took 12.08s\n",
            "[158:300] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.708, cum_G_cycle_loss: 0.073\n",
            "[158:300] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.664, cum_F_cycle_loss: 0.072\n",
            "[158:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:400] Took 12.02s\n",
            "[158:400] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.080\n",
            "[158:400] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.067\n",
            "[158:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:500] Took 12.16s\n",
            "[158:500] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.714, cum_G_cycle_loss: 0.072\n",
            "[158:500] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.066\n",
            "[158:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:600] Took 11.76s\n",
            "[158:600] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.749, cum_G_cycle_loss: 0.076\n",
            "[158:600] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.065\n",
            "[158:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:700] Took 12.15s\n",
            "[158:700] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.072\n",
            "[158:700] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.758, cum_F_cycle_loss: 0.067\n",
            "[158:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:800] Took 11.87s\n",
            "[158:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.733, cum_G_cycle_loss: 0.076\n",
            "[158:800] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.683, cum_F_cycle_loss: 0.069\n",
            "[158:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:900] Took 11.93s\n",
            "[158:900] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.708, cum_G_cycle_loss: 0.072\n",
            "[158:900] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.068\n",
            "[158:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:1000] Took 11.98s\n",
            "[158:1000] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.077\n",
            "[158:1000] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.688, cum_F_cycle_loss: 0.066\n",
            "[158:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:1100] Took 12.14s\n",
            "[158:1100] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.720, cum_G_cycle_loss: 0.073\n",
            "[158:1100] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.066\n",
            "[158:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[158:END] Completed epoch in 145.07524585723877s\n",
            "[158:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.729, ep_G_cycle_loss: 0.076\n",
            "[158:1187] ep_D_Y_loss: 0.077, ep_F_fool_loss: 0.704, ep_F_cycle_loss: 0.068\n",
            "[158:END] Completed eval in 1.3463640213012695s\n",
            "Updated G_opt learning rate from 8.514851485148517e-05 to 8.316831683168318e-05\n",
            "Updated F_opt learning rate from 8.514851485148517e-05 to 8.316831683168318e-05\n",
            "Updated D_X_opt learning rate from 8.514851485148517e-05 to 8.316831683168318e-05\n",
            "Updated D_Y_opt learning rate from 8.514851485148517e-05 to 8.316831683168318e-05\n",
            "[159:100] Took 14.66s\n",
            "[159:100] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.071\n",
            "[159:100] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.715, cum_F_cycle_loss: 0.070\n",
            "[159:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:200] Took 11.91s\n",
            "[159:200] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.073\n",
            "[159:200] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.070\n",
            "[159:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:300] Took 12.66s\n",
            "[159:300] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.076\n",
            "[159:300] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.749, cum_F_cycle_loss: 0.067\n",
            "[159:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:400] Took 12.09s\n",
            "[159:400] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.697, cum_G_cycle_loss: 0.075\n",
            "[159:400] cum_D_Y_loss: 0.103, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.069\n",
            "[159:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:500] Took 12.27s\n",
            "[159:500] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.072\n",
            "[159:500] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.067\n",
            "[159:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:600] Took 12.16s\n",
            "[159:600] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.068\n",
            "[159:600] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.063\n",
            "[159:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:700] Took 12.17s\n",
            "[159:700] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.782, cum_G_cycle_loss: 0.072\n",
            "[159:700] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.067\n",
            "[159:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:800] Took 12.29s\n",
            "[159:800] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.074\n",
            "[159:800] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.654, cum_F_cycle_loss: 0.066\n",
            "[159:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:900] Took 11.87s\n",
            "[159:900] cum_D_X_loss: 0.070, cum_G_fool_loss: 0.716, cum_G_cycle_loss: 0.071\n",
            "[159:900] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.067\n",
            "[159:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:1000] Took 12.19s\n",
            "[159:1000] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.070\n",
            "[159:1000] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.666, cum_F_cycle_loss: 0.068\n",
            "[159:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:1100] Took 12.04s\n",
            "[159:1100] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.792, cum_G_cycle_loss: 0.073\n",
            "[159:1100] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.672, cum_F_cycle_loss: 0.063\n",
            "[159:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[159:END] Completed epoch in 147.40692114830017s\n",
            "[159:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.743, ep_G_cycle_loss: 0.072\n",
            "[159:1187] ep_D_Y_loss: 0.071, ep_F_fool_loss: 0.693, ep_F_cycle_loss: 0.067\n",
            "[159:END] Completed eval in 1.349363088607788s\n",
            "Updated G_opt learning rate from 8.316831683168318e-05 to 8.11881188118812e-05\n",
            "Updated F_opt learning rate from 8.316831683168318e-05 to 8.11881188118812e-05\n",
            "Updated D_X_opt learning rate from 8.316831683168318e-05 to 8.11881188118812e-05\n",
            "Updated D_Y_opt learning rate from 8.316831683168318e-05 to 8.11881188118812e-05\n",
            "[160:100] Took 14.21s\n",
            "[160:100] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.766, cum_G_cycle_loss: 0.071\n",
            "[160:100] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.735, cum_F_cycle_loss: 0.069\n",
            "[160:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:200] Took 12.36s\n",
            "[160:200] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.789, cum_G_cycle_loss: 0.071\n",
            "[160:200] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.070\n",
            "[160:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:300] Took 11.88s\n",
            "[160:300] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.769, cum_G_cycle_loss: 0.072\n",
            "[160:300] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.068\n",
            "[160:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:400] Took 12.01s\n",
            "[160:400] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.761, cum_G_cycle_loss: 0.074\n",
            "[160:400] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.677, cum_F_cycle_loss: 0.068\n",
            "[160:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:500] Took 12.42s\n",
            "[160:500] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.075\n",
            "[160:500] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.731, cum_F_cycle_loss: 0.066\n",
            "[160:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:600] Took 12.14s\n",
            "[160:600] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.075\n",
            "[160:600] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.070\n",
            "[160:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:700] Took 12.22s\n",
            "[160:700] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.072\n",
            "[160:700] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.066\n",
            "[160:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:800] Took 11.68s\n",
            "[160:800] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.072\n",
            "[160:800] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.709, cum_F_cycle_loss: 0.067\n",
            "[160:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:900] Took 11.68s\n",
            "[160:900] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.075\n",
            "[160:900] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.671, cum_F_cycle_loss: 0.065\n",
            "[160:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:1000] Took 11.68s\n",
            "[160:1000] cum_D_X_loss: 0.071, cum_G_fool_loss: 0.786, cum_G_cycle_loss: 0.081\n",
            "[160:1000] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.065\n",
            "[160:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:1100] Took 11.68s\n",
            "[160:1100] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.071\n",
            "[160:1100] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.742, cum_F_cycle_loss: 0.069\n",
            "[160:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[160:END] Completed epoch in 144.85877752304077s\n",
            "[160:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.758, ep_G_cycle_loss: 0.074\n",
            "[160:1187] ep_D_Y_loss: 0.071, ep_F_fool_loss: 0.706, ep_F_cycle_loss: 0.067\n",
            "[160:END] Completed eval in 1.3314342498779297s\n",
            "Updated G_opt learning rate from 8.11881188118812e-05 to 7.920792079207921e-05\n",
            "Updated F_opt learning rate from 8.11881188118812e-05 to 7.920792079207921e-05\n",
            "Updated D_X_opt learning rate from 8.11881188118812e-05 to 7.920792079207921e-05\n",
            "Updated D_Y_opt learning rate from 8.11881188118812e-05 to 7.920792079207921e-05\n",
            "[160:END] Saving models and training information\n",
            "[161:100] Took 13.89s\n",
            "[161:100] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.071\n",
            "[161:100] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.670, cum_F_cycle_loss: 0.065\n",
            "[161:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:200] Took 11.68s\n",
            "[161:200] cum_D_X_loss: 0.071, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.070\n",
            "[161:200] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.067\n",
            "[161:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:300] Took 11.68s\n",
            "[161:300] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.790, cum_G_cycle_loss: 0.069\n",
            "[161:300] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.062\n",
            "[161:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:400] Took 11.68s\n",
            "[161:400] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.068\n",
            "[161:400] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.064\n",
            "[161:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:500] Took 11.67s\n",
            "[161:500] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.729, cum_G_cycle_loss: 0.066\n",
            "[161:500] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.715, cum_F_cycle_loss: 0.065\n",
            "[161:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:600] Took 11.68s\n",
            "[161:600] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.076\n",
            "[161:600] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.067\n",
            "[161:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:700] Took 11.68s\n",
            "[161:700] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.782, cum_G_cycle_loss: 0.072\n",
            "[161:700] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.746, cum_F_cycle_loss: 0.065\n",
            "[161:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:800] Took 11.68s\n",
            "[161:800] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.781, cum_G_cycle_loss: 0.071\n",
            "[161:800] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.666, cum_F_cycle_loss: 0.066\n",
            "[161:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:900] Took 11.86s\n",
            "[161:900] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.702, cum_G_cycle_loss: 0.072\n",
            "[161:900] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.068\n",
            "[161:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:1000] Took 12.05s\n",
            "[161:1000] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.069\n",
            "[161:1000] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.688, cum_F_cycle_loss: 0.065\n",
            "[161:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:1100] Took 11.81s\n",
            "[161:1100] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.746, cum_G_cycle_loss: 0.070\n",
            "[161:1100] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.067\n",
            "[161:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[161:END] Completed epoch in 142.29678916931152s\n",
            "[161:1187] ep_D_X_loss: 0.084, ep_G_fool_loss: 0.748, ep_G_cycle_loss: 0.071\n",
            "[161:1187] ep_D_Y_loss: 0.074, ep_F_fool_loss: 0.696, ep_F_cycle_loss: 0.065\n",
            "[161:END] Completed eval in 1.368340015411377s\n",
            "Updated G_opt learning rate from 7.920792079207921e-05 to 7.722772277227723e-05\n",
            "Updated F_opt learning rate from 7.920792079207921e-05 to 7.722772277227723e-05\n",
            "Updated D_X_opt learning rate from 7.920792079207921e-05 to 7.722772277227723e-05\n",
            "Updated D_Y_opt learning rate from 7.920792079207921e-05 to 7.722772277227723e-05\n",
            "[162:100] Took 13.92s\n",
            "[162:100] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.771, cum_G_cycle_loss: 0.070\n",
            "[162:100] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.688, cum_F_cycle_loss: 0.064\n",
            "[162:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:200] Took 11.68s\n",
            "[162:200] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.750, cum_G_cycle_loss: 0.075\n",
            "[162:200] cum_D_Y_loss: 0.086, cum_F_fool_loss: 0.709, cum_F_cycle_loss: 0.068\n",
            "[162:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:300] Took 11.68s\n",
            "[162:300] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.076\n",
            "[162:300] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.065\n",
            "[162:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:400] Took 11.68s\n",
            "[162:400] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.804, cum_G_cycle_loss: 0.066\n",
            "[162:400] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.668, cum_F_cycle_loss: 0.066\n",
            "[162:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:500] Took 11.81s\n",
            "[162:500] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.073\n",
            "[162:500] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.067\n",
            "[162:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:600] Took 11.68s\n",
            "[162:600] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.740, cum_G_cycle_loss: 0.074\n",
            "[162:600] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.065\n",
            "[162:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:700] Took 11.68s\n",
            "[162:700] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.735, cum_G_cycle_loss: 0.071\n",
            "[162:700] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.672, cum_F_cycle_loss: 0.066\n",
            "[162:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:800] Took 11.68s\n",
            "[162:800] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.784, cum_G_cycle_loss: 0.070\n",
            "[162:800] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.066\n",
            "[162:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:900] Took 11.68s\n",
            "[162:900] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.075\n",
            "[162:900] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.062\n",
            "[162:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:1000] Took 11.68s\n",
            "[162:1000] cum_D_X_loss: 0.100, cum_G_fool_loss: 0.711, cum_G_cycle_loss: 0.071\n",
            "[162:1000] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.659, cum_F_cycle_loss: 0.065\n",
            "[162:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:1100] Took 11.68s\n",
            "[162:1100] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.737, cum_G_cycle_loss: 0.072\n",
            "[162:1100] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.719, cum_F_cycle_loss: 0.062\n",
            "[162:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[162:END] Completed epoch in 141.77804613113403s\n",
            "[162:1187] ep_D_X_loss: 0.086, ep_G_fool_loss: 0.751, ep_G_cycle_loss: 0.072\n",
            "[162:1187] ep_D_Y_loss: 0.071, ep_F_fool_loss: 0.690, ep_F_cycle_loss: 0.065\n",
            "[162:END] Completed eval in 1.413215160369873s\n",
            "Updated G_opt learning rate from 7.722772277227723e-05 to 7.524752475247526e-05\n",
            "Updated F_opt learning rate from 7.722772277227723e-05 to 7.524752475247526e-05\n",
            "Updated D_X_opt learning rate from 7.722772277227723e-05 to 7.524752475247526e-05\n",
            "Updated D_Y_opt learning rate from 7.722772277227723e-05 to 7.524752475247526e-05\n",
            "[163:100] Took 13.90s\n",
            "[163:100] cum_D_X_loss: 0.102, cum_G_fool_loss: 0.740, cum_G_cycle_loss: 0.070\n",
            "[163:100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.709, cum_F_cycle_loss: 0.069\n",
            "[163:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:200] Took 11.68s\n",
            "[163:200] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.070\n",
            "[163:200] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.067\n",
            "[163:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:300] Took 11.68s\n",
            "[163:300] cum_D_X_loss: 0.107, cum_G_fool_loss: 0.715, cum_G_cycle_loss: 0.075\n",
            "[163:300] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.636, cum_F_cycle_loss: 0.067\n",
            "[163:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:400] Took 11.68s\n",
            "[163:400] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.074\n",
            "[163:400] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.658, cum_F_cycle_loss: 0.064\n",
            "[163:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:500] Took 11.68s\n",
            "[163:500] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.765, cum_G_cycle_loss: 0.068\n",
            "[163:500] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.679, cum_F_cycle_loss: 0.067\n",
            "[163:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:600] Took 11.68s\n",
            "[163:600] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.732, cum_G_cycle_loss: 0.072\n",
            "[163:600] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.070\n",
            "[163:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:700] Took 11.68s\n",
            "[163:700] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.076\n",
            "[163:700] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.063\n",
            "[163:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:800] Took 11.68s\n",
            "[163:800] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.069\n",
            "[163:800] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.739, cum_F_cycle_loss: 0.065\n",
            "[163:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:900] Took 11.68s\n",
            "[163:900] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.751, cum_G_cycle_loss: 0.075\n",
            "[163:900] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.731, cum_F_cycle_loss: 0.065\n",
            "[163:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:1000] Took 11.68s\n",
            "[163:1000] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.073\n",
            "[163:1000] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.753, cum_F_cycle_loss: 0.064\n",
            "[163:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:1100] Took 11.68s\n",
            "[163:1100] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.075\n",
            "[163:1100] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.740, cum_F_cycle_loss: 0.068\n",
            "[163:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[163:END] Completed epoch in 141.63170742988586s\n",
            "[163:1187] ep_D_X_loss: 0.087, ep_G_fool_loss: 0.751, ep_G_cycle_loss: 0.073\n",
            "[163:1187] ep_D_Y_loss: 0.072, ep_F_fool_loss: 0.702, ep_F_cycle_loss: 0.066\n",
            "[163:END] Completed eval in 1.37931227684021s\n",
            "Updated G_opt learning rate from 7.524752475247526e-05 to 7.326732673267327e-05\n",
            "Updated F_opt learning rate from 7.524752475247526e-05 to 7.326732673267327e-05\n",
            "Updated D_X_opt learning rate from 7.524752475247526e-05 to 7.326732673267327e-05\n",
            "Updated D_Y_opt learning rate from 7.524752475247526e-05 to 7.326732673267327e-05\n",
            "[164:100] Took 13.91s\n",
            "[164:100] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.712, cum_G_cycle_loss: 0.070\n",
            "[164:100] cum_D_Y_loss: 0.089, cum_F_fool_loss: 0.736, cum_F_cycle_loss: 0.066\n",
            "[164:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:200] Took 11.68s\n",
            "[164:200] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.731, cum_G_cycle_loss: 0.070\n",
            "[164:200] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.066\n",
            "[164:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:300] Took 11.68s\n",
            "[164:300] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.073\n",
            "[164:300] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.065\n",
            "[164:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:400] Took 11.68s\n",
            "[164:400] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.081\n",
            "[164:400] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.068\n",
            "[164:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:500] Took 11.68s\n",
            "[164:500] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.073\n",
            "[164:500] cum_D_Y_loss: 0.058, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.066\n",
            "[164:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:600] Took 11.67s\n",
            "[164:600] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.730, cum_G_cycle_loss: 0.073\n",
            "[164:600] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.664, cum_F_cycle_loss: 0.066\n",
            "[164:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:700] Took 11.68s\n",
            "[164:700] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.069\n",
            "[164:700] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.757, cum_F_cycle_loss: 0.064\n",
            "[164:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:800] Took 11.68s\n",
            "[164:800] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.071\n",
            "[164:800] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.065\n",
            "[164:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:900] Took 11.68s\n",
            "[164:900] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.691, cum_G_cycle_loss: 0.071\n",
            "[164:900] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.066\n",
            "[164:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:1000] Took 11.67s\n",
            "[164:1000] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.731, cum_G_cycle_loss: 0.073\n",
            "[164:1000] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.682, cum_F_cycle_loss: 0.064\n",
            "[164:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:1100] Took 11.68s\n",
            "[164:1100] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.765, cum_G_cycle_loss: 0.069\n",
            "[164:1100] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.741, cum_F_cycle_loss: 0.065\n",
            "[164:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[164:END] Completed epoch in 141.61932516098022s\n",
            "[164:1187] ep_D_X_loss: 0.088, ep_G_fool_loss: 0.734, ep_G_cycle_loss: 0.072\n",
            "[164:1187] ep_D_Y_loss: 0.076, ep_F_fool_loss: 0.705, ep_F_cycle_loss: 0.066\n",
            "[164:END] Completed eval in 1.386291742324829s\n",
            "Updated G_opt learning rate from 7.326732673267327e-05 to 7.128712871287129e-05\n",
            "Updated F_opt learning rate from 7.326732673267327e-05 to 7.128712871287129e-05\n",
            "Updated D_X_opt learning rate from 7.326732673267327e-05 to 7.128712871287129e-05\n",
            "Updated D_Y_opt learning rate from 7.326732673267327e-05 to 7.128712871287129e-05\n",
            "[165:100] Took 14.21s\n",
            "[165:100] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.810, cum_G_cycle_loss: 0.073\n",
            "[165:100] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.069\n",
            "[165:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:200] Took 11.68s\n",
            "[165:200] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.807, cum_G_cycle_loss: 0.067\n",
            "[165:200] cum_D_Y_loss: 0.054, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.070\n",
            "[165:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:300] Took 11.68s\n",
            "[165:300] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.075\n",
            "[165:300] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.068\n",
            "[165:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:400] Took 11.69s\n",
            "[165:400] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.740, cum_G_cycle_loss: 0.069\n",
            "[165:400] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.064\n",
            "[165:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:500] Took 11.68s\n",
            "[165:500] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.714, cum_G_cycle_loss: 0.073\n",
            "[165:500] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.068\n",
            "[165:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:600] Took 11.68s\n",
            "[165:600] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.717, cum_G_cycle_loss: 0.077\n",
            "[165:600] cum_D_Y_loss: 0.091, cum_F_fool_loss: 0.667, cum_F_cycle_loss: 0.066\n",
            "[165:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:700] Took 11.67s\n",
            "[165:700] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.720, cum_G_cycle_loss: 0.069\n",
            "[165:700] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.711, cum_F_cycle_loss: 0.068\n",
            "[165:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:800] Took 11.68s\n",
            "[165:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.705, cum_G_cycle_loss: 0.072\n",
            "[165:800] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.721, cum_F_cycle_loss: 0.068\n",
            "[165:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:900] Took 11.68s\n",
            "[165:900] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.818, cum_G_cycle_loss: 0.074\n",
            "[165:900] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.066\n",
            "[165:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:1000] Took 11.68s\n",
            "[165:1000] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.067\n",
            "[165:1000] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.658, cum_F_cycle_loss: 0.067\n",
            "[165:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:1100] Took 11.67s\n",
            "[165:1100] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.071\n",
            "[165:1100] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.653, cum_F_cycle_loss: 0.064\n",
            "[165:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[165:END] Completed epoch in 141.9414963722229s\n",
            "[165:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.753, ep_G_cycle_loss: 0.071\n",
            "[165:1187] ep_D_Y_loss: 0.072, ep_F_fool_loss: 0.695, ep_F_cycle_loss: 0.067\n",
            "[165:END] Completed eval in 1.3892555236816406s\n",
            "Updated G_opt learning rate from 7.128712871287129e-05 to 6.93069306930693e-05\n",
            "Updated F_opt learning rate from 7.128712871287129e-05 to 6.93069306930693e-05\n",
            "Updated D_X_opt learning rate from 7.128712871287129e-05 to 6.93069306930693e-05\n",
            "Updated D_Y_opt learning rate from 7.128712871287129e-05 to 6.93069306930693e-05\n",
            "[165:END] Saving models and training information\n",
            "[166:100] Took 14.26s\n",
            "[166:100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.737, cum_G_cycle_loss: 0.074\n",
            "[166:100] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.721, cum_F_cycle_loss: 0.062\n",
            "[166:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:200] Took 11.86s\n",
            "[166:200] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.756, cum_G_cycle_loss: 0.071\n",
            "[166:200] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.070\n",
            "[166:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:300] Took 11.74s\n",
            "[166:300] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.073\n",
            "[166:300] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.063\n",
            "[166:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:400] Took 11.67s\n",
            "[166:400] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.717, cum_G_cycle_loss: 0.069\n",
            "[166:400] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.065\n",
            "[166:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:500] Took 11.76s\n",
            "[166:500] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.070\n",
            "[166:500] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.066\n",
            "[166:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:600] Took 11.81s\n",
            "[166:600] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.072\n",
            "[166:600] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.067\n",
            "[166:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:700] Took 11.68s\n",
            "[166:700] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.075\n",
            "[166:700] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.676, cum_F_cycle_loss: 0.068\n",
            "[166:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:800] Took 11.68s\n",
            "[166:800] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.077\n",
            "[166:800] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.682, cum_F_cycle_loss: 0.064\n",
            "[166:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:900] Took 11.68s\n",
            "[166:900] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.677, cum_G_cycle_loss: 0.070\n",
            "[166:900] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.726, cum_F_cycle_loss: 0.064\n",
            "[166:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:1000] Took 11.68s\n",
            "[166:1000] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.073\n",
            "[166:1000] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.064\n",
            "[166:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:1100] Took 11.75s\n",
            "[166:1100] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.072\n",
            "[166:1100] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.065\n",
            "[166:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[166:END] Completed epoch in 142.54018783569336s\n",
            "[166:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.740, ep_G_cycle_loss: 0.072\n",
            "[166:1187] ep_D_Y_loss: 0.070, ep_F_fool_loss: 0.706, ep_F_cycle_loss: 0.065\n",
            "[166:END] Completed eval in 1.451089859008789s\n",
            "Updated G_opt learning rate from 6.93069306930693e-05 to 6.732673267326732e-05\n",
            "Updated F_opt learning rate from 6.93069306930693e-05 to 6.732673267326732e-05\n",
            "Updated D_X_opt learning rate from 6.93069306930693e-05 to 6.732673267326732e-05\n",
            "Updated D_Y_opt learning rate from 6.93069306930693e-05 to 6.732673267326732e-05\n",
            "[167:100] Took 14.18s\n",
            "[167:100] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.734, cum_G_cycle_loss: 0.072\n",
            "[167:100] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.661, cum_F_cycle_loss: 0.064\n",
            "[167:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:200] Took 11.68s\n",
            "[167:200] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.066\n",
            "[167:200] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.068\n",
            "[167:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:300] Took 11.68s\n",
            "[167:300] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.071\n",
            "[167:300] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.682, cum_F_cycle_loss: 0.067\n",
            "[167:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:400] Took 12.16s\n",
            "[167:400] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.079\n",
            "[167:400] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.071\n",
            "[167:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:500] Took 11.76s\n",
            "[167:500] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.796, cum_G_cycle_loss: 0.070\n",
            "[167:500] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.738, cum_F_cycle_loss: 0.068\n",
            "[167:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:600] Took 11.69s\n",
            "[167:600] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.073\n",
            "[167:600] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.738, cum_F_cycle_loss: 0.070\n",
            "[167:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:700] Took 12.23s\n",
            "[167:700] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.076\n",
            "[167:700] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.658, cum_F_cycle_loss: 0.066\n",
            "[167:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:800] Took 13.35s\n",
            "[167:800] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.069\n",
            "[167:800] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.719, cum_F_cycle_loss: 0.066\n",
            "[167:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:900] Took 12.99s\n",
            "[167:900] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.717, cum_G_cycle_loss: 0.073\n",
            "[167:900] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.670, cum_F_cycle_loss: 0.068\n",
            "[167:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:1000] Took 12.60s\n",
            "[167:1000] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.067\n",
            "[167:1000] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.065\n",
            "[167:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:1100] Took 12.12s\n",
            "[167:1100] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.069\n",
            "[167:1100] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.673, cum_F_cycle_loss: 0.066\n",
            "[167:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[167:END] Completed epoch in 147.47434997558594s\n",
            "[167:1187] ep_D_X_loss: 0.086, ep_G_fool_loss: 0.747, ep_G_cycle_loss: 0.071\n",
            "[167:1187] ep_D_Y_loss: 0.071, ep_F_fool_loss: 0.694, ep_F_cycle_loss: 0.067\n",
            "[167:END] Completed eval in 1.3942975997924805s\n",
            "Updated G_opt learning rate from 6.732673267326732e-05 to 6.534653465346535e-05\n",
            "Updated F_opt learning rate from 6.732673267326732e-05 to 6.534653465346535e-05\n",
            "Updated D_X_opt learning rate from 6.732673267326732e-05 to 6.534653465346535e-05\n",
            "Updated D_Y_opt learning rate from 6.732673267326732e-05 to 6.534653465346535e-05\n",
            "[168:100] Took 13.96s\n",
            "[168:100] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.071\n",
            "[168:100] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.708, cum_F_cycle_loss: 0.066\n",
            "[168:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:200] Took 11.69s\n",
            "[168:200] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.070\n",
            "[168:200] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.747, cum_F_cycle_loss: 0.065\n",
            "[168:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:300] Took 11.69s\n",
            "[168:300] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.791, cum_G_cycle_loss: 0.073\n",
            "[168:300] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.684, cum_F_cycle_loss: 0.068\n",
            "[168:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:400] Took 11.69s\n",
            "[168:400] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.828, cum_G_cycle_loss: 0.082\n",
            "[168:400] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.068\n",
            "[168:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:500] Took 11.68s\n",
            "[168:500] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.069\n",
            "[168:500] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.746, cum_F_cycle_loss: 0.063\n",
            "[168:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:600] Took 11.69s\n",
            "[168:600] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.726, cum_G_cycle_loss: 0.069\n",
            "[168:600] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.067\n",
            "[168:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:700] Took 11.69s\n",
            "[168:700] cum_D_X_loss: 0.070, cum_G_fool_loss: 0.772, cum_G_cycle_loss: 0.069\n",
            "[168:700] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.066\n",
            "[168:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:800] Took 11.69s\n",
            "[168:800] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.076\n",
            "[168:800] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.066\n",
            "[168:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:900] Took 11.69s\n",
            "[168:900] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.072\n",
            "[168:900] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.064\n",
            "[168:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:1000] Took 11.70s\n",
            "[168:1000] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.071\n",
            "[168:1000] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.066\n",
            "[168:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:1100] Took 11.69s\n",
            "[168:1100] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.734, cum_G_cycle_loss: 0.073\n",
            "[168:1100] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.065\n",
            "[168:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[168:END] Completed epoch in 141.7968521118164s\n",
            "[168:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.753, ep_G_cycle_loss: 0.072\n",
            "[168:1187] ep_D_Y_loss: 0.072, ep_F_fool_loss: 0.709, ep_F_cycle_loss: 0.066\n",
            "[168:END] Completed eval in 1.4481279850006104s\n",
            "Updated G_opt learning rate from 6.534653465346535e-05 to 6.336633663366336e-05\n",
            "Updated F_opt learning rate from 6.534653465346535e-05 to 6.336633663366336e-05\n",
            "Updated D_X_opt learning rate from 6.534653465346535e-05 to 6.336633663366336e-05\n",
            "Updated D_Y_opt learning rate from 6.534653465346535e-05 to 6.336633663366336e-05\n",
            "[169:100] Took 13.96s\n",
            "[169:100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.734, cum_G_cycle_loss: 0.068\n",
            "[169:100] cum_D_Y_loss: 0.094, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.068\n",
            "[169:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:200] Took 11.69s\n",
            "[169:200] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.072\n",
            "[169:200] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.063\n",
            "[169:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:300] Took 11.69s\n",
            "[169:300] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.718, cum_G_cycle_loss: 0.072\n",
            "[169:300] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.666, cum_F_cycle_loss: 0.065\n",
            "[169:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:400] Took 11.69s\n",
            "[169:400] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.733, cum_G_cycle_loss: 0.078\n",
            "[169:400] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.066\n",
            "[169:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:500] Took 11.68s\n",
            "[169:500] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.066\n",
            "[169:500] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.066\n",
            "[169:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:600] Took 11.75s\n",
            "[169:600] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.071\n",
            "[169:600] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.714, cum_F_cycle_loss: 0.062\n",
            "[169:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:700] Took 11.76s\n",
            "[169:700] cum_D_X_loss: 0.063, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.070\n",
            "[169:700] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.765, cum_F_cycle_loss: 0.064\n",
            "[169:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:800] Took 11.69s\n",
            "[169:800] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.076\n",
            "[169:800] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.745, cum_F_cycle_loss: 0.063\n",
            "[169:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:900] Took 11.68s\n",
            "[169:900] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.070\n",
            "[169:900] cum_D_Y_loss: 0.056, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.063\n",
            "[169:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:1000] Took 11.69s\n",
            "[169:1000] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.774, cum_G_cycle_loss: 0.071\n",
            "[169:1000] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.068\n",
            "[169:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:1100] Took 11.69s\n",
            "[169:1100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.067\n",
            "[169:1100] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.065\n",
            "[169:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[169:END] Completed epoch in 141.9118616580963s\n",
            "[169:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.753, ep_G_cycle_loss: 0.071\n",
            "[169:1187] ep_D_Y_loss: 0.069, ep_F_fool_loss: 0.709, ep_F_cycle_loss: 0.065\n",
            "[169:END] Completed eval in 1.4700894355773926s\n",
            "Updated G_opt learning rate from 6.336633663366336e-05 to 6.13861386138614e-05\n",
            "Updated F_opt learning rate from 6.336633663366336e-05 to 6.13861386138614e-05\n",
            "Updated D_X_opt learning rate from 6.336633663366336e-05 to 6.13861386138614e-05\n",
            "Updated D_Y_opt learning rate from 6.336633663366336e-05 to 6.13861386138614e-05\n",
            "[170:100] Took 13.82s\n",
            "[170:100] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.765, cum_G_cycle_loss: 0.068\n",
            "[170:100] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.731, cum_F_cycle_loss: 0.064\n",
            "[170:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:200] Took 11.69s\n",
            "[170:200] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.744, cum_G_cycle_loss: 0.068\n",
            "[170:200] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.066\n",
            "[170:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:300] Took 11.69s\n",
            "[170:300] cum_D_X_loss: 0.069, cum_G_fool_loss: 0.729, cum_G_cycle_loss: 0.067\n",
            "[170:300] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.731, cum_F_cycle_loss: 0.066\n",
            "[170:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:400] Took 11.70s\n",
            "[170:400] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.070\n",
            "[170:400] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.778, cum_F_cycle_loss: 0.067\n",
            "[170:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:500] Took 11.69s\n",
            "[170:500] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.072\n",
            "[170:500] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.739, cum_F_cycle_loss: 0.068\n",
            "[170:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:600] Took 11.69s\n",
            "[170:600] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.077\n",
            "[170:600] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.066\n",
            "[170:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:700] Took 11.69s\n",
            "[170:700] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.805, cum_G_cycle_loss: 0.072\n",
            "[170:700] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.068\n",
            "[170:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:800] Took 11.69s\n",
            "[170:800] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.800, cum_G_cycle_loss: 0.070\n",
            "[170:800] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.066\n",
            "[170:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:900] Took 11.68s\n",
            "[170:900] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.790, cum_G_cycle_loss: 0.071\n",
            "[170:900] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.064\n",
            "[170:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:1000] Took 11.69s\n",
            "[170:1000] cum_D_X_loss: 0.070, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.070\n",
            "[170:1000] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.774, cum_F_cycle_loss: 0.068\n",
            "[170:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:1100] Took 11.69s\n",
            "[170:1100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.076\n",
            "[170:1100] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.742, cum_F_cycle_loss: 0.069\n",
            "[170:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[170:END] Completed epoch in 141.65975284576416s\n",
            "[170:1187] ep_D_X_loss: 0.081, ep_G_fool_loss: 0.760, ep_G_cycle_loss: 0.071\n",
            "[170:1187] ep_D_Y_loss: 0.068, ep_F_fool_loss: 0.722, ep_F_cycle_loss: 0.066\n",
            "[170:END] Completed eval in 1.4760241508483887s\n",
            "Updated G_opt learning rate from 6.13861386138614e-05 to 5.940594059405942e-05\n",
            "Updated F_opt learning rate from 6.13861386138614e-05 to 5.940594059405942e-05\n",
            "Updated D_X_opt learning rate from 6.13861386138614e-05 to 5.940594059405942e-05\n",
            "Updated D_Y_opt learning rate from 6.13861386138614e-05 to 5.940594059405942e-05\n",
            "[170:END] Saving models and training information\n",
            "[171:100] Took 14.22s\n",
            "[171:100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.723, cum_G_cycle_loss: 0.069\n",
            "[171:100] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.719, cum_F_cycle_loss: 0.071\n",
            "[171:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:200] Took 11.68s\n",
            "[171:200] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.798, cum_G_cycle_loss: 0.074\n",
            "[171:200] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.715, cum_F_cycle_loss: 0.067\n",
            "[171:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:300] Took 11.68s\n",
            "[171:300] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.792, cum_G_cycle_loss: 0.072\n",
            "[171:300] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.737, cum_F_cycle_loss: 0.067\n",
            "[171:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:400] Took 11.69s\n",
            "[171:400] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.691, cum_G_cycle_loss: 0.070\n",
            "[171:400] cum_D_Y_loss: 0.084, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.064\n",
            "[171:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:500] Took 11.68s\n",
            "[171:500] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.720, cum_G_cycle_loss: 0.066\n",
            "[171:500] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.065\n",
            "[171:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:600] Took 11.69s\n",
            "[171:600] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.068\n",
            "[171:600] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.617, cum_F_cycle_loss: 0.064\n",
            "[171:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:700] Took 11.90s\n",
            "[171:700] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.751, cum_G_cycle_loss: 0.067\n",
            "[171:700] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.742, cum_F_cycle_loss: 0.067\n",
            "[171:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:800] Took 11.96s\n",
            "[171:800] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.789, cum_G_cycle_loss: 0.069\n",
            "[171:800] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.067\n",
            "[171:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:900] Took 12.02s\n",
            "[171:900] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.780, cum_G_cycle_loss: 0.066\n",
            "[171:900] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.706, cum_F_cycle_loss: 0.068\n",
            "[171:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:1000] Took 11.70s\n",
            "[171:1000] cum_D_X_loss: 0.099, cum_G_fool_loss: 0.767, cum_G_cycle_loss: 0.068\n",
            "[171:1000] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.686, cum_F_cycle_loss: 0.063\n",
            "[171:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:1100] Took 11.69s\n",
            "[171:1100] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.784, cum_G_cycle_loss: 0.080\n",
            "[171:1100] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.653, cum_F_cycle_loss: 0.062\n",
            "[171:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[171:END] Completed epoch in 142.84869265556335s\n",
            "[171:1187] ep_D_X_loss: 0.083, ep_G_fool_loss: 0.759, ep_G_cycle_loss: 0.069\n",
            "[171:1187] ep_D_Y_loss: 0.069, ep_F_fool_loss: 0.703, ep_F_cycle_loss: 0.066\n",
            "[171:END] Completed eval in 1.450120210647583s\n",
            "Updated G_opt learning rate from 5.940594059405942e-05 to 5.7425742574257435e-05\n",
            "Updated F_opt learning rate from 5.940594059405942e-05 to 5.7425742574257435e-05\n",
            "Updated D_X_opt learning rate from 5.940594059405942e-05 to 5.7425742574257435e-05\n",
            "Updated D_Y_opt learning rate from 5.940594059405942e-05 to 5.7425742574257435e-05\n",
            "[172:100] Took 13.90s\n",
            "[172:100] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.794, cum_G_cycle_loss: 0.072\n",
            "[172:100] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.688, cum_F_cycle_loss: 0.065\n",
            "[172:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:200] Took 11.69s\n",
            "[172:200] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.726, cum_G_cycle_loss: 0.069\n",
            "[172:200] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.063\n",
            "[172:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:300] Took 11.69s\n",
            "[172:300] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.070\n",
            "[172:300] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.063\n",
            "[172:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:400] Took 11.69s\n",
            "[172:400] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.801, cum_G_cycle_loss: 0.067\n",
            "[172:400] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.682, cum_F_cycle_loss: 0.061\n",
            "[172:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:500] Took 11.70s\n",
            "[172:500] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.712, cum_G_cycle_loss: 0.064\n",
            "[172:500] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.677, cum_F_cycle_loss: 0.061\n",
            "[172:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:600] Took 11.69s\n",
            "[172:600] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.717, cum_G_cycle_loss: 0.068\n",
            "[172:600] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.065\n",
            "[172:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:700] Took 11.70s\n",
            "[172:700] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.070\n",
            "[172:700] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.744, cum_F_cycle_loss: 0.062\n",
            "[172:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:800] Took 11.70s\n",
            "[172:800] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.787, cum_G_cycle_loss: 0.070\n",
            "[172:800] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.062\n",
            "[172:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:900] Took 11.69s\n",
            "[172:900] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.769, cum_G_cycle_loss: 0.066\n",
            "[172:900] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.718, cum_F_cycle_loss: 0.065\n",
            "[172:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:1000] Took 11.69s\n",
            "[172:1000] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.068\n",
            "[172:1000] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.064\n",
            "[172:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:1100] Took 11.69s\n",
            "[172:1100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.068\n",
            "[172:1100] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.666, cum_F_cycle_loss: 0.063\n",
            "[172:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[172:END] Completed epoch in 141.7723593711853s\n",
            "[172:1187] ep_D_X_loss: 0.083, ep_G_fool_loss: 0.756, ep_G_cycle_loss: 0.068\n",
            "[172:1187] ep_D_Y_loss: 0.070, ep_F_fool_loss: 0.699, ep_F_cycle_loss: 0.063\n",
            "[172:END] Completed eval in 1.4900412559509277s\n",
            "Updated G_opt learning rate from 5.7425742574257435e-05 to 5.544554455445545e-05\n",
            "Updated F_opt learning rate from 5.7425742574257435e-05 to 5.544554455445545e-05\n",
            "Updated D_X_opt learning rate from 5.7425742574257435e-05 to 5.544554455445545e-05\n",
            "Updated D_Y_opt learning rate from 5.7425742574257435e-05 to 5.544554455445545e-05\n",
            "[173:100] Took 14.05s\n",
            "[173:100] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.780, cum_G_cycle_loss: 0.069\n",
            "[173:100] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.721, cum_F_cycle_loss: 0.063\n",
            "[173:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:200] Took 11.69s\n",
            "[173:200] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.731, cum_G_cycle_loss: 0.069\n",
            "[173:200] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.749, cum_F_cycle_loss: 0.066\n",
            "[173:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:300] Took 11.69s\n",
            "[173:300] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.068\n",
            "[173:300] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.738, cum_F_cycle_loss: 0.065\n",
            "[173:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:400] Took 11.69s\n",
            "[173:400] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.750, cum_G_cycle_loss: 0.071\n",
            "[173:400] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.064\n",
            "[173:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:500] Took 11.69s\n",
            "[173:500] cum_D_X_loss: 0.067, cum_G_fool_loss: 0.799, cum_G_cycle_loss: 0.066\n",
            "[173:500] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.745, cum_F_cycle_loss: 0.066\n",
            "[173:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:600] Took 11.70s\n",
            "[173:600] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.069\n",
            "[173:600] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.066\n",
            "[173:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:700] Took 11.69s\n",
            "[173:700] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.788, cum_G_cycle_loss: 0.070\n",
            "[173:700] cum_D_Y_loss: 0.055, cum_F_fool_loss: 0.728, cum_F_cycle_loss: 0.066\n",
            "[173:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:800] Took 11.69s\n",
            "[173:800] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.756, cum_G_cycle_loss: 0.066\n",
            "[173:800] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.067\n",
            "[173:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:900] Took 11.70s\n",
            "[173:900] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.726, cum_G_cycle_loss: 0.069\n",
            "[173:900] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.066\n",
            "[173:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:1000] Took 11.70s\n",
            "[173:1000] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.774, cum_G_cycle_loss: 0.070\n",
            "[173:1000] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.671, cum_F_cycle_loss: 0.063\n",
            "[173:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:1100] Took 11.68s\n",
            "[173:1100] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.789, cum_G_cycle_loss: 0.065\n",
            "[173:1100] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.737, cum_F_cycle_loss: 0.060\n",
            "[173:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[173:END] Completed epoch in 141.92929363250732s\n",
            "[173:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.763, ep_G_cycle_loss: 0.068\n",
            "[173:1187] ep_D_Y_loss: 0.070, ep_F_fool_loss: 0.716, ep_F_cycle_loss: 0.065\n",
            "[173:END] Completed eval in 1.5049610137939453s\n",
            "Updated G_opt learning rate from 5.544554455445545e-05 to 5.346534653465347e-05\n",
            "Updated F_opt learning rate from 5.544554455445545e-05 to 5.346534653465347e-05\n",
            "Updated D_X_opt learning rate from 5.544554455445545e-05 to 5.346534653465347e-05\n",
            "Updated D_Y_opt learning rate from 5.544554455445545e-05 to 5.346534653465347e-05\n",
            "[174:100] Took 13.95s\n",
            "[174:100] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.777, cum_G_cycle_loss: 0.065\n",
            "[174:100] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.734, cum_F_cycle_loss: 0.064\n",
            "[174:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:200] Took 11.69s\n",
            "[174:200] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.765, cum_G_cycle_loss: 0.067\n",
            "[174:200] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.065\n",
            "[174:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:300] Took 11.69s\n",
            "[174:300] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.761, cum_G_cycle_loss: 0.069\n",
            "[174:300] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.061\n",
            "[174:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:400] Took 11.69s\n",
            "[174:400] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.729, cum_G_cycle_loss: 0.067\n",
            "[174:400] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.686, cum_F_cycle_loss: 0.067\n",
            "[174:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:500] Took 11.69s\n",
            "[174:500] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.066\n",
            "[174:500] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.065\n",
            "[174:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:600] Took 11.70s\n",
            "[174:600] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.787, cum_G_cycle_loss: 0.066\n",
            "[174:600] cum_D_Y_loss: 0.056, cum_F_fool_loss: 0.704, cum_F_cycle_loss: 0.065\n",
            "[174:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:700] Took 11.69s\n",
            "[174:700] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.833, cum_G_cycle_loss: 0.075\n",
            "[174:700] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.691, cum_F_cycle_loss: 0.067\n",
            "[174:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:800] Took 11.70s\n",
            "[174:800] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.072\n",
            "[174:800] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.066\n",
            "[174:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:900] Took 11.69s\n",
            "[174:900] cum_D_X_loss: 0.094, cum_G_fool_loss: 0.801, cum_G_cycle_loss: 0.068\n",
            "[174:900] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.070\n",
            "[174:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:1000] Took 11.70s\n",
            "[174:1000] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.067\n",
            "[174:1000] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.066\n",
            "[174:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:1100] Took 11.69s\n",
            "[174:1100] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.785, cum_G_cycle_loss: 0.067\n",
            "[174:1100] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.738, cum_F_cycle_loss: 0.062\n",
            "[174:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[174:END] Completed epoch in 141.82405853271484s\n",
            "[174:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.766, ep_G_cycle_loss: 0.068\n",
            "[174:1187] ep_D_Y_loss: 0.067, ep_F_fool_loss: 0.709, ep_F_cycle_loss: 0.065\n",
            "[174:END] Completed eval in 1.4600915908813477s\n",
            "Updated G_opt learning rate from 5.346534653465347e-05 to 5.148514851485149e-05\n",
            "Updated F_opt learning rate from 5.346534653465347e-05 to 5.148514851485149e-05\n",
            "Updated D_X_opt learning rate from 5.346534653465347e-05 to 5.148514851485149e-05\n",
            "Updated D_Y_opt learning rate from 5.346534653465347e-05 to 5.148514851485149e-05\n",
            "[175:100] Took 13.94s\n",
            "[175:100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.798, cum_G_cycle_loss: 0.066\n",
            "[175:100] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.064\n",
            "[175:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:200] Took 11.70s\n",
            "[175:200] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.067\n",
            "[175:200] cum_D_Y_loss: 0.054, cum_F_fool_loss: 0.730, cum_F_cycle_loss: 0.064\n",
            "[175:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:300] Took 11.70s\n",
            "[175:300] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.735, cum_G_cycle_loss: 0.070\n",
            "[175:300] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.715, cum_F_cycle_loss: 0.065\n",
            "[175:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:400] Took 11.69s\n",
            "[175:400] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.066\n",
            "[175:400] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.709, cum_F_cycle_loss: 0.066\n",
            "[175:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:500] Took 11.69s\n",
            "[175:500] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.796, cum_G_cycle_loss: 0.064\n",
            "[175:500] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.064\n",
            "[175:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:600] Took 11.69s\n",
            "[175:600] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.068\n",
            "[175:600] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.065\n",
            "[175:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:700] Took 11.70s\n",
            "[175:700] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.720, cum_G_cycle_loss: 0.068\n",
            "[175:700] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.736, cum_F_cycle_loss: 0.063\n",
            "[175:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:800] Took 11.68s\n",
            "[175:800] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.735, cum_G_cycle_loss: 0.068\n",
            "[175:800] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.688, cum_F_cycle_loss: 0.062\n",
            "[175:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:900] Took 11.70s\n",
            "[175:900] cum_D_X_loss: 0.064, cum_G_fool_loss: 0.749, cum_G_cycle_loss: 0.068\n",
            "[175:900] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.768, cum_F_cycle_loss: 0.070\n",
            "[175:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:1000] Took 12.27s\n",
            "[175:1000] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.681, cum_G_cycle_loss: 0.068\n",
            "[175:1000] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.069\n",
            "[175:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:1100] Took 11.68s\n",
            "[175:1100] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.067\n",
            "[175:1100] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.070\n",
            "[175:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[175:END] Completed epoch in 142.36317539215088s\n",
            "[175:1187] ep_D_X_loss: 0.078, ep_G_fool_loss: 0.744, ep_G_cycle_loss: 0.067\n",
            "[175:1187] ep_D_Y_loss: 0.071, ep_F_fool_loss: 0.722, ep_F_cycle_loss: 0.066\n",
            "[175:END] Completed eval in 1.5039763450622559s\n",
            "Updated G_opt learning rate from 5.148514851485149e-05 to 4.950495049504951e-05\n",
            "Updated F_opt learning rate from 5.148514851485149e-05 to 4.950495049504951e-05\n",
            "Updated D_X_opt learning rate from 5.148514851485149e-05 to 4.950495049504951e-05\n",
            "Updated D_Y_opt learning rate from 5.148514851485149e-05 to 4.950495049504951e-05\n",
            "[175:END] Saving models and training information\n",
            "[176:100] Took 13.88s\n",
            "[176:100] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.073\n",
            "[176:100] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.070\n",
            "[176:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:200] Took 11.69s\n",
            "[176:200] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.066\n",
            "[176:200] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.714, cum_F_cycle_loss: 0.067\n",
            "[176:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:300] Took 11.69s\n",
            "[176:300] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.067\n",
            "[176:300] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.728, cum_F_cycle_loss: 0.065\n",
            "[176:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:400] Took 11.68s\n",
            "[176:400] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.751, cum_G_cycle_loss: 0.065\n",
            "[176:400] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.744, cum_F_cycle_loss: 0.063\n",
            "[176:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:500] Took 11.68s\n",
            "[176:500] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.777, cum_G_cycle_loss: 0.070\n",
            "[176:500] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.063\n",
            "[176:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:600] Took 11.68s\n",
            "[176:600] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.704, cum_G_cycle_loss: 0.071\n",
            "[176:600] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.063\n",
            "[176:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:700] Took 11.69s\n",
            "[176:700] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.769, cum_G_cycle_loss: 0.066\n",
            "[176:700] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.737, cum_F_cycle_loss: 0.064\n",
            "[176:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:800] Took 11.67s\n",
            "[176:800] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.070\n",
            "[176:800] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.064\n",
            "[176:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:900] Took 11.68s\n",
            "[176:900] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.776, cum_G_cycle_loss: 0.070\n",
            "[176:900] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.062\n",
            "[176:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:1000] Took 12.12s\n",
            "[176:1000] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.727, cum_G_cycle_loss: 0.065\n",
            "[176:1000] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.743, cum_F_cycle_loss: 0.063\n",
            "[176:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:1100] Took 11.68s\n",
            "[176:1100] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.740, cum_G_cycle_loss: 0.072\n",
            "[176:1100] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.709, cum_F_cycle_loss: 0.065\n",
            "[176:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[176:END] Completed epoch in 142.05948996543884s\n",
            "[176:1187] ep_D_X_loss: 0.083, ep_G_fool_loss: 0.755, ep_G_cycle_loss: 0.069\n",
            "[176:1187] ep_D_Y_loss: 0.070, ep_F_fool_loss: 0.718, ep_F_cycle_loss: 0.065\n",
            "[176:END] Completed eval in 1.5488345623016357s\n",
            "Updated G_opt learning rate from 4.950495049504951e-05 to 4.7524752475247525e-05\n",
            "Updated F_opt learning rate from 4.950495049504951e-05 to 4.7524752475247525e-05\n",
            "Updated D_X_opt learning rate from 4.950495049504951e-05 to 4.7524752475247525e-05\n",
            "Updated D_Y_opt learning rate from 4.950495049504951e-05 to 4.7524752475247525e-05\n",
            "[177:100] Took 13.85s\n",
            "[177:100] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.064\n",
            "[177:100] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.748, cum_F_cycle_loss: 0.067\n",
            "[177:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:200] Took 11.68s\n",
            "[177:200] cum_D_X_loss: 0.069, cum_G_fool_loss: 0.709, cum_G_cycle_loss: 0.068\n",
            "[177:200] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.745, cum_F_cycle_loss: 0.068\n",
            "[177:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:300] Took 11.68s\n",
            "[177:300] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.069\n",
            "[177:300] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.729, cum_F_cycle_loss: 0.067\n",
            "[177:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:400] Took 11.68s\n",
            "[177:400] cum_D_X_loss: 0.097, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.073\n",
            "[177:400] cum_D_Y_loss: 0.079, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.068\n",
            "[177:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:500] Took 11.76s\n",
            "[177:500] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.068\n",
            "[177:500] cum_D_Y_loss: 0.085, cum_F_fool_loss: 0.715, cum_F_cycle_loss: 0.066\n",
            "[177:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:600] Took 11.75s\n",
            "[177:600] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.064\n",
            "[177:600] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.675, cum_F_cycle_loss: 0.062\n",
            "[177:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:700] Took 11.75s\n",
            "[177:700] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.740, cum_G_cycle_loss: 0.067\n",
            "[177:700] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.064\n",
            "[177:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:800] Took 11.68s\n",
            "[177:800] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.716, cum_G_cycle_loss: 0.068\n",
            "[177:800] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.072\n",
            "[177:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:900] Took 11.68s\n",
            "[177:900] cum_D_X_loss: 0.071, cum_G_fool_loss: 0.786, cum_G_cycle_loss: 0.066\n",
            "[177:900] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.066\n",
            "[177:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:1000] Took 11.69s\n",
            "[177:1000] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.735, cum_G_cycle_loss: 0.066\n",
            "[177:1000] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.061\n",
            "[177:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:1100] Took 11.68s\n",
            "[177:1100] cum_D_X_loss: 0.071, cum_G_fool_loss: 0.784, cum_G_cycle_loss: 0.066\n",
            "[177:1100] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.678, cum_F_cycle_loss: 0.066\n",
            "[177:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[177:END] Completed epoch in 141.81916689872742s\n",
            "[177:1187] ep_D_X_loss: 0.081, ep_G_fool_loss: 0.745, ep_G_cycle_loss: 0.067\n",
            "[177:1187] ep_D_Y_loss: 0.070, ep_F_fool_loss: 0.707, ep_F_cycle_loss: 0.066\n",
            "[177:END] Completed eval in 1.5249204635620117s\n",
            "Updated G_opt learning rate from 4.7524752475247525e-05 to 4.554455445544554e-05\n",
            "Updated F_opt learning rate from 4.7524752475247525e-05 to 4.554455445544554e-05\n",
            "Updated D_X_opt learning rate from 4.7524752475247525e-05 to 4.554455445544554e-05\n",
            "Updated D_Y_opt learning rate from 4.7524752475247525e-05 to 4.554455445544554e-05\n",
            "[178:100] Took 13.86s\n",
            "[178:100] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.766, cum_G_cycle_loss: 0.066\n",
            "[178:100] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.704, cum_F_cycle_loss: 0.063\n",
            "[178:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:200] Took 11.68s\n",
            "[178:200] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.071\n",
            "[178:200] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.729, cum_F_cycle_loss: 0.064\n",
            "[178:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:300] Took 11.68s\n",
            "[178:300] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.730, cum_G_cycle_loss: 0.063\n",
            "[178:300] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.750, cum_F_cycle_loss: 0.065\n",
            "[178:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:400] Took 11.68s\n",
            "[178:400] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.065\n",
            "[178:400] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.704, cum_F_cycle_loss: 0.066\n",
            "[178:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:500] Took 11.68s\n",
            "[178:500] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.792, cum_G_cycle_loss: 0.069\n",
            "[178:500] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.065\n",
            "[178:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:600] Took 11.68s\n",
            "[178:600] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.776, cum_G_cycle_loss: 0.067\n",
            "[178:600] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.065\n",
            "[178:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:700] Took 11.68s\n",
            "[178:700] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.067\n",
            "[178:700] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.736, cum_F_cycle_loss: 0.066\n",
            "[178:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:800] Took 11.69s\n",
            "[178:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.060\n",
            "[178:800] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.715, cum_F_cycle_loss: 0.062\n",
            "[178:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:900] Took 11.67s\n",
            "[178:900] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.749, cum_G_cycle_loss: 0.066\n",
            "[178:900] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.061\n",
            "[178:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:1000] Took 12.21s\n",
            "[178:1000] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.763, cum_G_cycle_loss: 0.069\n",
            "[178:1000] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.730, cum_F_cycle_loss: 0.064\n",
            "[178:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:1100] Took 11.68s\n",
            "[178:1100] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.067\n",
            "[178:1100] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.735, cum_F_cycle_loss: 0.066\n",
            "[178:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[178:END] Completed epoch in 142.12059259414673s\n",
            "[178:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.764, ep_G_cycle_loss: 0.066\n",
            "[178:1187] ep_D_Y_loss: 0.068, ep_F_fool_loss: 0.718, ep_F_cycle_loss: 0.064\n",
            "[178:END] Completed eval in 1.5578324794769287s\n",
            "Updated G_opt learning rate from 4.554455445544554e-05 to 4.356435643564356e-05\n",
            "Updated F_opt learning rate from 4.554455445544554e-05 to 4.356435643564356e-05\n",
            "Updated D_X_opt learning rate from 4.554455445544554e-05 to 4.356435643564356e-05\n",
            "Updated D_Y_opt learning rate from 4.554455445544554e-05 to 4.356435643564356e-05\n",
            "[179:100] Took 13.89s\n",
            "[179:100] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.793, cum_G_cycle_loss: 0.066\n",
            "[179:100] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.743, cum_F_cycle_loss: 0.070\n",
            "[179:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:200] Took 11.68s\n",
            "[179:200] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.720, cum_G_cycle_loss: 0.063\n",
            "[179:200] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.064\n",
            "[179:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:300] Took 11.68s\n",
            "[179:300] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.800, cum_G_cycle_loss: 0.063\n",
            "[179:300] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.064\n",
            "[179:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:400] Took 11.68s\n",
            "[179:400] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.069\n",
            "[179:400] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.729, cum_F_cycle_loss: 0.068\n",
            "[179:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:500] Took 11.68s\n",
            "[179:500] cum_D_X_loss: 0.101, cum_G_fool_loss: 0.772, cum_G_cycle_loss: 0.066\n",
            "[179:500] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.688, cum_F_cycle_loss: 0.065\n",
            "[179:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:600] Took 11.68s\n",
            "[179:600] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.767, cum_G_cycle_loss: 0.064\n",
            "[179:600] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.736, cum_F_cycle_loss: 0.066\n",
            "[179:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:700] Took 11.68s\n",
            "[179:700] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.073\n",
            "[179:700] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.747, cum_F_cycle_loss: 0.062\n",
            "[179:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:800] Took 11.68s\n",
            "[179:800] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.065\n",
            "[179:800] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.726, cum_F_cycle_loss: 0.062\n",
            "[179:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:900] Took 11.67s\n",
            "[179:900] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.068\n",
            "[179:900] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.757, cum_F_cycle_loss: 0.069\n",
            "[179:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:1000] Took 11.98s\n",
            "[179:1000] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.764, cum_G_cycle_loss: 0.065\n",
            "[179:1000] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.063\n",
            "[179:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:1100] Took 12.19s\n",
            "[179:1100] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.068\n",
            "[179:1100] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.064\n",
            "[179:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[179:END] Completed epoch in 142.4308614730835s\n",
            "[179:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.761, ep_G_cycle_loss: 0.067\n",
            "[179:1187] ep_D_Y_loss: 0.067, ep_F_fool_loss: 0.722, ep_F_cycle_loss: 0.065\n",
            "[179:END] Completed eval in 1.5608251094818115s\n",
            "Updated G_opt learning rate from 4.356435643564356e-05 to 4.158415841584158e-05\n",
            "Updated F_opt learning rate from 4.356435643564356e-05 to 4.158415841584158e-05\n",
            "Updated D_X_opt learning rate from 4.356435643564356e-05 to 4.158415841584158e-05\n",
            "Updated D_Y_opt learning rate from 4.356435643564356e-05 to 4.158415841584158e-05\n",
            "[180:100] Took 14.00s\n",
            "[180:100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.772, cum_G_cycle_loss: 0.065\n",
            "[180:100] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.064\n",
            "[180:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:200] Took 11.79s\n",
            "[180:200] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.807, cum_G_cycle_loss: 0.068\n",
            "[180:200] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.064\n",
            "[180:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:300] Took 11.84s\n",
            "[180:300] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.792, cum_G_cycle_loss: 0.062\n",
            "[180:300] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.708, cum_F_cycle_loss: 0.061\n",
            "[180:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:400] Took 61.02s\n",
            "[180:400] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.735, cum_G_cycle_loss: 0.066\n",
            "[180:400] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.061\n",
            "[180:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:500] Took 11.62s\n",
            "[180:500] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.065\n",
            "[180:500] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.062\n",
            "[180:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:600] Took 11.63s\n",
            "[180:600] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.065\n",
            "[180:600] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.067\n",
            "[180:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:700] Took 11.69s\n",
            "[180:700] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.064\n",
            "[180:700] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.743, cum_F_cycle_loss: 0.064\n",
            "[180:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:800] Took 11.69s\n",
            "[180:800] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.066\n",
            "[180:800] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.704, cum_F_cycle_loss: 0.062\n",
            "[180:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:900] Took 11.70s\n",
            "[180:900] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.069\n",
            "[180:900] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.776, cum_F_cycle_loss: 0.067\n",
            "[180:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:1000] Took 11.69s\n",
            "[180:1000] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.062\n",
            "[180:1000] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.737, cum_F_cycle_loss: 0.064\n",
            "[180:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:1100] Took 11.70s\n",
            "[180:1100] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.688, cum_G_cycle_loss: 0.071\n",
            "[180:1100] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.721, cum_F_cycle_loss: 0.063\n",
            "[180:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[180:END] Completed epoch in 191.30890917778015s\n",
            "[180:1187] ep_D_X_loss: 0.081, ep_G_fool_loss: 0.755, ep_G_cycle_loss: 0.066\n",
            "[180:1187] ep_D_Y_loss: 0.070, ep_F_fool_loss: 0.718, ep_F_cycle_loss: 0.064\n",
            "[180:END] Completed eval in 1.5288825035095215s\n",
            "Updated G_opt learning rate from 4.158415841584158e-05 to 3.96039603960396e-05\n",
            "Updated F_opt learning rate from 4.158415841584158e-05 to 3.96039603960396e-05\n",
            "Updated D_X_opt learning rate from 4.158415841584158e-05 to 3.96039603960396e-05\n",
            "Updated D_Y_opt learning rate from 4.158415841584158e-05 to 3.96039603960396e-05\n",
            "[180:END] Saving models and training information\n",
            "[181:100] Took 13.89s\n",
            "[181:100] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.759, cum_G_cycle_loss: 0.066\n",
            "[181:100] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.062\n",
            "[181:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:200] Took 11.69s\n",
            "[181:200] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.724, cum_G_cycle_loss: 0.064\n",
            "[181:200] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.061\n",
            "[181:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:300] Took 11.69s\n",
            "[181:300] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.061\n",
            "[181:300] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.069\n",
            "[181:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:400] Took 11.70s\n",
            "[181:400] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.067\n",
            "[181:400] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.708, cum_F_cycle_loss: 0.067\n",
            "[181:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:500] Took 11.69s\n",
            "[181:500] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.789, cum_G_cycle_loss: 0.065\n",
            "[181:500] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.066\n",
            "[181:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:600] Took 11.70s\n",
            "[181:600] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.068\n",
            "[181:600] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.064\n",
            "[181:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:700] Took 11.69s\n",
            "[181:700] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.070\n",
            "[181:700] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.062\n",
            "[181:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:800] Took 11.70s\n",
            "[181:800] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.763, cum_G_cycle_loss: 0.070\n",
            "[181:800] cum_D_Y_loss: 0.078, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.063\n",
            "[181:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:900] Took 11.70s\n",
            "[181:900] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.066\n",
            "[181:900] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.673, cum_F_cycle_loss: 0.063\n",
            "[181:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:1000] Took 11.69s\n",
            "[181:1000] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.756, cum_G_cycle_loss: 0.066\n",
            "[181:1000] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.061\n",
            "[181:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:1100] Took 11.69s\n",
            "[181:1100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.065\n",
            "[181:1100] cum_D_Y_loss: 0.058, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.065\n",
            "[181:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[181:END] Completed epoch in 141.79483318328857s\n",
            "[181:1187] ep_D_X_loss: 0.084, ep_G_fool_loss: 0.756, ep_G_cycle_loss: 0.066\n",
            "[181:1187] ep_D_Y_loss: 0.068, ep_F_fool_loss: 0.701, ep_F_cycle_loss: 0.064\n",
            "[181:END] Completed eval in 1.5847601890563965s\n",
            "Updated G_opt learning rate from 3.96039603960396e-05 to 3.7623762376237615e-05\n",
            "Updated F_opt learning rate from 3.96039603960396e-05 to 3.7623762376237615e-05\n",
            "Updated D_X_opt learning rate from 3.96039603960396e-05 to 3.7623762376237615e-05\n",
            "Updated D_Y_opt learning rate from 3.96039603960396e-05 to 3.7623762376237615e-05\n",
            "[182:100] Took 13.87s\n",
            "[182:100] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.065\n",
            "[182:100] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.767, cum_F_cycle_loss: 0.066\n",
            "[182:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:200] Took 11.70s\n",
            "[182:200] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.070\n",
            "[182:200] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.062\n",
            "[182:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:300] Took 11.69s\n",
            "[182:300] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.732, cum_G_cycle_loss: 0.064\n",
            "[182:300] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.062\n",
            "[182:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:400] Took 11.70s\n",
            "[182:400] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.065\n",
            "[182:400] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.060\n",
            "[182:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:500] Took 11.70s\n",
            "[182:500] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.062\n",
            "[182:500] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.640, cum_F_cycle_loss: 0.063\n",
            "[182:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:600] Took 11.69s\n",
            "[182:600] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.064\n",
            "[182:600] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.062\n",
            "[182:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:700] Took 11.69s\n",
            "[182:700] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.725, cum_G_cycle_loss: 0.064\n",
            "[182:700] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.736, cum_F_cycle_loss: 0.062\n",
            "[182:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:800] Took 11.69s\n",
            "[182:800] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.793, cum_G_cycle_loss: 0.067\n",
            "[182:800] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.062\n",
            "[182:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:900] Took 11.69s\n",
            "[182:900] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.752, cum_G_cycle_loss: 0.065\n",
            "[182:900] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.731, cum_F_cycle_loss: 0.057\n",
            "[182:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:1000] Took 11.70s\n",
            "[182:1000] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.071\n",
            "[182:1000] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.737, cum_F_cycle_loss: 0.064\n",
            "[182:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:1100] Took 11.69s\n",
            "[182:1100] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.761, cum_G_cycle_loss: 0.068\n",
            "[182:1100] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.751, cum_F_cycle_loss: 0.063\n",
            "[182:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[182:END] Completed epoch in 141.74752688407898s\n",
            "[182:1187] ep_D_X_loss: 0.083, ep_G_fool_loss: 0.753, ep_G_cycle_loss: 0.066\n",
            "[182:1187] ep_D_Y_loss: 0.069, ep_F_fool_loss: 0.716, ep_F_cycle_loss: 0.062\n",
            "[182:END] Completed eval in 1.6087243556976318s\n",
            "Updated G_opt learning rate from 3.7623762376237615e-05 to 3.564356435643565e-05\n",
            "Updated F_opt learning rate from 3.7623762376237615e-05 to 3.564356435643565e-05\n",
            "Updated D_X_opt learning rate from 3.7623762376237615e-05 to 3.564356435643565e-05\n",
            "Updated D_Y_opt learning rate from 3.7623762376237615e-05 to 3.564356435643565e-05\n",
            "[183:100] Took 13.85s\n",
            "[183:100] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.714, cum_G_cycle_loss: 0.067\n",
            "[183:100] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.062\n",
            "[183:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:200] Took 11.70s\n",
            "[183:200] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.065\n",
            "[183:200] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.718, cum_F_cycle_loss: 0.060\n",
            "[183:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:300] Took 11.69s\n",
            "[183:300] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.784, cum_G_cycle_loss: 0.062\n",
            "[183:300] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.748, cum_F_cycle_loss: 0.060\n",
            "[183:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:400] Took 11.69s\n",
            "[183:400] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.750, cum_G_cycle_loss: 0.067\n",
            "[183:400] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.067\n",
            "[183:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:500] Took 11.69s\n",
            "[183:500] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.072\n",
            "[183:500] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.064\n",
            "[183:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:600] Took 11.70s\n",
            "[183:600] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.831, cum_G_cycle_loss: 0.065\n",
            "[183:600] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.061\n",
            "[183:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:700] Took 11.69s\n",
            "[183:700] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.759, cum_G_cycle_loss: 0.065\n",
            "[183:700] cum_D_Y_loss: 0.058, cum_F_fool_loss: 0.669, cum_F_cycle_loss: 0.063\n",
            "[183:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:800] Took 11.69s\n",
            "[183:800] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.806, cum_G_cycle_loss: 0.064\n",
            "[183:800] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.742, cum_F_cycle_loss: 0.066\n",
            "[183:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:900] Took 11.69s\n",
            "[183:900] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.815, cum_G_cycle_loss: 0.064\n",
            "[183:900] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.733, cum_F_cycle_loss: 0.067\n",
            "[183:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:1000] Took 11.70s\n",
            "[183:1000] cum_D_X_loss: 0.068, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.065\n",
            "[183:1000] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.718, cum_F_cycle_loss: 0.062\n",
            "[183:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:1100] Took 11.69s\n",
            "[183:1100] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.068\n",
            "[183:1100] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.063\n",
            "[183:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[183:END] Completed epoch in 141.72379159927368s\n",
            "[183:1187] ep_D_X_loss: 0.081, ep_G_fool_loss: 0.771, ep_G_cycle_loss: 0.066\n",
            "[183:1187] ep_D_Y_loss: 0.065, ep_F_fool_loss: 0.712, ep_F_cycle_loss: 0.063\n",
            "[183:END] Completed eval in 1.6665420532226562s\n",
            "Updated G_opt learning rate from 3.564356435643565e-05 to 3.3663366336633676e-05\n",
            "Updated F_opt learning rate from 3.564356435643565e-05 to 3.3663366336633676e-05\n",
            "Updated D_X_opt learning rate from 3.564356435643565e-05 to 3.3663366336633676e-05\n",
            "Updated D_Y_opt learning rate from 3.564356435643565e-05 to 3.3663366336633676e-05\n",
            "[184:100] Took 14.16s\n",
            "[184:100] cum_D_X_loss: 0.092, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.068\n",
            "[184:100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.065\n",
            "[184:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:200] Took 11.69s\n",
            "[184:200] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.744, cum_G_cycle_loss: 0.064\n",
            "[184:200] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.730, cum_F_cycle_loss: 0.062\n",
            "[184:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:300] Took 11.70s\n",
            "[184:300] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.728, cum_G_cycle_loss: 0.067\n",
            "[184:300] cum_D_Y_loss: 0.090, cum_F_fool_loss: 0.740, cum_F_cycle_loss: 0.065\n",
            "[184:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:400] Took 11.69s\n",
            "[184:400] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.730, cum_G_cycle_loss: 0.062\n",
            "[184:400] cum_D_Y_loss: 0.081, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.065\n",
            "[184:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:500] Took 11.70s\n",
            "[184:500] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.756, cum_G_cycle_loss: 0.061\n",
            "[184:500] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.685, cum_F_cycle_loss: 0.060\n",
            "[184:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:600] Took 11.70s\n",
            "[184:600] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.721, cum_G_cycle_loss: 0.066\n",
            "[184:600] cum_D_Y_loss: 0.087, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.065\n",
            "[184:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:700] Took 11.70s\n",
            "[184:700] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.065\n",
            "[184:700] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.064\n",
            "[184:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:800] Took 11.68s\n",
            "[184:800] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.069\n",
            "[184:800] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.667, cum_F_cycle_loss: 0.064\n",
            "[184:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:900] Took 11.70s\n",
            "[184:900] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.065\n",
            "[184:900] cum_D_Y_loss: 0.058, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.062\n",
            "[184:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:1000] Took 11.69s\n",
            "[184:1000] cum_D_X_loss: 0.098, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.068\n",
            "[184:1000] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.064\n",
            "[184:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:1100] Took 11.69s\n",
            "[184:1100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.747, cum_G_cycle_loss: 0.063\n",
            "[184:1100] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.060\n",
            "[184:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[184:END] Completed epoch in 142.04363870620728s\n",
            "[184:1187] ep_D_X_loss: 0.085, ep_G_fool_loss: 0.754, ep_G_cycle_loss: 0.065\n",
            "[184:1187] ep_D_Y_loss: 0.070, ep_F_fool_loss: 0.709, ep_F_cycle_loss: 0.063\n",
            "[184:END] Completed eval in 1.576775312423706s\n",
            "Updated G_opt learning rate from 3.3663366336633676e-05 to 3.168316831683169e-05\n",
            "Updated F_opt learning rate from 3.3663366336633676e-05 to 3.168316831683169e-05\n",
            "Updated D_X_opt learning rate from 3.3663366336633676e-05 to 3.168316831683169e-05\n",
            "Updated D_Y_opt learning rate from 3.3663366336633676e-05 to 3.168316831683169e-05\n",
            "[185:100] Took 13.88s\n",
            "[185:100] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.065\n",
            "[185:100] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.729, cum_F_cycle_loss: 0.063\n",
            "[185:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:200] Took 11.69s\n",
            "[185:200] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.805, cum_G_cycle_loss: 0.066\n",
            "[185:200] cum_D_Y_loss: 0.057, cum_F_fool_loss: 0.744, cum_F_cycle_loss: 0.065\n",
            "[185:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:300] Took 11.70s\n",
            "[185:300] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.066\n",
            "[185:300] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.064\n",
            "[185:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:400] Took 11.70s\n",
            "[185:400] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.706, cum_G_cycle_loss: 0.068\n",
            "[185:400] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.704, cum_F_cycle_loss: 0.062\n",
            "[185:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:500] Took 11.69s\n",
            "[185:500] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.781, cum_G_cycle_loss: 0.069\n",
            "[185:500] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.062\n",
            "[185:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:600] Took 11.70s\n",
            "[185:600] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.065\n",
            "[185:600] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.066\n",
            "[185:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:700] Took 11.70s\n",
            "[185:700] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.809, cum_G_cycle_loss: 0.070\n",
            "[185:700] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.063\n",
            "[185:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:800] Took 11.69s\n",
            "[185:800] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.777, cum_G_cycle_loss: 0.066\n",
            "[185:800] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.658, cum_F_cycle_loss: 0.063\n",
            "[185:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:900] Took 11.69s\n",
            "[185:900] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.817, cum_G_cycle_loss: 0.062\n",
            "[185:900] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.735, cum_F_cycle_loss: 0.062\n",
            "[185:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:1000] Took 11.69s\n",
            "[185:1000] cum_D_X_loss: 0.065, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.062\n",
            "[185:1000] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.734, cum_F_cycle_loss: 0.064\n",
            "[185:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:1100] Took 11.69s\n",
            "[185:1100] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.700, cum_G_cycle_loss: 0.067\n",
            "[185:1100] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.671, cum_F_cycle_loss: 0.062\n",
            "[185:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[185:END] Completed epoch in 141.765362739563s\n",
            "[185:1187] ep_D_X_loss: 0.081, ep_G_fool_loss: 0.769, ep_G_cycle_loss: 0.066\n",
            "[185:1187] ep_D_Y_loss: 0.069, ep_F_fool_loss: 0.708, ep_F_cycle_loss: 0.063\n",
            "[185:END] Completed eval in 1.583763837814331s\n",
            "Updated G_opt learning rate from 3.168316831683169e-05 to 2.970297029702971e-05\n",
            "Updated F_opt learning rate from 3.168316831683169e-05 to 2.970297029702971e-05\n",
            "Updated D_X_opt learning rate from 3.168316831683169e-05 to 2.970297029702971e-05\n",
            "Updated D_Y_opt learning rate from 3.168316831683169e-05 to 2.970297029702971e-05\n",
            "[185:END] Saving models and training information\n",
            "[186:100] Took 13.86s\n",
            "[186:100] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.774, cum_G_cycle_loss: 0.065\n",
            "[186:100] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.729, cum_F_cycle_loss: 0.065\n",
            "[186:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:200] Took 11.69s\n",
            "[186:200] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.767, cum_G_cycle_loss: 0.065\n",
            "[186:200] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.063\n",
            "[186:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:300] Took 11.70s\n",
            "[186:300] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.794, cum_G_cycle_loss: 0.062\n",
            "[186:300] cum_D_Y_loss: 0.056, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.063\n",
            "[186:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:400] Took 11.70s\n",
            "[186:400] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.735, cum_G_cycle_loss: 0.067\n",
            "[186:400] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.064\n",
            "[186:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:500] Took 11.69s\n",
            "[186:500] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.822, cum_G_cycle_loss: 0.062\n",
            "[186:500] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.063\n",
            "[186:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:600] Took 11.69s\n",
            "[186:600] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.769, cum_G_cycle_loss: 0.066\n",
            "[186:600] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.062\n",
            "[186:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:700] Took 11.70s\n",
            "[186:700] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.744, cum_G_cycle_loss: 0.066\n",
            "[186:700] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.735, cum_F_cycle_loss: 0.062\n",
            "[186:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:800] Took 11.70s\n",
            "[186:800] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.722, cum_G_cycle_loss: 0.064\n",
            "[186:800] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.064\n",
            "[186:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:900] Took 11.69s\n",
            "[186:900] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.069\n",
            "[186:900] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.064\n",
            "[186:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:1000] Took 11.69s\n",
            "[186:1000] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.708, cum_G_cycle_loss: 0.064\n",
            "[186:1000] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.064\n",
            "[186:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:1100] Took 11.69s\n",
            "[186:1100] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.063\n",
            "[186:1100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.062\n",
            "[186:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[186:END] Completed epoch in 141.73330068588257s\n",
            "[186:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.759, ep_G_cycle_loss: 0.065\n",
            "[186:1187] ep_D_Y_loss: 0.065, ep_F_fool_loss: 0.716, ep_F_cycle_loss: 0.063\n",
            "[186:END] Completed eval in 1.591742753982544s\n",
            "Updated G_opt learning rate from 2.970297029702971e-05 to 2.7722772277227726e-05\n",
            "Updated F_opt learning rate from 2.970297029702971e-05 to 2.7722772277227726e-05\n",
            "Updated D_X_opt learning rate from 2.970297029702971e-05 to 2.7722772277227726e-05\n",
            "Updated D_Y_opt learning rate from 2.970297029702971e-05 to 2.7722772277227726e-05\n",
            "[187:100] Took 14.16s\n",
            "[187:100] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.064\n",
            "[187:100] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.749, cum_F_cycle_loss: 0.063\n",
            "[187:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:200] Took 11.69s\n",
            "[187:200] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.795, cum_G_cycle_loss: 0.063\n",
            "[187:200] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.062\n",
            "[187:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:300] Took 11.69s\n",
            "[187:300] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.062\n",
            "[187:300] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.060\n",
            "[187:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:400] Took 11.70s\n",
            "[187:400] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.766, cum_G_cycle_loss: 0.063\n",
            "[187:400] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.064\n",
            "[187:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:500] Took 11.69s\n",
            "[187:500] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.769, cum_G_cycle_loss: 0.065\n",
            "[187:500] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.721, cum_F_cycle_loss: 0.061\n",
            "[187:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:600] Took 11.85s\n",
            "[187:600] cum_D_X_loss: 0.069, cum_G_fool_loss: 0.766, cum_G_cycle_loss: 0.062\n",
            "[187:600] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.735, cum_F_cycle_loss: 0.066\n",
            "[187:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:700] Took 11.69s\n",
            "[187:700] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.736, cum_G_cycle_loss: 0.067\n",
            "[187:700] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.686, cum_F_cycle_loss: 0.063\n",
            "[187:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:800] Took 11.68s\n",
            "[187:800] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.739, cum_G_cycle_loss: 0.062\n",
            "[187:800] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.065\n",
            "[187:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:900] Took 11.70s\n",
            "[187:900] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.764, cum_G_cycle_loss: 0.064\n",
            "[187:900] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.708, cum_F_cycle_loss: 0.066\n",
            "[187:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:1000] Took 11.74s\n",
            "[187:1000] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.734, cum_G_cycle_loss: 0.065\n",
            "[187:1000] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.062\n",
            "[187:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:1100] Took 11.94s\n",
            "[187:1100] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.771, cum_G_cycle_loss: 0.064\n",
            "[187:1100] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.061\n",
            "[187:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[187:END] Completed epoch in 142.46595788002014s\n",
            "[187:1187] ep_D_X_loss: 0.079, ep_G_fool_loss: 0.761, ep_G_cycle_loss: 0.064\n",
            "[187:1187] ep_D_Y_loss: 0.069, ep_F_fool_loss: 0.710, ep_F_cycle_loss: 0.063\n",
            "[187:END] Completed eval in 1.6515486240386963s\n",
            "Updated G_opt learning rate from 2.7722772277227726e-05 to 2.5742574257425746e-05\n",
            "Updated F_opt learning rate from 2.7722772277227726e-05 to 2.5742574257425746e-05\n",
            "Updated D_X_opt learning rate from 2.7722772277227726e-05 to 2.5742574257425746e-05\n",
            "Updated D_Y_opt learning rate from 2.7722772277227726e-05 to 2.5742574257425746e-05\n",
            "[188:100] Took 13.91s\n",
            "[188:100] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.066\n",
            "[188:100] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.730, cum_F_cycle_loss: 0.062\n",
            "[188:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:200] Took 11.68s\n",
            "[188:200] cum_D_X_loss: 0.071, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.065\n",
            "[188:200] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.758, cum_F_cycle_loss: 0.061\n",
            "[188:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:300] Took 11.68s\n",
            "[188:300] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.772, cum_G_cycle_loss: 0.065\n",
            "[188:300] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.709, cum_F_cycle_loss: 0.061\n",
            "[188:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:400] Took 11.68s\n",
            "[188:400] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.795, cum_G_cycle_loss: 0.067\n",
            "[188:400] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.763, cum_F_cycle_loss: 0.063\n",
            "[188:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:500] Took 11.95s\n",
            "[188:500] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.064\n",
            "[188:500] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.759, cum_F_cycle_loss: 0.066\n",
            "[188:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:600] Took 12.31s\n",
            "[188:600] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.817, cum_G_cycle_loss: 0.064\n",
            "[188:600] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.061\n",
            "[188:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:700] Took 11.95s\n",
            "[188:700] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.744, cum_G_cycle_loss: 0.067\n",
            "[188:700] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.063\n",
            "[188:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:800] Took 12.09s\n",
            "[188:800] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.060\n",
            "[188:800] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.730, cum_F_cycle_loss: 0.062\n",
            "[188:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:900] Took 11.78s\n",
            "[188:900] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.067\n",
            "[188:900] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.060\n",
            "[188:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:1000] Took 11.94s\n",
            "[188:1000] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.782, cum_G_cycle_loss: 0.063\n",
            "[188:1000] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.730, cum_F_cycle_loss: 0.060\n",
            "[188:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:1100] Took 11.82s\n",
            "[188:1100] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.761, cum_G_cycle_loss: 0.063\n",
            "[188:1100] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.733, cum_F_cycle_loss: 0.064\n",
            "[188:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[188:END] Completed epoch in 143.93404388427734s\n",
            "[188:1187] ep_D_X_loss: 0.079, ep_G_fool_loss: 0.772, ep_G_cycle_loss: 0.064\n",
            "[188:1187] ep_D_Y_loss: 0.065, ep_F_fool_loss: 0.732, ep_F_cycle_loss: 0.062\n",
            "[188:END] Completed eval in 1.7164084911346436s\n",
            "Updated G_opt learning rate from 2.5742574257425746e-05 to 2.3762376237623762e-05\n",
            "Updated F_opt learning rate from 2.5742574257425746e-05 to 2.3762376237623762e-05\n",
            "Updated D_X_opt learning rate from 2.5742574257425746e-05 to 2.3762376237623762e-05\n",
            "Updated D_Y_opt learning rate from 2.5742574257425746e-05 to 2.3762376237623762e-05\n",
            "[189:100] Took 14.18s\n",
            "[189:100] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.788, cum_G_cycle_loss: 0.071\n",
            "[189:100] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.733, cum_F_cycle_loss: 0.062\n",
            "[189:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:200] Took 12.16s\n",
            "[189:200] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.726, cum_G_cycle_loss: 0.063\n",
            "[189:200] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.062\n",
            "[189:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:300] Took 12.28s\n",
            "[189:300] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.788, cum_G_cycle_loss: 0.066\n",
            "[189:300] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.676, cum_F_cycle_loss: 0.064\n",
            "[189:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:400] Took 12.18s\n",
            "[189:400] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.788, cum_G_cycle_loss: 0.062\n",
            "[189:400] cum_D_Y_loss: 0.055, cum_F_fool_loss: 0.731, cum_F_cycle_loss: 0.065\n",
            "[189:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:500] Took 12.12s\n",
            "[189:500] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.781, cum_G_cycle_loss: 0.063\n",
            "[189:500] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.065\n",
            "[189:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:600] Took 11.94s\n",
            "[189:600] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.062\n",
            "[189:600] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.690, cum_F_cycle_loss: 0.061\n",
            "[189:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:700] Took 11.95s\n",
            "[189:700] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.066\n",
            "[189:700] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.061\n",
            "[189:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:800] Took 12.00s\n",
            "[189:800] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.804, cum_G_cycle_loss: 0.067\n",
            "[189:800] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.058\n",
            "[189:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:900] Took 12.28s\n",
            "[189:900] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.062\n",
            "[189:900] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.705, cum_F_cycle_loss: 0.066\n",
            "[189:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:1000] Took 12.01s\n",
            "[189:1000] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.062\n",
            "[189:1000] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.772, cum_F_cycle_loss: 0.061\n",
            "[189:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:1100] Took 12.25s\n",
            "[189:1100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.067\n",
            "[189:1100] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.729, cum_F_cycle_loss: 0.063\n",
            "[189:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[189:END] Completed epoch in 146.64441776275635s\n",
            "[189:1187] ep_D_X_loss: 0.081, ep_G_fool_loss: 0.772, ep_G_cycle_loss: 0.065\n",
            "[189:1187] ep_D_Y_loss: 0.065, ep_F_fool_loss: 0.711, ep_F_cycle_loss: 0.063\n",
            "[189:END] Completed eval in 1.7084050178527832s\n",
            "Updated G_opt learning rate from 2.3762376237623762e-05 to 2.178217821782178e-05\n",
            "Updated F_opt learning rate from 2.3762376237623762e-05 to 2.178217821782178e-05\n",
            "Updated D_X_opt learning rate from 2.3762376237623762e-05 to 2.178217821782178e-05\n",
            "Updated D_Y_opt learning rate from 2.3762376237623762e-05 to 2.178217821782178e-05\n",
            "[190:100] Took 14.22s\n",
            "[190:100] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.755, cum_G_cycle_loss: 0.063\n",
            "[190:100] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.689, cum_F_cycle_loss: 0.064\n",
            "[190:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:200] Took 11.96s\n",
            "[190:200] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.063\n",
            "[190:200] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.061\n",
            "[190:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:300] Took 12.29s\n",
            "[190:300] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.062\n",
            "[190:300] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.736, cum_F_cycle_loss: 0.059\n",
            "[190:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:400] Took 12.16s\n",
            "[190:400] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.783, cum_G_cycle_loss: 0.063\n",
            "[190:400] cum_D_Y_loss: 0.057, cum_F_fool_loss: 0.712, cum_F_cycle_loss: 0.063\n",
            "[190:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:500] Took 12.42s\n",
            "[190:500] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.753, cum_G_cycle_loss: 0.065\n",
            "[190:500] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.721, cum_F_cycle_loss: 0.064\n",
            "[190:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:600] Took 12.10s\n",
            "[190:600] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.761, cum_G_cycle_loss: 0.061\n",
            "[190:600] cum_D_Y_loss: 0.082, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.061\n",
            "[190:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:700] Took 11.89s\n",
            "[190:700] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.746, cum_G_cycle_loss: 0.063\n",
            "[190:700] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.728, cum_F_cycle_loss: 0.065\n",
            "[190:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:800] Took 12.03s\n",
            "[190:800] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.065\n",
            "[190:800] cum_D_Y_loss: 0.056, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.065\n",
            "[190:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:900] Took 12.31s\n",
            "[190:900] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.062\n",
            "[190:900] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.759, cum_F_cycle_loss: 0.060\n",
            "[190:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:1000] Took 11.95s\n",
            "[190:1000] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.766, cum_G_cycle_loss: 0.064\n",
            "[190:1000] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.677, cum_F_cycle_loss: 0.063\n",
            "[190:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:1100] Took 11.69s\n",
            "[190:1100] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.792, cum_G_cycle_loss: 0.063\n",
            "[190:1100] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.752, cum_F_cycle_loss: 0.066\n",
            "[190:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[190:END] Completed epoch in 145.9864468574524s\n",
            "[190:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.761, ep_G_cycle_loss: 0.063\n",
            "[190:1187] ep_D_Y_loss: 0.066, ep_F_fool_loss: 0.715, ep_F_cycle_loss: 0.063\n",
            "[190:END] Completed eval in 1.6665725708007812s\n",
            "Updated G_opt learning rate from 2.178217821782178e-05 to 1.98019801980198e-05\n",
            "Updated F_opt learning rate from 2.178217821782178e-05 to 1.98019801980198e-05\n",
            "Updated D_X_opt learning rate from 2.178217821782178e-05 to 1.98019801980198e-05\n",
            "Updated D_Y_opt learning rate from 2.178217821782178e-05 to 1.98019801980198e-05\n",
            "[190:END] Saving models and training information\n",
            "[191:100] Took 13.87s\n",
            "[191:100] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.793, cum_G_cycle_loss: 0.068\n",
            "[191:100] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.734, cum_F_cycle_loss: 0.064\n",
            "[191:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:200] Took 11.70s\n",
            "[191:200] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.062\n",
            "[191:200] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.061\n",
            "[191:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:300] Took 11.69s\n",
            "[191:300] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.059\n",
            "[191:300] cum_D_Y_loss: 0.083, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.062\n",
            "[191:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:400] Took 11.69s\n",
            "[191:400] cum_D_X_loss: 0.067, cum_G_fool_loss: 0.767, cum_G_cycle_loss: 0.059\n",
            "[191:400] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.743, cum_F_cycle_loss: 0.064\n",
            "[191:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:500] Took 11.69s\n",
            "[191:500] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.783, cum_G_cycle_loss: 0.064\n",
            "[191:500] cum_D_Y_loss: 0.055, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.062\n",
            "[191:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:600] Took 11.69s\n",
            "[191:600] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.765, cum_G_cycle_loss: 0.061\n",
            "[191:600] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.742, cum_F_cycle_loss: 0.063\n",
            "[191:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:700] Took 11.68s\n",
            "[191:700] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.797, cum_G_cycle_loss: 0.064\n",
            "[191:700] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.062\n",
            "[191:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:800] Took 11.69s\n",
            "[191:800] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.776, cum_G_cycle_loss: 0.064\n",
            "[191:800] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.680, cum_F_cycle_loss: 0.065\n",
            "[191:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:900] Took 11.76s\n",
            "[191:900] cum_D_X_loss: 0.095, cum_G_fool_loss: 0.771, cum_G_cycle_loss: 0.063\n",
            "[191:900] cum_D_Y_loss: 0.056, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.062\n",
            "[191:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:1000] Took 11.75s\n",
            "[191:1000] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.771, cum_G_cycle_loss: 0.064\n",
            "[191:1000] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.763, cum_F_cycle_loss: 0.062\n",
            "[191:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:1100] Took 11.77s\n",
            "[191:1100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.811, cum_G_cycle_loss: 0.061\n",
            "[191:1100] cum_D_Y_loss: 0.061, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.066\n",
            "[191:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[191:END] Completed epoch in 141.9173300266266s\n",
            "[191:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.776, ep_G_cycle_loss: 0.063\n",
            "[191:1187] ep_D_Y_loss: 0.064, ep_F_fool_loss: 0.720, ep_F_cycle_loss: 0.063\n",
            "[191:END] Completed eval in 1.6695332527160645s\n",
            "Updated G_opt learning rate from 1.98019801980198e-05 to 1.7821782178217816e-05\n",
            "Updated F_opt learning rate from 1.98019801980198e-05 to 1.7821782178217816e-05\n",
            "Updated D_X_opt learning rate from 1.98019801980198e-05 to 1.7821782178217816e-05\n",
            "Updated D_Y_opt learning rate from 1.98019801980198e-05 to 1.7821782178217816e-05\n",
            "[192:100] Took 13.87s\n",
            "[192:100] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.066\n",
            "[192:100] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.064\n",
            "[192:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:200] Took 11.69s\n",
            "[192:200] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.786, cum_G_cycle_loss: 0.063\n",
            "[192:200] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.063\n",
            "[192:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:300] Took 11.69s\n",
            "[192:300] cum_D_X_loss: 0.087, cum_G_fool_loss: 0.780, cum_G_cycle_loss: 0.060\n",
            "[192:300] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.062\n",
            "[192:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:400] Took 11.69s\n",
            "[192:400] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.771, cum_G_cycle_loss: 0.064\n",
            "[192:400] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.746, cum_F_cycle_loss: 0.060\n",
            "[192:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:500] Took 11.69s\n",
            "[192:500] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.063\n",
            "[192:500] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.059\n",
            "[192:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:600] Took 11.69s\n",
            "[192:600] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.063\n",
            "[192:600] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.061\n",
            "[192:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:700] Took 11.73s\n",
            "[192:700] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.742, cum_G_cycle_loss: 0.065\n",
            "[192:700] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.065\n",
            "[192:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:800] Took 12.11s\n",
            "[192:800] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.795, cum_G_cycle_loss: 0.061\n",
            "[192:800] cum_D_Y_loss: 0.058, cum_F_fool_loss: 0.750, cum_F_cycle_loss: 0.066\n",
            "[192:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:900] Took 11.72s\n",
            "[192:900] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.771, cum_G_cycle_loss: 0.063\n",
            "[192:900] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.702, cum_F_cycle_loss: 0.068\n",
            "[192:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:1000] Took 11.92s\n",
            "[192:1000] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.772, cum_G_cycle_loss: 0.060\n",
            "[192:1000] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.063\n",
            "[192:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:1100] Took 12.16s\n",
            "[192:1100] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.764, cum_G_cycle_loss: 0.063\n",
            "[192:1100] cum_D_Y_loss: 0.058, cum_F_fool_loss: 0.750, cum_F_cycle_loss: 0.064\n",
            "[192:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[192:END] Completed epoch in 143.3254430294037s\n",
            "[192:1187] ep_D_X_loss: 0.078, ep_G_fool_loss: 0.772, ep_G_cycle_loss: 0.063\n",
            "[192:1187] ep_D_Y_loss: 0.065, ep_F_fool_loss: 0.716, ep_F_cycle_loss: 0.063\n",
            "[192:END] Completed eval in 1.6924729347229004s\n",
            "Updated G_opt learning rate from 1.7821782178217816e-05 to 1.5841584158415833e-05\n",
            "Updated F_opt learning rate from 1.7821782178217816e-05 to 1.5841584158415833e-05\n",
            "Updated D_X_opt learning rate from 1.7821782178217816e-05 to 1.5841584158415833e-05\n",
            "Updated D_Y_opt learning rate from 1.7821782178217816e-05 to 1.5841584158415833e-05\n",
            "[193:100] Took 14.11s\n",
            "[193:100] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.061\n",
            "[193:100] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.741, cum_F_cycle_loss: 0.059\n",
            "[193:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:200] Took 11.92s\n",
            "[193:200] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.060\n",
            "[193:200] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.711, cum_F_cycle_loss: 0.064\n",
            "[193:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:300] Took 11.78s\n",
            "[193:300] cum_D_X_loss: 0.071, cum_G_fool_loss: 0.760, cum_G_cycle_loss: 0.061\n",
            "[193:300] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.766, cum_F_cycle_loss: 0.061\n",
            "[193:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:400] Took 11.98s\n",
            "[193:400] cum_D_X_loss: 0.069, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.063\n",
            "[193:400] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.750, cum_F_cycle_loss: 0.063\n",
            "[193:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:500] Took 11.69s\n",
            "[193:500] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.810, cum_G_cycle_loss: 0.063\n",
            "[193:500] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.718, cum_F_cycle_loss: 0.059\n",
            "[193:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:600] Took 11.69s\n",
            "[193:600] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.772, cum_G_cycle_loss: 0.062\n",
            "[193:600] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.060\n",
            "[193:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:700] Took 12.06s\n",
            "[193:700] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.789, cum_G_cycle_loss: 0.063\n",
            "[193:700] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.674, cum_F_cycle_loss: 0.062\n",
            "[193:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:800] Took 11.77s\n",
            "[193:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.767, cum_G_cycle_loss: 0.061\n",
            "[193:800] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.713, cum_F_cycle_loss: 0.062\n",
            "[193:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:900] Took 11.92s\n",
            "[193:900] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.774, cum_G_cycle_loss: 0.062\n",
            "[193:900] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.744, cum_F_cycle_loss: 0.060\n",
            "[193:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:1000] Took 12.09s\n",
            "[193:1000] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.782, cum_G_cycle_loss: 0.062\n",
            "[193:1000] cum_D_Y_loss: 0.053, cum_F_fool_loss: 0.734, cum_F_cycle_loss: 0.062\n",
            "[193:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:1100] Took 11.75s\n",
            "[193:1100] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.804, cum_G_cycle_loss: 0.059\n",
            "[193:1100] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.715, cum_F_cycle_loss: 0.061\n",
            "[193:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[193:END] Completed epoch in 143.67532467842102s\n",
            "[193:1187] ep_D_X_loss: 0.080, ep_G_fool_loss: 0.775, ep_G_cycle_loss: 0.062\n",
            "[193:1187] ep_D_Y_loss: 0.063, ep_F_fool_loss: 0.722, ep_F_cycle_loss: 0.061\n",
            "[193:END] Completed eval in 1.6934983730316162s\n",
            "Updated G_opt learning rate from 1.5841584158415833e-05 to 1.3861386138613853e-05\n",
            "Updated F_opt learning rate from 1.5841584158415833e-05 to 1.3861386138613853e-05\n",
            "Updated D_X_opt learning rate from 1.5841584158415833e-05 to 1.3861386138613853e-05\n",
            "Updated D_Y_opt learning rate from 1.5841584158415833e-05 to 1.3861386138613853e-05\n",
            "[194:100] Took 14.27s\n",
            "[194:100] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.797, cum_G_cycle_loss: 0.066\n",
            "[194:100] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.709, cum_F_cycle_loss: 0.066\n",
            "[194:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:200] Took 11.98s\n",
            "[194:200] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.789, cum_G_cycle_loss: 0.062\n",
            "[194:200] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.706, cum_F_cycle_loss: 0.061\n",
            "[194:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:300] Took 11.97s\n",
            "[194:300] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.751, cum_G_cycle_loss: 0.064\n",
            "[194:300] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.066\n",
            "[194:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:400] Took 12.08s\n",
            "[194:400] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.798, cum_G_cycle_loss: 0.059\n",
            "[194:400] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.748, cum_F_cycle_loss: 0.062\n",
            "[194:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:500] Took 11.92s\n",
            "[194:500] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.792, cum_G_cycle_loss: 0.063\n",
            "[194:500] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.693, cum_F_cycle_loss: 0.065\n",
            "[194:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:600] Took 11.97s\n",
            "[194:600] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.795, cum_G_cycle_loss: 0.064\n",
            "[194:600] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.742, cum_F_cycle_loss: 0.060\n",
            "[194:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:700] Took 12.52s\n",
            "[194:700] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.823, cum_G_cycle_loss: 0.059\n",
            "[194:700] cum_D_Y_loss: 0.055, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.065\n",
            "[194:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:800] Took 12.23s\n",
            "[194:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.063\n",
            "[194:800] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.698, cum_F_cycle_loss: 0.063\n",
            "[194:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:900] Took 12.07s\n",
            "[194:900] cum_D_X_loss: 0.090, cum_G_fool_loss: 0.793, cum_G_cycle_loss: 0.064\n",
            "[194:900] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.707, cum_F_cycle_loss: 0.060\n",
            "[194:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:1000] Took 11.76s\n",
            "[194:1000] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.763, cum_G_cycle_loss: 0.060\n",
            "[194:1000] cum_D_Y_loss: 0.063, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.059\n",
            "[194:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:1100] Took 11.70s\n",
            "[194:1100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.063\n",
            "[194:1100] cum_D_Y_loss: 0.070, cum_F_fool_loss: 0.747, cum_F_cycle_loss: 0.062\n",
            "[194:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[194:END] Completed epoch in 145.4898898601532s\n",
            "[194:1187] ep_D_X_loss: 0.082, ep_G_fool_loss: 0.783, ep_G_cycle_loss: 0.062\n",
            "[194:1187] ep_D_Y_loss: 0.067, ep_F_fool_loss: 0.713, ep_F_cycle_loss: 0.063\n",
            "[194:END] Completed eval in 1.7371184825897217s\n",
            "Updated G_opt learning rate from 1.3861386138613853e-05 to 1.1881188118811893e-05\n",
            "Updated F_opt learning rate from 1.3861386138613853e-05 to 1.1881188118811893e-05\n",
            "Updated D_X_opt learning rate from 1.3861386138613853e-05 to 1.1881188118811893e-05\n",
            "Updated D_Y_opt learning rate from 1.3861386138613853e-05 to 1.1881188118811893e-05\n",
            "[195:100] Took 14.48s\n",
            "[195:100] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.768, cum_G_cycle_loss: 0.062\n",
            "[195:100] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.777, cum_F_cycle_loss: 0.065\n",
            "[195:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:200] Took 11.92s\n",
            "[195:200] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.762, cum_G_cycle_loss: 0.060\n",
            "[195:200] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.062\n",
            "[195:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:300] Took 11.86s\n",
            "[195:300] cum_D_X_loss: 0.088, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.061\n",
            "[195:300] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.701, cum_F_cycle_loss: 0.062\n",
            "[195:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:400] Took 12.23s\n",
            "[195:400] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.809, cum_G_cycle_loss: 0.061\n",
            "[195:400] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.063\n",
            "[195:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:500] Took 12.16s\n",
            "[195:500] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.740, cum_G_cycle_loss: 0.062\n",
            "[195:500] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.060\n",
            "[195:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:600] Took 11.96s\n",
            "[195:600] cum_D_X_loss: 0.066, cum_G_fool_loss: 0.802, cum_G_cycle_loss: 0.063\n",
            "[195:600] cum_D_Y_loss: 0.060, cum_F_fool_loss: 0.753, cum_F_cycle_loss: 0.061\n",
            "[195:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:700] Took 12.19s\n",
            "[195:700] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.814, cum_G_cycle_loss: 0.060\n",
            "[195:700] cum_D_Y_loss: 0.055, cum_F_fool_loss: 0.700, cum_F_cycle_loss: 0.065\n",
            "[195:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:800] Took 12.11s\n",
            "[195:800] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.790, cum_G_cycle_loss: 0.067\n",
            "[195:800] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.734, cum_F_cycle_loss: 0.061\n",
            "[195:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:900] Took 12.22s\n",
            "[195:900] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.791, cum_G_cycle_loss: 0.066\n",
            "[195:900] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.065\n",
            "[195:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:1000] Took 12.09s\n",
            "[195:1000] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.067\n",
            "[195:1000] cum_D_Y_loss: 0.054, cum_F_fool_loss: 0.751, cum_F_cycle_loss: 0.062\n",
            "[195:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:1100] Took 12.42s\n",
            "[195:1100] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.757, cum_G_cycle_loss: 0.061\n",
            "[195:1100] cum_D_Y_loss: 0.072, cum_F_fool_loss: 0.695, cum_F_cycle_loss: 0.064\n",
            "[195:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[195:END] Completed epoch in 146.78839683532715s\n",
            "[195:1187] ep_D_X_loss: 0.078, ep_G_fool_loss: 0.776, ep_G_cycle_loss: 0.062\n",
            "[195:1187] ep_D_Y_loss: 0.065, ep_F_fool_loss: 0.722, ep_F_cycle_loss: 0.063\n",
            "[195:END] Completed eval in 1.8161718845367432s\n",
            "Updated G_opt learning rate from 1.1881188118811893e-05 to 9.90099009900991e-06\n",
            "Updated F_opt learning rate from 1.1881188118811893e-05 to 9.90099009900991e-06\n",
            "Updated D_X_opt learning rate from 1.1881188118811893e-05 to 9.90099009900991e-06\n",
            "Updated D_Y_opt learning rate from 1.1881188118811893e-05 to 9.90099009900991e-06\n",
            "[195:END] Saving models and training information\n",
            "[196:100] Took 14.93s\n",
            "[196:100] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.063\n",
            "[196:100] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.699, cum_F_cycle_loss: 0.066\n",
            "[196:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:200] Took 11.90s\n",
            "[196:200] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.756, cum_G_cycle_loss: 0.063\n",
            "[196:200] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.064\n",
            "[196:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:300] Took 11.92s\n",
            "[196:300] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.809, cum_G_cycle_loss: 0.062\n",
            "[196:300] cum_D_Y_loss: 0.058, cum_F_fool_loss: 0.696, cum_F_cycle_loss: 0.059\n",
            "[196:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:400] Took 12.22s\n",
            "[196:400] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.058\n",
            "[196:400] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.670, cum_F_cycle_loss: 0.059\n",
            "[196:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:500] Took 12.15s\n",
            "[196:500] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.744, cum_G_cycle_loss: 0.065\n",
            "[196:500] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.714, cum_F_cycle_loss: 0.061\n",
            "[196:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:600] Took 11.87s\n",
            "[196:600] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.063\n",
            "[196:600] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.727, cum_F_cycle_loss: 0.067\n",
            "[196:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:700] Took 12.32s\n",
            "[196:700] cum_D_X_loss: 0.070, cum_G_fool_loss: 0.774, cum_G_cycle_loss: 0.064\n",
            "[196:700] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.758, cum_F_cycle_loss: 0.062\n",
            "[196:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:800] Took 12.08s\n",
            "[196:800] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.793, cum_G_cycle_loss: 0.062\n",
            "[196:800] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.733, cum_F_cycle_loss: 0.064\n",
            "[196:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:900] Took 11.72s\n",
            "[196:900] cum_D_X_loss: 0.089, cum_G_fool_loss: 0.738, cum_G_cycle_loss: 0.067\n",
            "[196:900] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.687, cum_F_cycle_loss: 0.065\n",
            "[196:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:1000] Took 11.69s\n",
            "[196:1000] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.784, cum_G_cycle_loss: 0.059\n",
            "[196:1000] cum_D_Y_loss: 0.057, cum_F_fool_loss: 0.714, cum_F_cycle_loss: 0.065\n",
            "[196:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:1100] Took 11.70s\n",
            "[196:1100] cum_D_X_loss: 0.067, cum_G_fool_loss: 0.775, cum_G_cycle_loss: 0.059\n",
            "[196:1100] cum_D_Y_loss: 0.057, cum_F_fool_loss: 0.726, cum_F_cycle_loss: 0.060\n",
            "[196:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[196:END] Completed epoch in 145.6333405971527s\n",
            "[196:1187] ep_D_X_loss: 0.078, ep_G_fool_loss: 0.773, ep_G_cycle_loss: 0.062\n",
            "[196:1187] ep_D_Y_loss: 0.065, ep_F_fool_loss: 0.712, ep_F_cycle_loss: 0.063\n",
            "[196:END] Completed eval in 1.7732856273651123s\n",
            "Updated G_opt learning rate from 9.90099009900991e-06 to 7.920792079207928e-06\n",
            "Updated F_opt learning rate from 9.90099009900991e-06 to 7.920792079207928e-06\n",
            "Updated D_X_opt learning rate from 9.90099009900991e-06 to 7.920792079207928e-06\n",
            "Updated D_Y_opt learning rate from 9.90099009900991e-06 to 7.920792079207928e-06\n",
            "[197:100] Took 13.94s\n",
            "[197:100] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.794, cum_G_cycle_loss: 0.059\n",
            "[197:100] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.716, cum_F_cycle_loss: 0.065\n",
            "[197:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:200] Took 11.68s\n",
            "[197:200] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.062\n",
            "[197:200] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.061\n",
            "[197:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:300] Took 11.68s\n",
            "[197:300] cum_D_X_loss: 0.093, cum_G_fool_loss: 0.744, cum_G_cycle_loss: 0.062\n",
            "[197:300] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.694, cum_F_cycle_loss: 0.069\n",
            "[197:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:400] Took 11.68s\n",
            "[197:400] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.794, cum_G_cycle_loss: 0.065\n",
            "[197:400] cum_D_Y_loss: 0.067, cum_F_fool_loss: 0.745, cum_F_cycle_loss: 0.061\n",
            "[197:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:500] Took 12.26s\n",
            "[197:500] cum_D_X_loss: 0.068, cum_G_fool_loss: 0.756, cum_G_cycle_loss: 0.060\n",
            "[197:500] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.767, cum_F_cycle_loss: 0.060\n",
            "[197:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:600] Took 11.68s\n",
            "[197:600] cum_D_X_loss: 0.070, cum_G_fool_loss: 0.748, cum_G_cycle_loss: 0.060\n",
            "[197:600] cum_D_Y_loss: 0.073, cum_F_fool_loss: 0.737, cum_F_cycle_loss: 0.063\n",
            "[197:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:700] Took 11.68s\n",
            "[197:700] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.784, cum_G_cycle_loss: 0.061\n",
            "[197:700] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.753, cum_F_cycle_loss: 0.059\n",
            "[197:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:800] Took 11.80s\n",
            "[197:800] cum_D_X_loss: 0.084, cum_G_fool_loss: 0.758, cum_G_cycle_loss: 0.066\n",
            "[197:800] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.745, cum_F_cycle_loss: 0.059\n",
            "[197:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:900] Took 11.74s\n",
            "[197:900] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.788, cum_G_cycle_loss: 0.062\n",
            "[197:900] cum_D_Y_loss: 0.055, cum_F_fool_loss: 0.785, cum_F_cycle_loss: 0.057\n",
            "[197:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:1000] Took 11.69s\n",
            "[197:1000] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.788, cum_G_cycle_loss: 0.059\n",
            "[197:1000] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.677, cum_F_cycle_loss: 0.064\n",
            "[197:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:1100] Took 11.70s\n",
            "[197:1100] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.759, cum_G_cycle_loss: 0.061\n",
            "[197:1100] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.754, cum_F_cycle_loss: 0.061\n",
            "[197:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[197:END] Completed epoch in 142.45510005950928s\n",
            "[197:1187] ep_D_X_loss: 0.080, ep_G_fool_loss: 0.773, ep_G_cycle_loss: 0.062\n",
            "[197:1187] ep_D_Y_loss: 0.064, ep_F_fool_loss: 0.732, ep_F_cycle_loss: 0.062\n",
            "[197:END] Completed eval in 1.7822320461273193s\n",
            "Updated G_opt learning rate from 7.920792079207928e-06 to 5.9405940594059465e-06\n",
            "Updated F_opt learning rate from 7.920792079207928e-06 to 5.9405940594059465e-06\n",
            "Updated D_X_opt learning rate from 7.920792079207928e-06 to 5.9405940594059465e-06\n",
            "Updated D_Y_opt learning rate from 7.920792079207928e-06 to 5.9405940594059465e-06\n",
            "[198:100] Took 13.91s\n",
            "[198:100] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.789, cum_G_cycle_loss: 0.059\n",
            "[198:100] cum_D_Y_loss: 0.053, cum_F_fool_loss: 0.710, cum_F_cycle_loss: 0.063\n",
            "[198:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:200] Took 11.88s\n",
            "[198:200] cum_D_X_loss: 0.074, cum_G_fool_loss: 0.785, cum_G_cycle_loss: 0.060\n",
            "[198:200] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.736, cum_F_cycle_loss: 0.063\n",
            "[198:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:300] Took 11.69s\n",
            "[198:300] cum_D_X_loss: 0.071, cum_G_fool_loss: 0.741, cum_G_cycle_loss: 0.060\n",
            "[198:300] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.746, cum_F_cycle_loss: 0.061\n",
            "[198:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:400] Took 11.70s\n",
            "[198:400] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.746, cum_G_cycle_loss: 0.060\n",
            "[198:400] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.063\n",
            "[198:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:500] Took 11.77s\n",
            "[198:500] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.061\n",
            "[198:500] cum_D_Y_loss: 0.080, cum_F_fool_loss: 0.742, cum_F_cycle_loss: 0.065\n",
            "[198:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:600] Took 12.39s\n",
            "[198:600] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.733, cum_G_cycle_loss: 0.060\n",
            "[198:600] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.706, cum_F_cycle_loss: 0.064\n",
            "[198:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:700] Took 11.68s\n",
            "[198:700] cum_D_X_loss: 0.078, cum_G_fool_loss: 0.719, cum_G_cycle_loss: 0.059\n",
            "[198:700] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.719, cum_F_cycle_loss: 0.060\n",
            "[198:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:800] Took 11.68s\n",
            "[198:800] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.064\n",
            "[198:800] cum_D_Y_loss: 0.065, cum_F_fool_loss: 0.732, cum_F_cycle_loss: 0.063\n",
            "[198:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:900] Took 11.67s\n",
            "[198:900] cum_D_X_loss: 0.072, cum_G_fool_loss: 0.751, cum_G_cycle_loss: 0.063\n",
            "[198:900] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.064\n",
            "[198:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:1000] Took 11.68s\n",
            "[198:1000] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.780, cum_G_cycle_loss: 0.064\n",
            "[198:1000] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.725, cum_F_cycle_loss: 0.061\n",
            "[198:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:1100] Took 11.68s\n",
            "[198:1100] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.813, cum_G_cycle_loss: 0.062\n",
            "[198:1100] cum_D_Y_loss: 0.055, cum_F_fool_loss: 0.752, cum_F_cycle_loss: 0.062\n",
            "[198:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[198:END] Completed epoch in 142.65343165397644s\n",
            "[198:1187] ep_D_X_loss: 0.077, ep_G_fool_loss: 0.762, ep_G_cycle_loss: 0.061\n",
            "[198:1187] ep_D_Y_loss: 0.067, ep_F_fool_loss: 0.728, ep_F_cycle_loss: 0.063\n",
            "[198:END] Completed eval in 1.7632601261138916s\n",
            "Updated G_opt learning rate from 5.9405940594059465e-06 to 3.960396039603964e-06\n",
            "Updated F_opt learning rate from 5.9405940594059465e-06 to 3.960396039603964e-06\n",
            "Updated D_X_opt learning rate from 5.9405940594059465e-06 to 3.960396039603964e-06\n",
            "Updated D_Y_opt learning rate from 5.9405940594059465e-06 to 3.960396039603964e-06\n",
            "[199:100] Took 13.86s\n",
            "[199:100] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.779, cum_G_cycle_loss: 0.062\n",
            "[199:100] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.762, cum_F_cycle_loss: 0.063\n",
            "[199:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:200] Took 11.68s\n",
            "[199:200] cum_D_X_loss: 0.081, cum_G_fool_loss: 0.789, cum_G_cycle_loss: 0.062\n",
            "[199:200] cum_D_Y_loss: 0.062, cum_F_fool_loss: 0.723, cum_F_cycle_loss: 0.063\n",
            "[199:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:300] Took 11.68s\n",
            "[199:300] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.835, cum_G_cycle_loss: 0.065\n",
            "[199:300] cum_D_Y_loss: 0.054, cum_F_fool_loss: 0.724, cum_F_cycle_loss: 0.063\n",
            "[199:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:400] Took 11.68s\n",
            "[199:400] cum_D_X_loss: 0.069, cum_G_fool_loss: 0.795, cum_G_cycle_loss: 0.059\n",
            "[199:400] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.730, cum_F_cycle_loss: 0.062\n",
            "[199:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:500] Took 11.67s\n",
            "[199:500] cum_D_X_loss: 0.075, cum_G_fool_loss: 0.732, cum_G_cycle_loss: 0.060\n",
            "[199:500] cum_D_Y_loss: 0.074, cum_F_fool_loss: 0.697, cum_F_cycle_loss: 0.065\n",
            "[199:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:600] Took 11.68s\n",
            "[199:600] cum_D_X_loss: 0.073, cum_G_fool_loss: 0.770, cum_G_cycle_loss: 0.061\n",
            "[199:600] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.692, cum_F_cycle_loss: 0.059\n",
            "[199:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:700] Took 11.68s\n",
            "[199:700] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.771, cum_G_cycle_loss: 0.062\n",
            "[199:700] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.717, cum_F_cycle_loss: 0.062\n",
            "[199:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:800] Took 11.68s\n",
            "[199:800] cum_D_X_loss: 0.079, cum_G_fool_loss: 0.754, cum_G_cycle_loss: 0.060\n",
            "[199:800] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.754, cum_F_cycle_loss: 0.062\n",
            "[199:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:900] Took 11.67s\n",
            "[199:900] cum_D_X_loss: 0.065, cum_G_fool_loss: 0.761, cum_G_cycle_loss: 0.060\n",
            "[199:900] cum_D_Y_loss: 0.068, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.061\n",
            "[199:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:1000] Took 11.68s\n",
            "[199:1000] cum_D_X_loss: 0.068, cum_G_fool_loss: 0.745, cum_G_cycle_loss: 0.059\n",
            "[199:1000] cum_D_Y_loss: 0.069, cum_F_fool_loss: 0.773, cum_F_cycle_loss: 0.062\n",
            "[199:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:1100] Took 11.69s\n",
            "[199:1100] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.058\n",
            "[199:1100] cum_D_Y_loss: 0.066, cum_F_fool_loss: 0.766, cum_F_cycle_loss: 0.063\n",
            "[199:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[199:END] Completed epoch in 141.59120655059814s\n",
            "[199:1187] ep_D_X_loss: 0.076, ep_G_fool_loss: 0.773, ep_G_cycle_loss: 0.060\n",
            "[199:1187] ep_D_Y_loss: 0.066, ep_F_fool_loss: 0.733, ep_F_cycle_loss: 0.062\n",
            "[199:END] Completed eval in 1.7662749290466309s\n",
            "Updated G_opt learning rate from 3.960396039603964e-06 to 1.980198019801982e-06\n",
            "Updated F_opt learning rate from 3.960396039603964e-06 to 1.980198019801982e-06\n",
            "Updated D_X_opt learning rate from 3.960396039603964e-06 to 1.980198019801982e-06\n",
            "Updated D_Y_opt learning rate from 3.960396039603964e-06 to 1.980198019801982e-06\n",
            "[200:100] Took 13.94s\n",
            "[200:100] cum_D_X_loss: 0.067, cum_G_fool_loss: 0.819, cum_G_cycle_loss: 0.060\n",
            "[200:100] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.764, cum_F_cycle_loss: 0.065\n",
            "[200:100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:200] Took 11.67s\n",
            "[200:200] cum_D_X_loss: 0.077, cum_G_fool_loss: 0.756, cum_G_cycle_loss: 0.064\n",
            "[200:200] cum_D_Y_loss: 0.076, cum_F_fool_loss: 0.738, cum_F_cycle_loss: 0.063\n",
            "[200:200] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:300] Took 11.68s\n",
            "[200:300] cum_D_X_loss: 0.076, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.060\n",
            "[200:300] cum_D_Y_loss: 0.058, cum_F_fool_loss: 0.722, cum_F_cycle_loss: 0.064\n",
            "[200:300] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:400] Took 11.68s\n",
            "[200:400] cum_D_X_loss: 0.086, cum_G_fool_loss: 0.811, cum_G_cycle_loss: 0.062\n",
            "[200:400] cum_D_Y_loss: 0.058, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.061\n",
            "[200:400] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:500] Took 11.68s\n",
            "[200:500] cum_D_X_loss: 0.069, cum_G_fool_loss: 0.787, cum_G_cycle_loss: 0.064\n",
            "[200:500] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.744, cum_F_cycle_loss: 0.064\n",
            "[200:500] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:600] Took 11.68s\n",
            "[200:600] cum_D_X_loss: 0.096, cum_G_fool_loss: 0.778, cum_G_cycle_loss: 0.064\n",
            "[200:600] cum_D_Y_loss: 0.059, cum_F_fool_loss: 0.735, cum_F_cycle_loss: 0.063\n",
            "[200:600] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:700] Took 11.68s\n",
            "[200:700] cum_D_X_loss: 0.082, cum_G_fool_loss: 0.743, cum_G_cycle_loss: 0.061\n",
            "[200:700] cum_D_Y_loss: 0.071, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.061\n",
            "[200:700] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:800] Took 11.74s\n",
            "[200:800] cum_D_X_loss: 0.080, cum_G_fool_loss: 0.773, cum_G_cycle_loss: 0.060\n",
            "[200:800] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.720, cum_F_cycle_loss: 0.061\n",
            "[200:800] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:900] Took 11.69s\n",
            "[200:900] cum_D_X_loss: 0.083, cum_G_fool_loss: 0.774, cum_G_cycle_loss: 0.060\n",
            "[200:900] cum_D_Y_loss: 0.064, cum_F_fool_loss: 0.703, cum_F_cycle_loss: 0.065\n",
            "[200:900] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:1000] Took 11.68s\n",
            "[200:1000] cum_D_X_loss: 0.091, cum_G_fool_loss: 0.790, cum_G_cycle_loss: 0.061\n",
            "[200:1000] cum_D_Y_loss: 0.077, cum_F_fool_loss: 0.708, cum_F_cycle_loss: 0.058\n",
            "[200:1000] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:1100] Took 11.68s\n",
            "[200:1100] cum_D_X_loss: 0.085, cum_G_fool_loss: 0.750, cum_G_cycle_loss: 0.061\n",
            "[200:1100] cum_D_Y_loss: 0.075, cum_F_fool_loss: 0.759, cum_F_cycle_loss: 0.062\n",
            "[200:1100] fake_X_buffer: 50, fake_Y_buffer: 50\n",
            "[200:END] Completed epoch in 141.7181272506714s\n",
            "[200:1187] ep_D_X_loss: 0.081, ep_G_fool_loss: 0.778, ep_G_cycle_loss: 0.061\n",
            "[200:1187] ep_D_Y_loss: 0.065, ep_F_fool_loss: 0.728, ep_F_cycle_loss: 0.062\n",
            "[200:END] Completed eval in 1.9285621643066406s\n",
            "Updated G_opt learning rate from 1.980198019801982e-06 to 0.0\n",
            "Updated F_opt learning rate from 1.980198019801982e-06 to 0.0\n",
            "Updated D_X_opt learning rate from 1.980198019801982e-06 to 0.0\n",
            "Updated D_Y_opt learning rate from 1.980198019801982e-06 to 0.0\n",
            "[200:END] Saving models and training information\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen_A = Generator()\n",
        "test_gen_A.load_state_dict(torch.load(\"./runs/1677670890.2960796/25/G_A.pth\"))\n",
        "test_gen_A.eval()\n",
        "test_gen_A.to(device)\n",
        ";"
      ],
      "metadata": {
        "id": "sjBk2ODdMZpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected = dataset.dataloader.dataset[random.randint(0, len(dataset.dataloader.dataset))][\"A\"]\n",
        "print(selected.shape)"
      ],
      "metadata": {
        "id": "SN922T3IMzRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(selected.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "EKfOp1XcOdzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "better = tensor2im(selected.unsqueeze(0))\n",
        "plt.imshow(better)"
      ],
      "metadata": {
        "id": "vo5G2ivvOgGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "better_2 = revert_normalisation(selected)\n",
        "plt.imshow(better_2)"
      ],
      "metadata": {
        "id": "-NFRq9vkSHY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    transferred = test_gen_A(selected.to(device)).detach().cpu()"
      ],
      "metadata": {
        "id": "Ph6UixOkOnOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(transferred.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "G7dKdV5pO6Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "better_transferred = tensor2im(transferred.unsqueeze(0))\n",
        "plt.imshow(better_transferred)"
      ],
      "metadata": {
        "id": "aRMk4YLsO_8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "better_transferred_2 = revert_normalisation(transferred)\n",
        "plt.imshow(better_transferred_2)"
      ],
      "metadata": {
        "id": "I58eA95SSONq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}